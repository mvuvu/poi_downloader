# POIçˆ¬è™«ä½¿ç”¨æŒ‡å— ðŸ“–

## ðŸš€ å¿«é€Ÿå¼€å§‹

### 1. åˆå§‹åŒ–é¡¹ç›®

```bash
# é¦–æ¬¡ä½¿ç”¨ï¼Œè¿è¡Œåˆå§‹åŒ–è„šæœ¬
python init.py
```

è¿™ä¼šè‡ªåŠ¨ï¼š
- å®‰è£…æ‰€éœ€ä¾èµ–åŒ…
- åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æž„
- ç”Ÿæˆé…ç½®æ–‡ä»¶æ¨¡æ¿
- æ£€æŸ¥WebDriverçŽ¯å¢ƒ

### 2. å‡†å¤‡è¾“å…¥æ•°æ®

å°†åŒ…å«åœ°å€çš„CSVæ–‡ä»¶æ”¾å…¥ `data/input/` ç›®å½•ï¼š

```csv
District,Latitude,Longitude,Address
åƒä»£ç”°åŒº,35.6862245,139.7347045,æ±äº¬éƒ½åƒä»£ç”°åŒºé›å†¶ç”º1ä¸ç›®7-1
åƒä»£ç”°åŒº,35.6903667,139.7712745,æ±äº¬éƒ½åƒä»£ç”°åŒºäºŒç•ªç”º10-46
```

**é‡è¦**: Addressåˆ—å¿…é¡»åŒ…å«å®Œæ•´çš„æ—¥æœ¬åœ°å€ã€‚

### 3. è¿è¡Œçˆ¬è™«

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®
python final_crawler.py

# æˆ–è€…åœ¨ä»£ç ä¸­æŒ‡å®šè¾“å…¥æ–‡ä»¶
python -c "
import pandas as pd
from final_crawler import FinalPOICrawler

df = pd.read_csv('data/input/your_file.csv')
addresses = df['Address'].tolist()

config = {
    'max_workers': 2,
    'output_file': 'data/output/results.csv'
}

crawler = FinalPOICrawler(config)
crawler.process_addresses(addresses)
crawler.close()
"
```

## âš™ï¸ é…ç½®ç®¡ç†

### åˆ›å»ºé¢„è®¾é…ç½®

```bash
python config.py create-presets
```

è¿™ä¼šåˆ›å»º4ç§é¢„è®¾é…ç½®ï¼š

| é…ç½® | ç”¨é€” | ç‰¹ç‚¹ |
|------|------|------|
| `default` | æ—¥å¸¸ä½¿ç”¨ | å¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§ |
| `fast` | å¿«é€Ÿçˆ¬å– | é«˜å¹¶å‘ï¼Œæ— å¤´æ¨¡å¼ |
| `stable` | ç¨³å®šçˆ¬å– | å•çº¿ç¨‹ï¼Œé«˜é‡è¯• |
| `debug` | è°ƒè¯•æ¨¡å¼ | è¯¦ç»†æ—¥å¿—ï¼Œæˆªå›¾ |

### ä½¿ç”¨ä¸åŒé…ç½®

```python
from config import ConfigManager

# åŠ è½½é…ç½®
manager = ConfigManager()
config = manager.load_config('fast')  # ä½¿ç”¨å¿«é€Ÿæ¨¡å¼

# åˆ›å»ºçˆ¬è™«
crawler = FinalPOICrawler(config.__dict__)
```

### è‡ªå®šä¹‰é…ç½®

ç¼–è¾‘ `config/default.json`:

```json
{
  "crawler": {
    "max_workers": 3,          // å¹¶å‘çº¿ç¨‹æ•° (1-4æŽ¨è)
    "driver_pool_size": 3,     // WebDriveræ± å¤§å°
    "batch_size": 20,          // æ‰¹é‡ä¿å­˜æ•°æ®é‡
    "timeout": 15,             // é¡µé¢åŠ è½½è¶…æ—¶(ç§’)
    "retry_times": 3,          // é‡è¯•æ¬¡æ•°
    "headless": false,         // æ— å¤´æ¨¡å¼ (true/false)
    "checkpoint_interval": 50, // æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”
    "enable_images": false,    // åŠ è½½å›¾ç‰‡ (falseæ›´å¿«)
    "debug_mode": false        // è°ƒè¯•æ¨¡å¼
  }
}
```

## ðŸ“Š ç›‘æŽ§å’Œè°ƒè¯•

### å®žæ—¶è¿›åº¦ç›‘æŽ§

è¿è¡Œæ—¶ä¼šæ˜¾ç¤ºè¯¦ç»†è¿›åº¦ï¼š

```
ðŸ“Š è¿›åº¦æŠ¥å‘Š:
  å¤„ç†: 25/100
  æˆåŠŸ: 21 (84.0%)
  å¤±è´¥: 4
  å¹³å‡: 3.3s/ä¸ª
  é¢„è®¡å‰©ä½™: 4.2åˆ†é’Ÿ
```

### æ£€æŸ¥ç‚¹æ¢å¤

çˆ¬è™«æ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼š

```bash
# ä¸­æ–­åŽé‡æ–°è¿è¡Œï¼Œè‡ªåŠ¨ä»Žæ–­ç‚¹ç»§ç»­
python final_crawler.py
```

æ£€æŸ¥ç‚¹ä¿¡æ¯ä¿å­˜åœ¨ `checkpoint.json`ï¼š

```json
{
  "processed_addresses": ["åœ°å€1", "åœ°å€2"],
  "processed_count": 25,
  "success_count": 21,
  "failed_addresses": ["å¤±è´¥åœ°å€1"],
  "timestamp": "2025-07-02T15:30:00"
}
```

### æŸ¥çœ‹æ—¥å¿—

```bash
# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
tail -f logs/crawler.log

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
grep "ERROR" logs/crawler.log
```

## ðŸŽ¨ é«˜çº§ç”¨æ³•

### Jupyter Notebookäº¤äº’å¼ä½¿ç”¨

```python
# åœ¨Jupyterä¸­è¿è¡Œ
import pandas as pd
from final_crawler import FinalPOICrawler

# å°æ‰¹é‡æµ‹è¯•
addresses = ["æ±äº¬éƒ½åƒä»£ç”°åŒºé›å†¶ç”º1ä¸ç›®7-1"]
config = {'max_workers': 1, 'output_file': 'test.csv'}

crawler = FinalPOICrawler(config)
crawler.process_addresses(addresses)
crawler.close()

# æŸ¥çœ‹ç»“æžœ
results = pd.read_csv('test.csv')
print(results.head())
```

### æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶

```python
import glob
from pathlib import Path

# å¤„ç†data/input/ç›®å½•ä¸‹æ‰€æœ‰CSVæ–‡ä»¶
input_dir = Path('data/input')
output_dir = Path('data/output')

for csv_file in input_dir.glob('*.csv'):
    print(f"å¤„ç†æ–‡ä»¶: {csv_file}")
    
    df = pd.read_csv(csv_file)
    addresses = df['Address'].tolist()
    
    output_file = output_dir / f"poi_{csv_file.stem}.csv"
    config = {'output_file': str(output_file)}
    
    crawler = FinalPOICrawler(config)
    crawler.process_addresses(addresses)
    crawler.close()
```

### æ•°æ®é¢„å¤„ç†

```python
# åœ°å€æ•°æ®æ¸…ç†
def clean_addresses(df):
    # ç§»é™¤ç©ºå€¼
    df = df.dropna(subset=['Address'])
    
    # æ ‡å‡†åŒ–åœ°å€æ ¼å¼
    df['Address'] = df['Address'].str.strip()
    
    # ç§»é™¤é‡å¤åœ°å€
    df = df.drop_duplicates(subset=['Address'])
    
    return df

df = pd.read_csv('input.csv')
df_clean = clean_addresses(df)
df_clean.to_csv('cleaned_input.csv', index=False)
```

## ðŸ”§ æ€§èƒ½ä¼˜åŒ–

### 1. ç¡¬ä»¶ä¼˜åŒ–

- **CPU**: å¤šæ ¸å¿ƒæœ‰åŠ©äºŽå¹¶å‘å¤„ç†
- **å†…å­˜**: å»ºè®®8GBä»¥ä¸Šï¼Œç‰¹åˆ«æ˜¯å¤§æ‰¹é‡æ•°æ®
- **ç½‘ç»œ**: ç¨³å®šçš„å®½å¸¦è¿žæŽ¥

### 2. å‚æ•°è°ƒä¼˜

**é«˜æ€§èƒ½é…ç½®** (é€‚åˆæœåŠ¡å™¨):
```json
{
  "max_workers": 4,
  "driver_pool_size": 4,
  "batch_size": 10,
  "headless": true,
  "enable_images": false
}
```

**ç¨³å®šé…ç½®** (é€‚åˆç½‘ç»œä¸ç¨³å®š):
```json
{
  "max_workers": 1,
  "retry_times": 5,
  "timeout": 30,
  "checkpoint_interval": 25
}
```

### 3. ç³»ç»Ÿä¼˜åŒ–

```bash
# Linuxç³»ç»Ÿä¼˜åŒ–
# å¢žåŠ æ–‡ä»¶æè¿°ç¬¦é™åˆ¶
ulimit -n 4096

# ä¼˜åŒ–TCPè¿žæŽ¥
echo 'net.ipv4.tcp_tw_reuse = 1' >> /etc/sysctl.conf
sysctl -p
```

## â“ å¸¸è§é—®é¢˜

### Q: çˆ¬å–é€Ÿåº¦å¾ˆæ…¢æ€Žä¹ˆåŠžï¼Ÿ

A: 
1. å¯ç”¨æ— å¤´æ¨¡å¼: `"headless": true`
2. ç¦ç”¨å›¾ç‰‡åŠ è½½: `"enable_images": false`
3. å¢žåŠ å¹¶å‘æ•°: `"max_workers": 3`
4. å‡å°‘è¶…æ—¶æ—¶é—´: `"timeout": 10`

### Q: ç»å¸¸å‡ºçŽ°"å»ºé€ ç‰©"è¯†åˆ«é”™è¯¯ï¼Ÿ

A: è¿™æ˜¯Google Mapsé¡µé¢ç»“æž„å˜åŒ–å¯¼è‡´çš„ï¼Œæ–°ç‰ˆæœ¬å·²ä¿®å¤ï¼š
- ä½¿ç”¨å¤šç­–ç•¥å…ƒç´ å®šä½
- æ”¯æŒ"å»ºé€ ç‰©"/"å»ºç­‘ç‰©"/"å»ºç¯‰ç‰©"ç­‰å¤šç§è¡¨è¿°

### Q: å¦‚ä½•å¤„ç†åçˆ¬è™«æ£€æµ‹ï¼Ÿ

A:
1. é™ä½Žå¹¶å‘æ•°: `"max_workers": 1`
2. å¢žåŠ éšæœºå»¶è¿Ÿ
3. ä½¿ç”¨ä»£ç†IP (éœ€è¦è‡ªè¡Œé…ç½®)
4. æ›´æ¢User-Agent

### Q: å†…å­˜å ç”¨è¿‡é«˜ï¼Ÿ

A:
1. å‡å°‘æ‰¹å¤„ç†å¤§å°: `"batch_size": 10`
2. å‡å°‘WebDriveræ± : `"driver_pool_size": 2`
3. å¯ç”¨æ— å¤´æ¨¡å¼: `"headless": true`

### Q: å¦‚ä½•éªŒè¯æ•°æ®è´¨é‡ï¼Ÿ

A:
```python
# æ•°æ®è´¨é‡æ£€æŸ¥
results = pd.read_csv('output.csv')

print("æ•°æ®ç»Ÿè®¡:")
print(f"æ€»POIæ•°: {len(results)}")
print(f"å”¯ä¸€å»ºç­‘: {results['blt_name'].nunique()}")
print(f"å¹³å‡è¯„åˆ†: {results['rating'].mean():.2f}")
print(f"ç©ºå€¼æ£€æŸ¥: {results.isnull().sum()}")

# æ£€æŸ¥å¼‚å¸¸æ•°æ®
print("\nå¼‚å¸¸æ•°æ®:")
print(f"è¯„åˆ†>5: {len(results[results['rating'] > 5])}")
print(f"è¯„åˆ†<1: {len(results[results['rating'] < 1])}")
```

## ðŸ“ž æŠ€æœ¯æ”¯æŒ

é‡åˆ°é—®é¢˜æ—¶ï¼š

1. **æŸ¥çœ‹æ—¥å¿—**: `logs/crawler.log`
2. **æ£€æŸ¥é…ç½®**: `config/default.json`
3. **éªŒè¯çŽ¯å¢ƒ**: `python init.py`
4. **é‡ç½®çŠ¶æ€**: åˆ é™¤ `checkpoint.json`

## ðŸ”„ ç‰ˆæœ¬æ›´æ–°

æ£€æŸ¥æ›´æ–°ï¼š
```bash
git pull origin main
pip install -r requirements.txt --upgrade
```

---

**æç¤º**: å»ºè®®å…ˆç”¨å°æ‰¹é‡æ•°æ®æµ‹è¯•ï¼Œç¡®è®¤é…ç½®æ— è¯¯åŽå†è¿›è¡Œå¤§è§„æ¨¡çˆ¬å–ã€‚