#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æPOIçˆ¬è™«è¿è¡Œæ—¶çš„æ€§èƒ½ç“¶é¢ˆ
é€šè¿‡åœ¨ç°æœ‰ä»£ç ä¸­æ·»åŠ ç›‘æ§ç‚¹æ¥æ”¶é›†æ•°æ®
"""

import json
import time
import threading
from datetime import datetime
from pathlib import Path
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import os

class CrawlerPerformanceAnalyzer:
    """çˆ¬è™«æ€§èƒ½åˆ†æå™¨ - ä¸ä¿®æ”¹åŸä»£ç ï¼Œé€šè¿‡æ—¥å¿—åˆ†æ"""
    
    def __init__(self):
        self.analysis_dir = Path("performance_analysis")
        self.analysis_dir.mkdir(exist_ok=True)
        
    def analyze_queue_status(self):
        """åˆ†æä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€ - é€šè¿‡æ¨¡æ‹Ÿç›‘æ§"""
        print("\nğŸ“Š åˆ†æä»»åŠ¡é˜Ÿåˆ—çŠ¶æ€...")
        
        # åˆ›å»ºæ¨¡æ‹Ÿç›‘æ§è„šæœ¬
        monitor_script = """
import time
import json
from datetime import datetime
from pathlib import Path

# ç›‘æ§è¾“å‡ºç›®å½•
output_dir = Path("performance_analysis/queue_monitor")
output_dir.mkdir(parents=True, exist_ok=True)

print("ğŸ” å¼€å§‹ç›‘æ§ä»»åŠ¡é˜Ÿåˆ—...")
print("âš ï¸  æ³¨æ„ï¼šéœ€è¦æ‰‹åŠ¨è§‚å¯Ÿpoi_crawler_simple.pyçš„è¾“å‡ºæ¥æ”¶é›†ä»¥ä¸‹æ•°æ®ï¼š")
print("1. æ¯5åˆ†é’Ÿè®°å½•ä¸€æ¬¡å·²å¤„ç†ä»»åŠ¡æ•°")
print("2. è§‚å¯Ÿé‡è¯•ä»»åŠ¡çš„å‡ºç°é¢‘ç‡")
print("3. è®°å½•Chromeé‡å¯çš„æ—¶é—´ç‚¹")
print("4. æ³¨æ„æ˜¯å¦å‡ºç°é•¿æ—¶é—´æ— è¾“å‡ºçš„æƒ…å†µ")

# åˆ›å»ºæ•°æ®è®°å½•æ¨¡æ¿
data_template = {
    "timestamp": "",
    "elapsed_minutes": 0,
    "total_processed": 0,
    "retry_count": 0,
    "chrome_restart_count": 0,
    "notes": ""
}

# ä¿å­˜æ¨¡æ¿
with open(output_dir / "data_template.json", 'w') as f:
    json.dump(data_template, f, indent=2)

print(f"\\nğŸ“ è¯·æ‰‹åŠ¨è®°å½•æ•°æ®åˆ°: {output_dir / 'manual_records.json'}")
print("æ ¼å¼ç¤ºä¾‹ï¼š")
print(json.dumps(data_template, indent=2))
"""
        
        # ä¿å­˜ç›‘æ§è„šæœ¬
        script_path = self.analysis_dir / "queue_monitor.py"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(monitor_script)
            
        print(f"âœ… ç›‘æ§è„šæœ¬å·²åˆ›å»º: {script_path}")
        print("ğŸ“‹ ä½¿ç”¨æ–¹æ³•ï¼š")
        print("1. åœ¨è¿è¡Œçˆ¬è™«çš„åŒæ—¶è¿è¡Œæ­¤è„šæœ¬")
        print("2. æ‰‹åŠ¨è®°å½•è§‚å¯Ÿåˆ°çš„æ•°æ®")
        print("3. åˆ†æè®°å½•çš„æ•°æ®æ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆ")
        
    def analyze_task_timing(self):
        """åˆ†æä»»åŠ¡å¤„ç†æ—¶é—´ - é€šè¿‡æ—¥å¿—åˆ†æ"""
        print("\nâ±ï¸  åˆ†æä»»åŠ¡å¤„ç†æ—¶é—´...")
        
        # åˆ›å»ºæ—¶é—´åˆ†æè„šæœ¬
        timing_script = """
import re
from datetime import datetime
from pathlib import Path
import json

def analyze_log_timing(log_file):
    \"\"\"åˆ†ææ—¥å¿—ä¸­çš„æ—¶é—´ä¿¡æ¯\"\"\"
    
    # æ—¶é—´æ¨¡å¼åŒ¹é…
    patterns = {
        'task_start': r'ğŸ” å¤„ç†åœ°å€: (.+)',
        'task_complete': r'âœ… (.+) \\| POI: (\\d+) \\| çŠ¶æ€: å·²ä¿å­˜',
        'task_retry': r'ğŸ”„ éå»ºç­‘ç‰©ï¼Œä½¿ç”¨æ—¥æ–‡åœ°å€é‡è¯•: (.+)',
        'chrome_restart': r'ğŸ”„ Worker (\\d+): è¾¾åˆ°1000ä¸ªä»»åŠ¡ï¼Œé‡å¯Chromeé©±åŠ¨',
        'error': r'âŒ (.+) \\| é”™è¯¯: (.+)'
    }
    
    # è¯»å–æ—¥å¿—å¹¶åˆ†æ
    task_times = []
    current_tasks = {}
    
    with open(log_file, 'r', encoding='utf-8') as f:
        for line in f:
            # æå–æ—¶é—´æˆ³ï¼ˆå¦‚æœæ—¥å¿—åŒ…å«ï¼‰
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ—¥å¿—æ ¼å¼è°ƒæ•´
            
            for pattern_name, pattern in patterns.items():
                match = re.search(pattern, line)
                if match:
                    if pattern_name == 'task_start':
                        address = match.group(1)
                        current_tasks[address] = {'start': datetime.now()}
                    elif pattern_name == 'task_complete':
                        address = match.group(1)
                        if address in current_tasks:
                            duration = (datetime.now() - current_tasks[address]['start']).seconds
                            task_times.append({
                                'address': address,
                                'duration': duration,
                                'poi_count': int(match.group(2)),
                                'type': 'normal'
                            })
                    elif pattern_name == 'task_retry':
                        # è®°å½•é‡è¯•ä»»åŠ¡
                        task_times.append({
                            'address': match.group(1),
                            'type': 'retry'
                        })
    
    return task_times

print("ğŸ“‹ ä»»åŠ¡æ—¶é—´åˆ†æè„šæœ¬")
print("ä½¿ç”¨æ–¹æ³•ï¼š")
print("1. å°†çˆ¬è™«è¾“å‡ºé‡å®šå‘åˆ°æ–‡ä»¶: python poi_crawler_simple.py > crawler_output.log 2>&1")
print("2. è¿è¡Œåˆ†æ: python analyze_timing.py crawler_output.log")
"""
        
        script_path = self.analysis_dir / "analyze_timing.py"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(timing_script)
            
        print(f"âœ… æ—¶é—´åˆ†æè„šæœ¬å·²åˆ›å»º: {script_path}")
        
    def create_worker_monitor(self):
        """åˆ›å»ºWorkerçŠ¶æ€ç›‘æ§å·¥å…·"""
        print("\nğŸ‘· åˆ›å»ºWorkerçŠ¶æ€ç›‘æ§...")
        
        worker_monitor = """
import psutil
import time
import json
from datetime import datetime
from pathlib import Path

class WorkerMonitor:
    \"\"\"ç›‘æ§Workerçº¿ç¨‹çŠ¶æ€\"\"\"
    
    def __init__(self):
        self.output_dir = Path("performance_analysis/worker_stats")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def monitor_threads(self, pid):
        \"\"\"ç›‘æ§æŒ‡å®šè¿›ç¨‹çš„çº¿ç¨‹çŠ¶æ€\"\"\"
        try:
            process = psutil.Process(pid)
            threads = process.threads()
            
            stats = {
                'timestamp': datetime.now().isoformat(),
                'pid': pid,
                'thread_count': len(threads),
                'cpu_percent': process.cpu_percent(interval=1),
                'memory_mb': process.memory_info().rss / 1024 / 1024,
                'threads': []
            }
            
            # åˆ†æçº¿ç¨‹çŠ¶æ€
            for thread in threads:
                thread_info = {
                    'id': thread.id,
                    'cpu_time': thread.user_time + thread.system_time
                }
                stats['threads'].append(thread_info)
                
            return stats
            
        except Exception as e:
            print(f"ç›‘æ§é”™è¯¯: {e}")
            return None
            
    def find_crawler_process(self):
        \"\"\"æŸ¥æ‰¾çˆ¬è™«è¿›ç¨‹\"\"\"
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                cmdline = ' '.join(proc.info['cmdline'] or [])
                if 'poi_crawler_simple.py' in cmdline:
                    return proc.info['pid']
            except:
                continue
        return None
        
    def run(self, duration=300):
        \"\"\"è¿è¡Œç›‘æ§\"\"\"
        print("ğŸ” æ­£åœ¨æŸ¥æ‰¾çˆ¬è™«è¿›ç¨‹...")
        pid = self.find_crawler_process()
        
        if not pid:
            print("âŒ æœªæ‰¾åˆ°çˆ¬è™«è¿›ç¨‹ï¼Œè¯·å…ˆå¯åŠ¨poi_crawler_simple.py")
            return
            
        print(f"âœ… æ‰¾åˆ°çˆ¬è™«è¿›ç¨‹ PID: {pid}")
        print(f"ğŸ“Š å¼€å§‹ç›‘æ§ {duration} ç§’...")
        
        stats_file = self.output_dir / f"worker_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        all_stats = []
        
        start_time = time.time()
        while time.time() - start_time < duration:
            stats = self.monitor_threads(pid)
            if stats:
                all_stats.append(stats)
                print(f"  çº¿ç¨‹æ•°: {stats['thread_count']} | CPU: {stats['cpu_percent']:.1f}% | å†…å­˜: {stats['memory_mb']:.1f}MB")
            
            time.sleep(5)  # æ¯5ç§’é‡‡æ ·ä¸€æ¬¡
            
        # ä¿å­˜ç»Ÿè®¡æ•°æ®
        with open(stats_file, 'w') as f:
            json.dump(all_stats, f, indent=2)
            
        print(f"\\nâœ… ç›‘æ§å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ°: {stats_file}")
        
        # åˆ†æç»“æœ
        self.analyze_results(all_stats)
        
    def analyze_results(self, stats):
        \"\"\"åˆ†æç›‘æ§ç»“æœ\"\"\"
        print("\\nğŸ“ˆ ç›‘æ§ç»“æœåˆ†æ:")
        
        cpu_values = [s['cpu_percent'] for s in stats]
        thread_counts = [s['thread_count'] for s in stats]
        
        print(f"  å¹³å‡CPUä½¿ç”¨ç‡: {sum(cpu_values)/len(cpu_values):.1f}%")
        print(f"  CPUä½¿ç”¨ç‡èŒƒå›´: {min(cpu_values):.1f}% - {max(cpu_values):.1f}%")
        print(f"  å¹³å‡çº¿ç¨‹æ•°: {sum(thread_counts)/len(thread_counts):.1f}")
        
        # æ£€æµ‹ä½CPUä½¿ç”¨ç‡
        low_cpu_count = sum(1 for cpu in cpu_values if cpu < 20)
        if low_cpu_count > len(cpu_values) * 0.5:
            print("\\nâš ï¸  è­¦å‘Š: è¶…è¿‡50%çš„æ—¶é—´CPUä½¿ç”¨ç‡ä½äº20%")
            print("  å¯èƒ½åŸå› :")
            print("  - Workerçº¿ç¨‹åœ¨ç­‰å¾…I/Oï¼ˆç½‘é¡µåŠ è½½ï¼‰")
            print("  - ä»»åŠ¡é˜Ÿåˆ—ä¸ºç©º")
            print("  - è¿‡å¤šçš„åŒæ­¥ç­‰å¾…")

if __name__ == "__main__":
    monitor = WorkerMonitor()
    monitor.run(duration=300)  # ç›‘æ§5åˆ†é’Ÿ
"""
        
        script_path = self.analysis_dir / "worker_monitor.py"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(worker_monitor)
            
        print(f"âœ… Workerç›‘æ§è„šæœ¬å·²åˆ›å»º: {script_path}")
        
    def create_retry_analyzer(self):
        """åˆ›å»ºé‡è¯•ä»»åŠ¡åˆ†æå™¨"""
        print("\nğŸ”„ åˆ›å»ºé‡è¯•ä»»åŠ¡åˆ†æå™¨...")
        
        retry_analyzer = """
import re
from collections import Counter
from pathlib import Path
import json

def analyze_retry_patterns(log_file):
    \"\"\"åˆ†æé‡è¯•ä»»åŠ¡æ¨¡å¼\"\"\"
    
    retry_addresses = []
    normal_addresses = []
    retry_pattern = r'ğŸ”„ éå»ºç­‘ç‰©ï¼Œä½¿ç”¨æ—¥æ–‡åœ°å€é‡è¯•: (.+)'
    success_pattern = r'âœ… (.+) \\| POI: (\\d+) \\| çŠ¶æ€: å·²ä¿å­˜'
    
    with open(log_file, 'r', encoding='utf-8') as f:
        for line in f:
            # æ”¶é›†é‡è¯•åœ°å€
            retry_match = re.search(retry_pattern, line)
            if retry_match:
                retry_addresses.append(retry_match.group(1))
            
            # æ”¶é›†æˆåŠŸåœ°å€
            success_match = re.search(success_pattern, line)
            if success_match:
                normal_addresses.append(success_match.group(1))
    
    # åˆ†æ
    total_tasks = len(normal_addresses) + len(retry_addresses)
    retry_rate = len(retry_addresses) / total_tasks * 100 if total_tasks > 0 else 0
    
    print(f"\\nğŸ“Š é‡è¯•ä»»åŠ¡åˆ†æ:")
    print(f"  æ€»ä»»åŠ¡æ•°: {total_tasks}")
    print(f"  é‡è¯•ä»»åŠ¡æ•°: {len(retry_addresses)}")
    print(f"  é‡è¯•ç‡: {retry_rate:.1f}%")
    
    # åˆ†æé‡è¯•åœ°å€ç‰¹å¾
    if retry_addresses:
        print(f"\\nğŸ” é‡è¯•åœ°å€ç¤ºä¾‹:")
        for addr in retry_addresses[:5]:
            print(f"  - {addr}")
    
    # ä¿å­˜åˆ†æç»“æœ
    results = {
        'total_tasks': total_tasks,
        'retry_count': len(retry_addresses),
        'retry_rate': retry_rate,
        'retry_addresses_sample': retry_addresses[:20]
    }
    
    with open('performance_analysis/retry_analysis.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    return results

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        analyze_retry_patterns(sys.argv[1])
    else:
        print("ä½¿ç”¨æ–¹æ³•: python retry_analyzer.py crawler_output.log")
"""
        
        script_path = self.analysis_dir / "retry_analyzer.py"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(retry_analyzer)
            
        print(f"âœ… é‡è¯•åˆ†æè„šæœ¬å·²åˆ›å»º: {script_path}")
        
    def create_performance_test(self):
        """åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬"""
        print("\nğŸ§ª åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬...")
        
        test_script = """
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
\"\"\"
POIçˆ¬è™«æ€§èƒ½æµ‹è¯•è„šæœ¬
ç”¨äºè¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
\"\"\"

import subprocess
import time
import psutil
import json
from datetime import datetime
from pathlib import Path

def run_performance_test():
    \"\"\"è¿è¡Œæ€§èƒ½æµ‹è¯•\"\"\"
    
    print("ğŸš€ POIçˆ¬è™«æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    # æµ‹è¯•é…ç½®
    test_configs = [
        {
            'name': 'å°æ‰¹é‡æµ‹è¯•',
            'file': 'data/input/test_small.csv',  # éœ€è¦å‡†å¤‡10æ¡æ•°æ®çš„æµ‹è¯•æ–‡ä»¶
            'expected_time': 300  # é¢„æœŸ5åˆ†é’Ÿå†…å®Œæˆ
        },
        {
            'name': 'ä¸­ç­‰æ‰¹é‡æµ‹è¯•',
            'file': 'data/input/test_medium.csv',  # éœ€è¦å‡†å¤‡50æ¡æ•°æ®çš„æµ‹è¯•æ–‡ä»¶
            'expected_time': 1800  # é¢„æœŸ30åˆ†é’Ÿå†…å®Œæˆ
        }
    ]
    
    results = []
    
    for config in test_configs:
        print(f"\\nğŸ“Š è¿è¡Œæµ‹è¯•: {config['name']}")
        print(f"  æµ‹è¯•æ–‡ä»¶: {config['file']}")
        
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        start_cpu = psutil.cpu_percent(interval=1)
        start_memory = psutil.virtual_memory().percent
        
        # å¯åŠ¨çˆ¬è™«
        process = subprocess.Popen([
            'python', 'poi_crawler_simple.py', config['file']
        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        
        # ç›‘æ§è¿›ç¨‹
        cpu_samples = []
        memory_samples = []
        
        while process.poll() is None:
            try:
                proc = psutil.Process(process.pid)
                cpu_samples.append(proc.cpu_percent(interval=1))
                memory_samples.append(proc.memory_info().rss / 1024 / 1024)  # MB
            except:
                pass
            time.sleep(5)
        
        # è®°å½•ç»“æŸçŠ¶æ€
        end_time = time.time()
        duration = end_time - start_time
        
        # æ”¶é›†ç»“æœ
        result = {
            'test_name': config['name'],
            'duration_seconds': duration,
            'expected_seconds': config['expected_time'],
            'performance_ratio': duration / config['expected_time'],
            'avg_cpu': sum(cpu_samples) / len(cpu_samples) if cpu_samples else 0,
            'max_memory_mb': max(memory_samples) if memory_samples else 0,
            'status': 'PASS' if duration <= config['expected_time'] else 'SLOW'
        }
        
        results.append(result)
        
        # è¾“å‡ºç»“æœ
        print(f"\\n  æµ‹è¯•ç»“æœ: {result['status']}")
        print(f"  å®é™…ç”¨æ—¶: {duration:.1f}ç§’ (é¢„æœŸ: {config['expected_time']}ç§’)")
        print(f"  å¹³å‡CPU: {result['avg_cpu']:.1f}%")
        print(f"  æœ€å¤§å†…å­˜: {result['max_memory_mb']:.1f}MB")
        
        if result['status'] == 'SLOW':
            print(f"  âš ï¸  æ€§èƒ½ä½äºé¢„æœŸ {result['performance_ratio']:.1f}å€")
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    output_file = Path('performance_analysis') / f"test_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    output_file.parent.mkdir(exist_ok=True)
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\\nâœ… æµ‹è¯•å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    # æ€§èƒ½è¯Šæ–­
    print("\\nğŸ” æ€§èƒ½è¯Šæ–­:")
    slow_tests = [r for r in results if r['status'] == 'SLOW']
    if slow_tests:
        print("  å‘ç°æ€§èƒ½é—®é¢˜:")
        for test in slow_tests:
            print(f"  - {test['test_name']}: æ¯”é¢„æœŸæ…¢{test['performance_ratio']:.1f}å€")
        print("\\n  å¯èƒ½çš„åŸå› :")
        print("  1. ç½‘ç»œå»¶è¿Ÿå¢åŠ ")
        print("  2. é‡è¯•ä»»åŠ¡è¿‡å¤š")
        print("  3. é¡µé¢åŠ è½½è¶…æ—¶")
        print("  4. èµ„æºç«äº‰ï¼ˆCPU/å†…å­˜ï¼‰")
    else:
        print("  æ€§èƒ½ç¬¦åˆé¢„æœŸ")

if __name__ == "__main__":
    run_performance_test()
"""
        
        script_path = self.analysis_dir / "performance_test.py"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(test_script)
            
        print(f"âœ… æ€§èƒ½æµ‹è¯•è„šæœ¬å·²åˆ›å»º: {script_path}")
        
    def generate_all_tools(self):
        """ç”Ÿæˆæ‰€æœ‰åˆ†æå·¥å…·"""
        print("ğŸ› ï¸  ç”Ÿæˆæ€§èƒ½åˆ†æå·¥å…·é›†...")
        
        self.analyze_queue_status()
        self.analyze_task_timing()
        self.create_worker_monitor()
        self.create_retry_analyzer()
        self.create_performance_test()
        
        # åˆ›å»ºä½¿ç”¨è¯´æ˜
        readme_content = """# POIçˆ¬è™«æ€§èƒ½åˆ†æå·¥å…·é›†

## ğŸ¯ ç›®çš„
è¯†åˆ«POIçˆ¬è™«é•¿æ—¶é—´è¿è¡ŒåCPUå’Œå†…å­˜åˆ©ç”¨ç‡ä¸‹é™çš„åŸå› 

## ğŸ› ï¸ å·¥å…·åˆ—è¡¨

### 1. queue_monitor.py - ä»»åŠ¡é˜Ÿåˆ—ç›‘æ§
æ‰‹åŠ¨è®°å½•ä»»åŠ¡å¤„ç†è¿›åº¦ï¼Œåˆ†æé˜Ÿåˆ—çŠ¶æ€

### 2. analyze_timing.py - ä»»åŠ¡æ—¶é—´åˆ†æ
åˆ†ææ¯ä¸ªä»»åŠ¡çš„å¤„ç†æ—¶é—´ï¼Œæ‰¾å‡ºæ…¢ä»»åŠ¡

### 3. worker_monitor.py - Workerçº¿ç¨‹ç›‘æ§
å®æ—¶ç›‘æ§Workerçº¿ç¨‹çš„CPUä½¿ç”¨ç‡å’ŒçŠ¶æ€

### 4. retry_analyzer.py - é‡è¯•ä»»åŠ¡åˆ†æ
ç»Ÿè®¡é‡è¯•ä»»åŠ¡çš„æ¯”ä¾‹å’Œç‰¹å¾

### 5. performance_test.py - æ€§èƒ½åŸºå‡†æµ‹è¯•
è¿è¡Œæ ‡å‡†æµ‹è¯•ç”¨ä¾‹ï¼Œå¯¹æ¯”æ€§èƒ½

## ğŸ“‹ ä½¿ç”¨æ­¥éª¤

1. **å‡†å¤‡æµ‹è¯•æ•°æ®**
   ```bash
   # åˆ›å»ºå°æ‰¹é‡æµ‹è¯•æ–‡ä»¶ï¼ˆ10æ¡æ•°æ®ï¼‰
   head -n 11 data/input/ä½ çš„æ–‡ä»¶.csv > data/input/test_small.csv
   
   # åˆ›å»ºä¸­ç­‰æ‰¹é‡æµ‹è¯•æ–‡ä»¶ï¼ˆ50æ¡æ•°æ®ï¼‰
   head -n 51 data/input/ä½ çš„æ–‡ä»¶.csv > data/input/test_medium.csv
   ```

2. **è¿è¡Œçˆ¬è™«å¹¶è®°å½•è¾“å‡º**
   ```bash
   python poi_crawler_simple.py --all > crawler_output.log 2>&1
   ```

3. **åŒæ—¶è¿è¡Œç›‘æ§å·¥å…·**
   ```bash
   # åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡ŒWorkerç›‘æ§
   python performance_analysis/worker_monitor.py
   ```

4. **åˆ†æç»“æœ**
   ```bash
   # åˆ†æé‡è¯•æ¨¡å¼
   python performance_analysis/retry_analyzer.py crawler_output.log
   
   # è¿è¡Œæ€§èƒ½æµ‹è¯•
   python performance_analysis/performance_test.py
   ```

## ğŸ” é‡ç‚¹å…³æ³¨

1. **CPUä½¿ç”¨ç‡ä¸‹é™æ—¶é—´ç‚¹** - è®°å½•ä½•æ—¶å¼€å§‹å‡ºç°ä½CPU
2. **é‡è¯•ä»»åŠ¡æ¯”ä¾‹å˜åŒ–** - è§‚å¯Ÿé‡è¯•æ˜¯å¦é€æ¸å¢å¤š
3. **å•ä»»åŠ¡å¤„ç†æ—¶é—´** - æ˜¯å¦æœ‰ä»»åŠ¡ç‰¹åˆ«æ…¢
4. **Workerçº¿ç¨‹çŠ¶æ€** - æ˜¯å¦å¤§éƒ¨åˆ†æ—¶é—´åœ¨ç­‰å¾…

## ğŸ“Š é¢„æœŸå‘ç°

- ä»»åŠ¡é˜Ÿåˆ—åæœŸä»¥é‡è¯•ä»»åŠ¡ä¸ºä¸»
- Workerçº¿ç¨‹å¤§éƒ¨åˆ†æ—¶é—´åœ¨ç­‰å¾…ç½‘é¡µåŠ è½½
- æŸäº›ç‰¹å®šåœ°å€å¯¼è‡´å¤„ç†æ—¶é—´è¿‡é•¿
- Chromeè¿›ç¨‹èµ„æºå ç”¨é€æ¸å¢åŠ 
"""
        
        readme_path = self.analysis_dir / "README.md"
        with open(readme_path, 'w', encoding='utf-8') as f:
            f.write(readme_content)
            
        print(f"\nâœ… æ‰€æœ‰åˆ†æå·¥å…·å·²ç”Ÿæˆåˆ°: {self.analysis_dir}")
        print(f"ğŸ“– æŸ¥çœ‹ä½¿ç”¨è¯´æ˜: {readme_path}")


if __name__ == "__main__":
    analyzer = CrawlerPerformanceAnalyzer()
    analyzer.generate_all_tools()