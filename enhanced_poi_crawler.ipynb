{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# å¢å¼ºç‰ˆPOIçˆ¬è™« v2.0 ğŸš€\n\n## ğŸ†• æœ€æ–°ç‰¹æ€§ (2025-07-02)\n- ğŸ”¥ **ç»Ÿä¸€ç¨‹åºå…¥å£** - æ‰€æœ‰åŠŸèƒ½é›†æˆåˆ°ä¸€ä¸ªç¨‹åº\n- ğŸ”‡ **å®Œå…¨é™é»˜è¿è¡Œ** - å±è”½æ‰€æœ‰Chromeè­¦å‘Šä¿¡æ¯\n- ğŸ“Š **9ä¸ªå¢å¼ºå­—æ®µ** - è¯„è®ºæ•°é‡ã€ç”µè¯ã€ç½‘ç«™ã€è¥ä¸šæ—¶é—´ã€ä»·æ ¼ç­‰çº§\n- âš¡ **4å€å¹¶å‘ä¼˜åŒ–** - æ™ºèƒ½WebDriveræ± ï¼Œé«˜æ•ˆå¤„ç†\n- ğŸ›¡ï¸ **ç¨³å®šå¯é ** - å¤šç­–ç•¥å…ƒç´ å®šä½ï¼Œæ™ºèƒ½é‡è¯•æœºåˆ¶\n- ğŸ’¾ **æ–­ç‚¹ç»­çˆ¬** - è‡ªåŠ¨æ£€æŸ¥ç‚¹ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®æ”¶é›†\n- ğŸ§ª **æµ‹è¯•æ¨¡å¼** - å¿«é€ŸéªŒè¯ç¯å¢ƒå’ŒåŠŸèƒ½\n- ğŸ“‚ **æ™ºèƒ½æ–‡ä»¶é€‰æ‹©** - è‡ªåŠ¨æ‰«æå¹¶é€‰æ‹©æœ€å¤§CSVæ–‡ä»¶\n\n## ğŸ”§ æ¨èä½¿ç”¨æ–¹å¼\n1. **é¦–æ¬¡ä½¿ç”¨**: è¿è¡Œæµ‹è¯•æ¨¡å¼éªŒè¯ç¯å¢ƒ\n2. **å°è§„æ¨¡éªŒè¯**: å¤„ç†éƒ¨åˆ†åœ°å€ç¡®è®¤æ•ˆæœ\n3. **ç”Ÿäº§è¿è¡Œ**: å¼€å¯å…¨é‡çˆ¬å–è·å–å®Œæ•´æ•°æ®",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# å¯¼å…¥å¿…è¦çš„åº“\nimport pandas as pd\nimport numpy as np\nimport time\nimport json\nfrom pathlib import Path\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# å¯¼å…¥å¢å¼ºç‰ˆçˆ¬è™«æ¨¡å—\nfrom final_crawler import FinalPOICrawler\nfrom simple_file_selector import SimpleFileSelector, get_simple_file_config\n\nprint(\"ğŸ“¦ æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸï¼\")\nprint(f\"ğŸ“… å½“å‰æ—¶é—´: {pd.Timestamp.now()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ 1. é…ç½®è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”¥ æ–°åŠŸèƒ½ï¼šæ™ºèƒ½æ–‡ä»¶é€‰æ‹©å™¨\nENABLE_FILE_SELECTION = True  # è®¾ç½®ä¸ºTrueå¯ç”¨æ–‡ä»¶é€‰æ‹©ï¼ŒFalseä½¿ç”¨é»˜è®¤æ–‡ä»¶\n\nif ENABLE_FILE_SELECTION:\n    print(\"ğŸ“‚ å¯åŠ¨æ™ºèƒ½æ–‡ä»¶é€‰æ‹©å™¨...\")\n    print(\"=\" * 50)\n    \n    # ä½¿ç”¨ç®€åŒ–çš„æ–‡ä»¶é€‰æ‹©å™¨\n    file_config = get_simple_file_config(suffix=\"poi_notebook\")\n    \n    if file_config['has_input']:\n        INPUT_FILE = file_config['input_file']\n        OUTPUT_FILE = file_config['output_file']\n        \n        print(f\"âœ… æ–‡ä»¶é€‰æ‹©å®Œæˆ:\")\n        print(f\"ğŸ“¥ è¾“å…¥æ–‡ä»¶: {INPUT_FILE}\")\n        print(f\"ğŸ“¤ è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}\")\n        \n        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯\n        try:\n            df_check = pd.read_csv(INPUT_FILE)\n            print(f\"ğŸ“Š æ–‡ä»¶ä¿¡æ¯: {len(df_check):,} è¡Œ, åˆ—: {list(df_check.columns)}\")\n        except Exception as e:\n            print(f\"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶ä¿¡æ¯: {e}\")\n            \n    else:\n        print(\"âŒ æœªæ‰¾åˆ°å¯ç”¨çš„CSVæ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®\")\n        INPUT_FILE = 'data/input/åƒä»£ç”°åŒº_complete_1751433587.csv'\n        OUTPUT_FILE = 'data/output/åƒä»£ç”°åŒº_poi_notebook.csv'\n\nelse:\n    # ä½¿ç”¨é»˜è®¤æ–‡ä»¶\n    print(\"ğŸ“„ ä½¿ç”¨é»˜è®¤æ–‡ä»¶é…ç½®\")\n    INPUT_FILE = 'data/input/åƒä»£ç”°åŒº_complete_1751433587.csv'\n    OUTPUT_FILE = 'data/output/åƒä»£ç”°åŒº_poi_notebook.csv'\n\nprint(f\"\\nğŸ“‹ æœ€ç»ˆé…ç½®:\")\nprint(f\"  è¾“å…¥: {INPUT_FILE}\")\nprint(f\"  è¾“å‡º: {OUTPUT_FILE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‚ 1. æ–‡ä»¶é€‰æ‹©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ æœ€ç»ˆè¿è¡Œé…ç½®:\n",
      "  ğŸ“ max_workers: 4\n",
      "  ğŸ“ driver_pool_size: 4\n",
      "  ğŸ“ batch_size: 15\n",
      "  ğŸ“ timeout: 12\n",
      "  ğŸ“ retry_times: 2\n",
      "  ğŸ”¥ headless: True\n",
      "  ğŸ“ checkpoint_interval: 30\n",
      "  ğŸ”¥ input_file: data/input/åƒä»£ç”°åŒº_complete_1751433587.csv\n",
      "  ğŸ”¥ output_file: data\\output\\åƒä»£ç”°åŒº_1751433587_poi_notebook_20250702_1632.csv\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¥ åŠ¨æ€é…ç½® - åŸºäºæ–‡ä»¶é€‰æ‹©ç»“æœ\n",
    "CONFIG = {\n",
    "    'max_workers': 4,          # å¹¶å‘çº¿ç¨‹æ•°\n",
    "    'driver_pool_size': 4,     # WebDriveræ± å¤§å°\n",
    "    'batch_size': 15,          # æ‰¹é‡ä¿å­˜æ•°æ®é‡\n",
    "    'timeout': 12,             # é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´(ç§’)\n",
    "    'retry_times': 2,          # é‡è¯•æ¬¡æ•°\n",
    "    'headless': True,          # ğŸ”¥ å¼ºåˆ¶æ— å¤´æ¨¡å¼ - ä¸æ˜¾ç¤ºChromeçª—å£\n",
    "    'checkpoint_interval': 30, # æ£€æŸ¥ç‚¹ä¿å­˜é—´éš”\n",
    "    'input_file': INPUT_FILE,  # ğŸ”¥ ä½¿ç”¨é€‰æ‹©çš„è¾“å…¥æ–‡ä»¶\n",
    "    'output_file': OUTPUT_FILE # ğŸ”¥ ä½¿ç”¨ç”Ÿæˆçš„è¾“å‡ºæ–‡ä»¶\n",
    "}\n",
    "\n",
    "print(\"âš™ï¸ æœ€ç»ˆè¿è¡Œé…ç½®:\")\n",
    "for key, value in CONFIG.items():\n",
    "    icon = \"ğŸ”¥\" if key in ['headless', 'input_file', 'output_file'] else \"ğŸ“\"\n",
    "    print(f\"  {icon} {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‚ 2. æ•°æ®é¢„è§ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ æˆåŠŸè¯»å–è¾“å…¥æ–‡ä»¶: data/input/åƒä»£ç”°åŒº_complete_1751433587.csv\n",
      "ğŸ“Š æ•°æ®ç»Ÿè®¡:\n",
      "  æ€»è¡Œæ•°: 8,693\n",
      "  åˆ—å: ['District', 'Latitude', 'Longitude', 'Address']\n",
      "\n",
      "ğŸ“‹ æ•°æ®é¢„è§ˆ:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>District</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>åƒä»£ç”°åŒº</td>\n",
       "      <td>35.690357</td>\n",
       "      <td>139.771265</td>\n",
       "      <td>æ±äº¬éƒ½åƒä»£ç”°åŒºé›å†¶ç”º1ä¸ç›®7-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>åƒä»£ç”°åŒº</td>\n",
       "      <td>35.686220</td>\n",
       "      <td>139.734724</td>\n",
       "      <td>æ±äº¬éƒ½åƒä»£ç”°åŒºäºŒç•ªç”º10-46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>åƒä»£ç”°åŒº</td>\n",
       "      <td>35.695114</td>\n",
       "      <td>139.762307</td>\n",
       "      <td>æ±äº¬éƒ½åƒä»£ç”°åŒºç¥ç”°å°å·ç”º3ä¸ç›®6-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>åƒä»£ç”°åŒº</td>\n",
       "      <td>35.690443</td>\n",
       "      <td>139.773287</td>\n",
       "      <td>æ±äº¬éƒ½åƒä»£ç”°åŒºé›å†¶ç”º1ä¸ç›®10-7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>åƒä»£ç”°åŒº</td>\n",
       "      <td>35.703297</td>\n",
       "      <td>139.770201</td>\n",
       "      <td>æ±äº¬éƒ½åƒä»£ç”°åŒºå¤–ç¥ç”°6ä¸ç›®5-9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  District   Latitude   Longitude             Address\n",
       "0     åƒä»£ç”°åŒº  35.690357  139.771265    æ±äº¬éƒ½åƒä»£ç”°åŒºé›å†¶ç”º1ä¸ç›®7-1\n",
       "1     åƒä»£ç”°åŒº  35.686220  139.734724     æ±äº¬éƒ½åƒä»£ç”°åŒºäºŒç•ªç”º10-46\n",
       "2     åƒä»£ç”°åŒº  35.695114  139.762307  æ±äº¬éƒ½åƒä»£ç”°åŒºç¥ç”°å°å·ç”º3ä¸ç›®6-2\n",
       "3     åƒä»£ç”°åŒº  35.690443  139.773287   æ±äº¬éƒ½åƒä»£ç”°åŒºé›å†¶ç”º1ä¸ç›®10-7\n",
       "4     åƒä»£ç”°åŒº  35.703297  139.770201    æ±äº¬éƒ½åƒä»£ç”°åŒºå¤–ç¥ç”°6ä¸ç›®5-9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ æœ‰æ•ˆåœ°å€æ•°é‡: 8,693\n",
      "ğŸ“ ç¤ºä¾‹åœ°å€:\n",
      "  1. æ±äº¬éƒ½åƒä»£ç”°åŒºé›å†¶ç”º1ä¸ç›®7-1\n",
      "  2. æ±äº¬éƒ½åƒä»£ç”°åŒºäºŒç•ªç”º10-46\n",
      "  3. æ±äº¬éƒ½åƒä»£ç”°åŒºç¥ç”°å°å·ç”º3ä¸ç›®6-2\n"
     ]
    }
   ],
   "source": [
    "# è¯»å–å¹¶é¢„è§ˆè¾“å…¥æ•°æ®\n",
    "try:\n",
    "    df_input = pd.read_csv(CONFIG['input_file'])\n",
    "    print(f\"ğŸ“„ æˆåŠŸè¯»å–è¾“å…¥æ–‡ä»¶: {CONFIG['input_file']}\")\n",
    "    print(f\"ğŸ“Š æ•°æ®ç»Ÿè®¡:\")\n",
    "    print(f\"  æ€»è¡Œæ•°: {len(df_input):,}\")\n",
    "    print(f\"  åˆ—å: {list(df_input.columns)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ æ•°æ®é¢„è§ˆ:\")\n",
    "    display(df_input.head())\n",
    "    \n",
    "    # å‡†å¤‡åœ°å€åˆ—è¡¨\n",
    "    addresses = df_input['Address'].dropna().tolist()\n",
    "    print(f\"\\nğŸ“ æœ‰æ•ˆåœ°å€æ•°é‡: {len(addresses):,}\")\n",
    "    print(f\"ğŸ“ ç¤ºä¾‹åœ°å€:\")\n",
    "    for i, addr in enumerate(addresses[:3], 1):\n",
    "        print(f\"  {i}. {addr}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "    addresses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª 3. å°è§„æ¨¡æµ‹è¯• (å¯é€‰)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# æµ‹è¯•å‰5ä¸ªåœ°å€\nTEST_MODE = True  # è®¾ç½®ä¸ºFalseè·³è¿‡æµ‹è¯•ï¼Œç›´æ¥å…¨é‡è¿è¡Œ\nTEST_COUNT = 5\n\nif TEST_MODE and addresses:\n    print(f\"ğŸ§ª æµ‹è¯•æ¨¡å¼: å¤„ç†å‰ {TEST_COUNT} ä¸ªåœ°å€\")\n    \n    test_config = CONFIG.copy()\n    test_config['output_file'] = 'data/output/æµ‹è¯•ç»“æœ_poi.csv'\n    test_config['max_workers'] = 2  # æµ‹è¯•æ—¶å‡å°‘å¹¶å‘\n    \n    test_addresses = addresses[:TEST_COUNT]\n    \n    print(f\"ğŸš€ å¼€å§‹æµ‹è¯•çˆ¬å–...\")\n    start_time = time.time()\n    \n    # æ¸…ç†æµ‹è¯•æ¨¡å¼çš„æ£€æŸ¥ç‚¹\n    checkpoint_file = Path('checkpoint.json')\n    if checkpoint_file.exists():\n        checkpoint_file.unlink()\n        print(\"ğŸ§¹ å·²æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶\")\n    \n    crawler = FinalPOICrawler(test_config)\n    try:\n        crawler.process_addresses(test_addresses)\n        \n        elapsed = time.time() - start_time\n        print(f\"\\nâ±ï¸ æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {elapsed:.1f} ç§’\")\n        print(f\"ğŸ“ˆ å¹³å‡é€Ÿåº¦: {elapsed/TEST_COUNT:.1f} ç§’/ä¸ª\")\n        \n        # æŸ¥çœ‹æµ‹è¯•ç»“æœ\n        if Path(test_config['output_file']).exists():\n            test_results = pd.read_csv(test_config['output_file'])\n            print(f\"\\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡:\")\n            print(f\"  æ€»POIæ•°: {len(test_results)}\")\n            \n            if len(test_results) > 0:\n                print(f\"\\nğŸ“‹ å¢å¼ºæ•°æ®å­—æ®µé¢„è§ˆ:\")\n                display(test_results.head())\n                \n                print(f\"\\nğŸ“ˆ æ•°æ®è´¨é‡ç»Ÿè®¡:\")\n                print(f\"  æœ‰è¯„åˆ†çš„POI: {test_results['rating'].notna().sum()}\")\n                print(f\"  æœ‰è¯„è®ºæ•°çš„POI: {test_results['review_count'].notna().sum()}\")\n                print(f\"  æœ‰ç”µè¯çš„POI: {test_results['phone'].notna().sum()}\")\n                print(f\"  æœ‰ç½‘ç«™çš„POI: {test_results['website'].notna().sum()}\")\n        \n    except Exception as e:\n        print(f\"âŒ æµ‹è¯•å¤±è´¥: {e}\")\n    finally:\n        crawler.close()\n        \nelse:\n    print(\"â­ï¸ è·³è¿‡æµ‹è¯•æ¨¡å¼\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 4. å…¨é‡ç”Ÿäº§çˆ¬å–"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# ç¡®è®¤æ˜¯å¦æ‰§è¡Œå…¨é‡çˆ¬å–\nRUN_FULL_CRAWL = False  # è®¾ç½®ä¸ºTrueå¼€å§‹å…¨é‡çˆ¬å–\n\nif RUN_FULL_CRAWL and addresses:\n    print(f\"ğŸ¯ å¼€å§‹å…¨é‡POIçˆ¬å–\")\n    print(f\"ğŸ“Š é¢„è®¡å¤„ç† {len(addresses):,} ä¸ªåœ°å€\")\n    print(f\"âš¡ æ€§èƒ½è®¾ç½®: {CONFIG['max_workers']} å¹¶å‘çº¿ç¨‹ï¼Œæ— å¤´æ¨¡å¼\")\n    \n    estimated_time = len(addresses) * 2.5 / CONFIG['max_workers'] / 60  # ä¼°ç®—æ—¶é—´\n    print(f\"â±ï¸ é¢„è®¡è€—æ—¶: {estimated_time:.1f} åˆ†é’Ÿ\")\n    \n    confirmation = input(\"\\nç¡®è®¤å¼€å§‹å…¨é‡çˆ¬å–ï¼Ÿ(è¾“å…¥ 'yes' ç¡®è®¤): \")\n    \n    if confirmation.lower() == 'yes':\n        print(f\"\\nğŸš€ å¼€å§‹å…¨é‡çˆ¬å–...\")\n        start_time = time.time()\n        \n        crawler = FinalPOICrawler(CONFIG)\n        try:\n            crawler.process_addresses(addresses)\n            \n            elapsed = time.time() - start_time\n            print(f\"\\nğŸ‰ å…¨é‡çˆ¬å–å®Œæˆï¼\")\n            print(f\"â±ï¸ æ€»è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ\")\n            print(f\"ğŸ“ˆ å¹³å‡é€Ÿåº¦: {elapsed/len(addresses):.1f} ç§’/ä¸ª\")\n            \n        except KeyboardInterrupt:\n            print(\"\\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–\")\n        except Exception as e:\n            print(f\"\\nâŒ çˆ¬å–è¿‡ç¨‹å‡ºé”™: {e}\")\n        finally:\n            crawler.close()\n    else:\n        print(\"âŒ å·²å–æ¶ˆå…¨é‡çˆ¬å–\")\n        \nelse:\n    print(\"â­ï¸ è·³è¿‡å…¨é‡çˆ¬å–\")\n    print(\"ğŸ’¡ å¦‚éœ€è¿è¡Œå…¨é‡çˆ¬å–ï¼Œè¯·å°† RUN_FULL_CRAWL è®¾ç½®ä¸º True\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 5. ç»“æœåˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ†æçˆ¬å–ç»“æœ\n",
    "output_file = CONFIG['output_file']\n",
    "\n",
    "if Path(output_file).exists():\n",
    "    results = pd.read_csv(output_file)\n",
    "    \n",
    "    print(f\"ğŸ“Š ç»“æœç»Ÿè®¡æŠ¥å‘Š\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_file}\")\n",
    "    print(f\"ğŸ“ˆ æ€»POIæ•°é‡: {len(results):,}\")\n",
    "    print(f\"ğŸ¢ å”¯ä¸€å»ºç­‘ç‰©: {results['blt_name'].nunique():,}\")\n",
    "    print(f\"â­ å¹³å‡è¯„åˆ†: {results['rating'].mean():.2f}\")\n",
    "    print(f\"ğŸ’¬ å¹³å‡è¯„è®ºæ•°: {results['review_count'].mean():.0f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ æ•°æ®å®Œæ•´æ€§åˆ†æ:\")\n",
    "    completeness = {\n",
    "        'åç§°': results['name'].notna().sum(),\n",
    "        'è¯„åˆ†': results['rating'].notna().sum(), \n",
    "        'è¯„è®ºæ•°': results['review_count'].notna().sum(),\n",
    "        'ç±»åˆ«': results['category'].notna().sum(),\n",
    "        'åœ°å€': results['address'].notna().sum(),\n",
    "        'ç”µè¯': results['phone'].notna().sum(),\n",
    "        'ç½‘ç«™': results['website'].notna().sum(),\n",
    "        'è¥ä¸šæ—¶é—´': results['hours'].notna().sum(),\n",
    "        'ä»·æ ¼ç­‰çº§': results['price_level'].notna().sum()\n",
    "    }\n",
    "    \n",
    "    for field, count in completeness.items():\n",
    "        percentage = count / len(results) * 100\n",
    "        print(f\"  {field}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nğŸ† TOP 10 å»ºç­‘ç‰© (æŒ‰POIæ•°é‡):\")\n",
    "    top_buildings = results.groupby('blt_name').size().sort_values(ascending=False).head(10)\n",
    "    for i, (building, count) in enumerate(top_buildings.items(), 1):\n",
    "        print(f\"  {i:2d}. {building}: {count} ä¸ªPOI\")\n",
    "    \n",
    "    print(f\"\\nâ­ è¯„åˆ†åˆ†å¸ƒ:\")\n",
    "    rating_dist = results['rating'].value_counts().sort_index()\n",
    "    for rating, count in rating_dist.head().items():\n",
    "        print(f\"  {rating}æ˜Ÿ: {count} ä¸ªPOI\")\n",
    "    \n",
    "    print(f\"\\nğŸ“± è”ç³»æ–¹å¼ç»Ÿè®¡:\")\n",
    "    print(f\"  æœ‰ç”µè¯å·ç : {results['phone'].notna().sum()} ä¸ªPOI\")\n",
    "    print(f\"  æœ‰å®˜æ–¹ç½‘ç«™: {results['website'].notna().sum()} ä¸ªPOI\")\n",
    "    \n",
    "    print(f\"\\nğŸ’° ä»·æ ¼ç­‰çº§åˆ†å¸ƒ:\")\n",
    "    if 'price_level' in results.columns:\n",
    "        price_dist = results['price_level'].value_counts().sort_index()\n",
    "        price_labels = {1: '$ (ä¾¿å®œ)', 2: '$$ (ä¸­ç­‰)', 3: '$$$ (åè´µ)', 4: '$$$$ (æ˜‚è´µ)'}\n",
    "        for level, count in price_dist.items():\n",
    "            label = price_labels.get(level, f'{level}çº§')\n",
    "            print(f\"  {label}: {count} ä¸ªPOI\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ æ•°æ®æ ·ä¾‹:\")\n",
    "    display(results.head())\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ 6. æ•°æ®å¯¼å‡ºå’Œåå¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ¸…ç†å’Œå¯¼å‡º\n",
    "if Path(output_file).exists():\n",
    "    results = pd.read_csv(output_file)\n",
    "    \n",
    "    print(f\"ğŸ”§ æ•°æ®åå¤„ç†...\")\n",
    "    \n",
    "    # æ•°æ®æ¸…ç†\n",
    "    cleaned_results = results.copy()\n",
    "    \n",
    "    # ç§»é™¤é‡å¤POI\n",
    "    before_dedup = len(cleaned_results)\n",
    "    cleaned_results = cleaned_results.drop_duplicates(subset=['name', 'blt_name'], keep='first')\n",
    "    after_dedup = len(cleaned_results)\n",
    "    print(f\"  ğŸ—‘ï¸ ç§»é™¤é‡å¤POI: {before_dedup - after_dedup} ä¸ª\")\n",
    "    \n",
    "    # æ•°æ®ç±»å‹ä¼˜åŒ–\n",
    "    if 'rating' in cleaned_results.columns:\n",
    "        cleaned_results['rating'] = pd.to_numeric(cleaned_results['rating'], errors='coerce')\n",
    "    if 'review_count' in cleaned_results.columns:\n",
    "        cleaned_results['review_count'] = pd.to_numeric(cleaned_results['review_count'], errors='coerce')\n",
    "    if 'price_level' in cleaned_results.columns:\n",
    "        cleaned_results['price_level'] = pd.to_numeric(cleaned_results['price_level'], errors='coerce')\n",
    "    \n",
    "    # ä¿å­˜æ¸…ç†åçš„æ•°æ®\n",
    "    clean_output_file = output_file.replace('.csv', '_æ¸…ç†ç‰ˆ.csv')\n",
    "    cleaned_results.to_csv(clean_output_file, index=False, encoding='utf-8')\n",
    "    print(f\"  ğŸ’¾ æ¸…ç†ç‰ˆæ•°æ®å·²ä¿å­˜: {clean_output_file}\")\n",
    "    \n",
    "    # æŒ‰å»ºç­‘ç‰©åˆ†ç»„å¯¼å‡º\n",
    "    building_summary = cleaned_results.groupby('blt_name').agg({\n",
    "        'name': 'count',\n",
    "        'rating': 'mean',\n",
    "        'review_count': 'sum',\n",
    "        'category': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown'\n",
    "    }).round(2)\n",
    "    \n",
    "    building_summary.columns = ['POIæ•°é‡', 'å¹³å‡è¯„åˆ†', 'æ€»è¯„è®ºæ•°', 'ä¸»è¦ç±»åˆ«']\n",
    "    building_summary = building_summary.sort_values('POIæ•°é‡', ascending=False)\n",
    "    \n",
    "    summary_file = output_file.replace('.csv', '_å»ºç­‘ç‰©æ±‡æ€».csv')\n",
    "    building_summary.to_csv(summary_file, encoding='utf-8')\n",
    "    print(f\"  ğŸ“Š å»ºç­‘ç‰©æ±‡æ€»å·²ä¿å­˜: {summary_file}\")\n",
    "    \n",
    "    print(f\"\\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“ è¾“å‡ºæ–‡ä»¶:\")\n",
    "    print(f\"  - åŸå§‹æ•°æ®: {output_file}\")\n",
    "    print(f\"  - æ¸…ç†ç‰ˆæœ¬: {clean_output_file}\")\n",
    "    print(f\"  - å»ºç­‘æ±‡æ€»: {summary_file}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ æ²¡æœ‰æ‰¾åˆ°ç»“æœæ–‡ä»¶è¿›è¡Œåå¤„ç†\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” 7. æ£€æŸ¥ç‚¹çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æ£€æŸ¥ç‚¹ä¿¡æ¯\n",
    "checkpoint_file = 'checkpoint.json'\n",
    "\n",
    "if Path(checkpoint_file).exists():\n",
    "    try:\n",
    "        with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
    "            checkpoint = json.load(f)\n",
    "        \n",
    "        print(f\"ğŸ“‹ æ£€æŸ¥ç‚¹çŠ¶æ€æŠ¥å‘Š\")\n",
    "        print(f\"=\"*40)\n",
    "        print(f\"â° æœ€åæ›´æ–°: {checkpoint.get('timestamp', 'Unknown')}\")\n",
    "        print(f\"ğŸ“Š å·²å¤„ç†åœ°å€: {checkpoint.get('processed_count', 0):,}\")\n",
    "        print(f\"âœ… æˆåŠŸæ•°é‡: {checkpoint.get('success_count', 0):,}\")\n",
    "        print(f\"âŒ å¤±è´¥æ•°é‡: {len(checkpoint.get('failed_addresses', [])):,}\")\n",
    "        \n",
    "        if checkpoint.get('processed_count', 0) > 0:\n",
    "            success_rate = checkpoint.get('success_count', 0) / checkpoint.get('processed_count', 1) * 100\n",
    "            print(f\"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%\")\n",
    "        \n",
    "        failed_addresses = checkpoint.get('failed_addresses', [])\n",
    "        if failed_addresses:\n",
    "            print(f\"\\nâŒ å¤±è´¥åœ°å€æ ·ä¾‹ (å‰5ä¸ª):\")\n",
    "            for i, addr in enumerate(failed_addresses[:5], 1):\n",
    "                print(f\"  {i}. {addr}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–æ£€æŸ¥ç‚¹æ–‡ä»¶å¤±è´¥: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"â„¹ï¸ æ²¡æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## ğŸ¯ v2.0 æ›´æ–°æ€»ç»“\n\n### ğŸš€ æ€§èƒ½æå‡\n- **é™é»˜è¿è¡Œ**: å®Œå…¨å±è”½Chromeå¯åŠ¨è­¦å‘Šï¼Œçº¯å‡€è¾“å‡ºä½“éªŒ\n- **4å€å¹¶å‘**: æ™ºèƒ½WebDriveræ± ï¼ŒåŒæ—¶å¤„ç†4ä¸ªåœ°å€\n- **å†…å­˜ä¼˜åŒ–**: ç¦ç”¨å›¾ç‰‡ã€æ’ä»¶ç­‰ï¼Œä¸“æ³¨æ•°æ®æå–\n- **æ™ºèƒ½ç­‰å¾…**: æ›¿ä»£å›ºå®šå»¶æ—¶ï¼Œæé«˜çˆ¬å–æ•ˆç‡\n\n### ğŸ“Š æ•°æ®å¢å¼º  \n- **9ä¸ªå­—æ®µ**: name, rating, review_count, category, address, phone, website, hours, price_level\n- **å…ƒæ•°æ®å®Œæ•´**: å»ºç­‘ç‰©åç§°ã€åæ ‡ã€çˆ¬å–æ—¶é—´ç­‰\n- **æ•°æ®è´¨é‡**: æ”¯æŒå¤šç§POIç±»å‹ï¼Œæ™ºèƒ½å»é‡\n\n### ğŸ›¡ï¸ ç¨³å®šå¯é \n- **å¤šç­–ç•¥å®šä½**: é¡µé¢ç»“æ„å˜åŒ–ä¹Ÿèƒ½æ­£å¸¸å·¥ä½œ\n- **æ™ºèƒ½é‡è¯•**: è‡ªåŠ¨å¤„ç†ä¸´æ—¶ç½‘ç»œé”™è¯¯\n- **æ–­ç‚¹ç»­çˆ¬**: æ”¯æŒå¤§è§„æ¨¡æ•°æ®æ”¶é›†ï¼Œæ„å¤–ä¸­æ–­å¯æ¢å¤\n- **é”™è¯¯å®¹é”™**: å•ä¸ªåœ°å€å¤±è´¥ä¸å½±å“æ•´ä½“è¿›åº¦\n\n### ğŸ”§ æ˜“ç”¨æ€§æ”¹è¿›\n- **ç»Ÿä¸€å…¥å£**: ä¸€ä¸ªç¨‹åºåŒ…å«æ‰€æœ‰åŠŸèƒ½\n- **æ™ºèƒ½æ–‡ä»¶é€‰æ‹©**: è‡ªåŠ¨æ‰«æå¹¶é€‰æ‹©æœ€å¤§CSVæ–‡ä»¶\n- **æµ‹è¯•æ¨¡å¼**: å¿«é€ŸéªŒè¯ç¯å¢ƒå’Œé…ç½®\n- **è¯¦ç»†è¿›åº¦**: å®æ—¶æ˜¾ç¤ºçˆ¬å–è¿›åº¦å’Œç»Ÿè®¡ä¿¡æ¯\n\n---\n**ğŸ’¡ æç¤º**: \n- é¦–æ¬¡ä½¿ç”¨å»ºè®®å…ˆè¿è¡Œæµ‹è¯•æ¨¡å¼éªŒè¯ç¯å¢ƒ\n- å¤§è§„æ¨¡çˆ¬å–å‰å»ºè®®å°æ‰¹é‡æµ‹è¯•ç¡®è®¤æ•ˆæœ\n- ç”Ÿæˆçš„CSVæ–‡ä»¶åŒ…å«å®Œæ•´çš„9ä¸ªæ•°æ®å­—æ®µ\n- ç¨‹åºæ”¯æŒä¸­æ–­åæ–­ç‚¹ç»­çˆ¬ï¼Œé€‚åˆé•¿æ—¶é—´è¿è¡Œ",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tokyo_poi_crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}