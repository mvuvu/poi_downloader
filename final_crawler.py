#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆä¿®å¤ç‰ˆPOIçˆ¬è™« - è§£å†³æ‰€æœ‰å·²çŸ¥é—®é¢˜
"""

import os
import warnings
import sys

# å±è”½æ‰€æœ‰è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
os.environ['PYTHONWARNINGS'] = 'ignore'

import pandas as pd
import time
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
import json
from pathlib import Path
import queue

# ä½¿ç”¨å¢å¼ºç‰ˆçš„å·¥å…·å‡½æ•°
try:
    from enhanced_poi_extractor import (
        get_building_type_robust, is_building,
        safe_get_building_name, safe_get_coords, safe_get_all_poi_info
    )
    from enhanced_driver_actions import click_on_more_button, scroll_poi_section, get_poi_count_enhanced
    from simple_file_selector import get_simple_file_config
    print("âœ… å¢å¼ºç‰ˆå·¥å…·å‡½æ•°å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ å·¥å…·å‡½æ•°å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# å®Œå…¨ç¦ç”¨æ—¥å¿—
logging.basicConfig(level=logging.CRITICAL)
logging.getLogger('selenium').setLevel(logging.CRITICAL)
logging.getLogger('urllib3').setLevel(logging.CRITICAL)

class RobustWebDriverPool:
    """é«˜æ€§èƒ½WebDriveræ±  - ä¼˜åŒ–ç‰ˆ"""
    
    def __init__(self, pool_size=3, headless=True):
        self.pool_size = pool_size
        self.headless = headless
        self.available_drivers = queue.Queue()
        self.all_drivers = []
        self._initialize_pool()
    
    def _create_driver(self):
        options = webdriver.ChromeOptions()
        
        # åŸºç¡€ä¼˜åŒ–å‚æ•°
        options.add_argument('--disable-gpu')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-logging')
        options.add_argument('--disable-extensions')
        
        # æ€§èƒ½ä¼˜åŒ–å‚æ•°
        options.add_argument('--disable-images')
        options.add_argument('--disable-plugins')
        options.add_argument('--disable-plugins-discovery')
        options.add_argument('--disable-preconnect')
        options.add_argument('--disable-prefetch')
        options.add_argument('--disable-background-networking')
        options.add_argument('--disable-background-timer-throttling')
        options.add_argument('--disable-backgrounding-occluded-windows')
        options.add_argument('--disable-renderer-backgrounding')
        options.add_argument('--disable-features=TranslateUI')
        options.add_argument('--disable-default-apps')
        options.add_argument('--no-default-browser-check')
        options.add_argument('--disable-hang-monitor')
        options.add_argument('--disable-prompt-on-repost')
        
        # å†…å­˜ä¼˜åŒ–
        options.add_argument('--memory-pressure-off')
        options.add_argument('--max_old_space_size=4096')
        
        # ç½‘ç»œä¼˜åŒ–
        options.add_argument('--aggressive-cache-discard')
        options.add_argument('--disable-background-downloads')
        
        # å±è”½è­¦å‘Šå’Œé”™è¯¯ä¿¡æ¯
        options.add_argument('--log-level=3')
        options.add_argument('--silent')
        options.add_argument('--disable-dev-tools')
        options.add_argument('--disable-features=VizDisplayCompositor')
        options.add_argument('--disable-ipc-flooding-protection')
        
        # å¼ºåˆ¶æ— å¤´æ¨¡å¼ä»¥æé«˜æ€§èƒ½
        if self.headless:
            options.add_argument('--headless=new')  # ä½¿ç”¨æ–°ç‰ˆæ— å¤´æ¨¡å¼
            options.add_argument('--disable-web-security')
            options.add_argument('--allow-running-insecure-content')
            options.add_argument('--window-size=1920,1080')
        
        # ç¦ç”¨ä¸å¿…è¦çš„åŠŸèƒ½
        prefs = {
            'profile.default_content_setting_values': {
                'images': 2,  # ç¦ç”¨å›¾ç‰‡
                'plugins': 2,  # ç¦ç”¨æ’ä»¶
                'popups': 2,  # ç¦ç”¨å¼¹çª—
                'geolocation': 2,  # ç¦ç”¨åœ°ç†ä½ç½®
                'notifications': 2,  # ç¦ç”¨é€šçŸ¥
                'media_stream': 2,  # ç¦ç”¨åª’ä½“æµ
            },
            'profile.managed_default_content_settings': {
                'images': 2
            }
        }
        options.add_experimental_option('prefs', prefs)
        
        # è®¾ç½®é¡µé¢åŠ è½½ç­–ç•¥
        options.add_argument('--page-load-strategy=eager')  # ä¸ç­‰å¾…æ‰€æœ‰èµ„æºåŠ è½½å®Œæˆ
        
        try:
            # å°è¯•ä½¿ç”¨ç³»ç»ŸChrome (å±è”½Serviceæ—¥å¿—)
            service = Service()
            service.log_path = os.devnull
            driver = webdriver.Chrome(service=service, options=options)
        except:
            # å±è”½webdriver managerçš„æ—¥å¿—
            os.environ['WDM_LOG_LEVEL'] = '0'
            os.environ['WDM_PRINT_FIRST_LINE'] = 'False'
            from webdriver_manager.chrome import ChromeDriverManager
            service = Service(ChromeDriverManager().install())
            service.log_path = os.devnull
            driver = webdriver.Chrome(service=service, options=options)
        
        # è®¾ç½®é¡µé¢åŠ è½½è¶…æ—¶
        driver.set_page_load_timeout(15)
        driver.implicitly_wait(5)
        
        return driver
    
    def _initialize_pool(self):
        print(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–WebDriveræ±  (å¤§å°: {self.pool_size})...")
        for i in range(self.pool_size):
            try:
                driver = self._create_driver()
                self.available_drivers.put(driver)
                self.all_drivers.append(driver)
                print(f"  âœ… WebDriver {i+1} åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                print(f"  âŒ WebDriver {i+1} åˆ›å»ºå¤±è´¥: {e}")
    
    def get_driver(self, timeout=30):
        try:
            return self.available_drivers.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def release_driver(self, driver):
        if driver in self.all_drivers:
            self.available_drivers.put(driver)
    
    def close_all(self):
        print("ğŸ”„ æ­£åœ¨å…³é—­æ‰€æœ‰WebDriver...")
        for driver in self.all_drivers:
            try:
                driver.quit()
            except:
                pass

class FinalPOICrawler:
    """æœ€ç»ˆç‰ˆPOIçˆ¬è™«"""
    
    def __init__(self, config):
        self.config = config
        self.driver_pool = RobustWebDriverPool(config['driver_pool_size'], config['headless'])
        self.data_buffer = []
        self.processed_count = 0
        self.success_count = 0
        self.failed_addresses = []
        self._data_lock = threading.Lock()
        self.start_time = time.time()
    
    def _wait_for_page_load(self, driver, timeout=10):
        """ä¼˜åŒ–çš„é¡µé¢åŠ è½½ç­‰å¾…"""
        try:
            # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½å®Œæˆ
            WebDriverWait(driver, timeout).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )
            
            # ç­‰å¾…Google Mapsç‰¹å®šå…ƒç´ ï¼Œä½†æ—¶é—´æ›´çŸ­
            try:
                WebDriverWait(driver, 3).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
            except TimeoutException:
                pass
            
            # å‡å°‘ç­‰å¾…æ—¶é—´
            time.sleep(1)
            return True
        except TimeoutException:
            return False
    
    def _crawl_single_poi(self, address, driver):
        max_retries = self.config['retry_times']
        
        # å¤„ç†ç»“æœè®°å½•
        result_info = {
            'address': address,
            'status': 'failed',
            'reason': '',
            'place_type': '',
            'place_name': '',
            'has_expand_button': False,
            'poi_count': 0,
            'attempt_count': 0
        }
        
        for attempt in range(max_retries):
            result_info['attempt_count'] = attempt + 1
            try:
                url = f'https://www.google.com/maps/place/{address}'
                driver.get(url)
                
                if not self._wait_for_page_load(driver, self.config['timeout']):
                    result_info['reason'] = 'é¡µé¢åŠ è½½è¶…æ—¶'
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        continue
                    break
                
                # 1. é¦–å…ˆåˆ¤æ–­æ˜¯å¦ä¸ºå»ºç­‘ç‰©
                place_type = get_building_type_robust(driver)
                result_info['place_type'] = place_type or 'æœªçŸ¥'
                
                if not is_building(place_type):
                    result_info['status'] = 'skipped'
                    result_info['reason'] = f'éå»ºç­‘ç‰©({place_type})'
                    break
                
                # è·å–å»ºç­‘ç‰©åç§°
                place_name = safe_get_building_name(driver)
                result_info['place_name'] = place_name
                
                # 2. æ£€æŸ¥æ˜¯å¦æœ‰å±•å¼€æŒ‰é’®
                try:
                    more_button = WebDriverWait(driver, 3).until(
                        EC.presence_of_element_located((By.CLASS_NAME, 'M77dve'))
                    )
                    result_info['has_expand_button'] = True
                    click_on_more_button(driver)
                    
                    # è·å–å±•å¼€å‰çš„POIæ•°é‡ç”¨äºéªŒè¯
                    initial_poi_count = get_poi_count_enhanced(driver)
                    
                    # æ‰§è¡Œå¼ºåŒ–æ»šåŠ¨
                    scroll_poi_section(driver)
                    
                    # å¿«é€ŸéªŒè¯æ»šåŠ¨æ•ˆæœ
                    time.sleep(1)  # å‡å°‘ç­‰å¾…æ—¶é—´
                    df_first = safe_get_all_poi_info(driver)
                    
                    # å¦‚æœåˆå§‹æ˜¾ç¤ºå¾ˆå¤šPOIä½†å®é™…æå–å¾ˆå°‘ï¼Œå¿«é€Ÿå†æ¬¡å°è¯•
                    if initial_poi_count > 50 and len(df_first) < initial_poi_count * 0.4:  # æé«˜é˜ˆå€¼
                        # å¿«é€Ÿå†æ¬¡æ»šåŠ¨
                        scroll_poi_section(driver)
                        time.sleep(1.5)  # å‡å°‘ç­‰å¾…
                        
                except TimeoutException:
                    result_info['has_expand_button'] = False
                
                # 3. æå–POIæ•°æ®
                df = safe_get_all_poi_info(driver)
                if df.empty:
                    result_info['reason'] = 'æœªæ‰¾åˆ°POI'
                    break
                
                # è·å–åæ ‡
                lat, lng = safe_get_coords(driver.current_url)
                
                # æ·»åŠ é¢å¤–ä¿¡æ¯
                df['blt_name'] = place_name
                df['place_type'] = place_type
                df['lat'] = lat
                df['lng'] = lng
                df['crawl_time'] = pd.Timestamp.now()
                df['source_address'] = address
                
                result_info['poi_count'] = len(df)
                result_info['status'] = 'success'
                
                # è¾“å‡ºå¤„ç†ç»“æœ
                self._print_address_result(result_info)
                return df
                
            except Exception as e:
                result_info['reason'] = str(e)[:50]
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                else:
                    break
        
        # è¾“å‡ºå¤„ç†ç»“æœ
        self._print_address_result(result_info)
        return None
    
    def _print_address_result(self, result_info):
        """è¾“å‡ºå•ä¸ªåœ°å€çš„å¤„ç†ç»“æœ"""
        address = result_info['address']
        status = result_info['status']
        
        if status == 'success':
            expand_status = "å±•å¼€åæå–" if result_info['has_expand_button'] else "ç›´æ¥æå–"
            # ç®€åŒ–åœ°å€æ˜¾ç¤ºï¼ˆåªæ˜¾ç¤ºå‰30ä¸ªå­—ç¬¦ï¼‰
            short_address = address[:30] + "..." if len(address) > 30 else address
            print(f"âœ… [{short_address}] å»ºç­‘ç‰©({result_info['place_type']}) - {expand_status} - POIæ•°é‡: {result_info['poi_count']}")
        elif status == 'skipped':
            short_address = address[:30] + "..." if len(address) > 30 else address
            print(f"â­ï¸ [{short_address}] è·³è¿‡ - {result_info['reason']}")
        else:
            short_address = address[:30] + "..." if len(address) > 30 else address
            attempt_info = f" (å°è¯•{result_info['attempt_count']}æ¬¡)" if result_info['attempt_count'] > 1 else ""
            print(f"âŒ [{short_address}] å¤±è´¥{attempt_info} - {result_info['reason']}")
    
    def _process_address(self, address):
        driver = self.driver_pool.get_driver()
        if driver is None:
            print(f"âŒ æ— æ³•è·å–WebDriver: {address}")
            return None, address
        
        try:
            result = self._crawl_single_poi(address, driver)
            return result, address
        finally:
            self.driver_pool.release_driver(driver)
    
    def _save_batch_data(self, force=False):
        with self._data_lock:
            if len(self.data_buffer) >= self.config['batch_size'] or force:
                if self.data_buffer:
                    df = pd.concat(self.data_buffer, ignore_index=True)
                    
                    file_exists = Path(self.config['output_file']).exists()
                    df.to_csv(
                        self.config['output_file'], 
                        mode='a', 
                        header=not file_exists, 
                        index=False,
                        encoding='utf-8'
                    )
                    
                    print(f"ğŸ’¾ ä¿å­˜äº† {len(self.data_buffer)} æ¡æ•°æ®åˆ° {self.config['output_file']}")
                    self.data_buffer.clear()
    
    def _save_checkpoint(self, processed_addresses):
        checkpoint_data = {
            'processed_addresses': processed_addresses,
            'processed_count': self.processed_count,
            'success_count': self.success_count,
            'failed_addresses': self.failed_addresses,
            'timestamp': pd.Timestamp.now().isoformat()
        }
        
        with open('checkpoint.json', 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    
    def _load_checkpoint(self):
        try:
            with open('checkpoint.json', 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
                self.processed_count = checkpoint_data.get('processed_count', 0)
                self.success_count = checkpoint_data.get('success_count', 0)
                self.failed_addresses = checkpoint_data.get('failed_addresses', [])
                return checkpoint_data.get('processed_addresses', [])
        except FileNotFoundError:
            return []
    
    def _print_progress(self):
        elapsed = time.time() - self.start_time
        if self.processed_count > 0:
            avg_time = elapsed / self.processed_count
            remaining = len(self.remaining_addresses) - self.processed_count
            eta = remaining * avg_time / 60
            
            success_rate = self.success_count / self.processed_count * 100 if self.processed_count > 0 else 0
            
            print(f"\nğŸ“Š è¿›åº¦æŠ¥å‘Š:")
            print(f"  å¤„ç†: {self.processed_count}/{len(self.remaining_addresses)}")
            print(f"  æˆåŠŸ: {self.success_count} ({success_rate:.1f}%)")
            print(f"  å¤±è´¥: {len(self.failed_addresses)}")
            print(f"  å¹³å‡: {avg_time:.1f}s/ä¸ª")
            print(f"  é¢„è®¡å‰©ä½™: {eta:.1f}åˆ†é’Ÿ")
            print("-" * 50)
    
    def process_addresses(self, addresses):
        processed_addresses = self._load_checkpoint()
        self.remaining_addresses = [addr for addr in addresses if addr not in processed_addresses]
        
        print(f"ğŸ“Š åˆå§‹ç»Ÿè®¡:")
        print(f"  æ€»åœ°å€æ•°: {len(addresses)}")
        print(f"  å·²å¤„ç†: {len(processed_addresses)}")
        print(f"  å‰©ä½™: {len(self.remaining_addresses)}")
        print(f"  å†å²æˆåŠŸ: {self.success_count}")
        print(f"  å†å²å¤±è´¥: {len(self.failed_addresses)}")
        
        if not self.remaining_addresses:
            print("ğŸ‰ æ‰€æœ‰åœ°å€å·²å¤„ç†å®Œæˆ")
            return
        
        print(f"\nğŸš€ å¼€å§‹å¤„ç†ï¼Œä½¿ç”¨ {self.config['max_workers']} ä¸ªå¹¶å‘çº¿ç¨‹...")
        
        with ThreadPoolExecutor(max_workers=self.config['max_workers']) as executor:
            future_to_address = {
                executor.submit(self._process_address, addr): addr 
                for addr in self.remaining_addresses
            }
            
            for future in as_completed(future_to_address):
                original_address = future_to_address[future]
                try:
                    result, address = future.result()
                    
                    if result is not None:
                        with self._data_lock:
                            self.data_buffer.append(result)
                        self.success_count += 1
                    else:
                        self.failed_addresses.append(address)
                    
                    self.processed_count += 1
                    processed_addresses.append(address)
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    if self.processed_count % 5 == 0:
                        self._print_progress()
                    
                    self._save_batch_data()
                    
                    if self.processed_count % self.config['checkpoint_interval'] == 0:
                        self._save_checkpoint(processed_addresses)
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†åœ°å€ {original_address} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    self.failed_addresses.append(original_address)
        
        self._save_batch_data(force=True)
        self._save_checkpoint(processed_addresses)
        
        elapsed = time.time() - self.start_time
        success_rate = self.success_count / (self.success_count + len(self.failed_addresses)) * 100
        
        print(f"\nğŸ¯ æœ€ç»ˆæŠ¥å‘Š:")
        print(f"  æ€»è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
        print(f"  æˆåŠŸ: {self.success_count}")
        print(f"  å¤±è´¥: {len(self.failed_addresses)}")
        print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"  å¹³å‡é€Ÿåº¦: {elapsed/len(self.remaining_addresses):.1f}ç§’/ä¸ª")
    
    def close(self):
        self.driver_pool.close_all()

def show_startup_menu():
    """æ˜¾ç¤ºå¯åŠ¨èœå•"""
    print("ğŸ¯ POIçˆ¬è™«å¯åŠ¨å™¨")
    print("=" * 40)
    print("ğŸ“‹ å¯ç”¨æ¨¡å¼:")
    print("1. ğŸš€ è‡ªåŠ¨çˆ¬å– (è‡ªåŠ¨é€‰æ‹©æœ€å¤§CSVæ–‡ä»¶)")
    print("2. ğŸ§ª æµ‹è¯•æ¨¡å¼ (å‰5ä¸ªåœ°å€)")
    print("3. ğŸ–¥ï¸ æ˜¾ç¤ºChromeçª—å£æ¨¡å¼")
    print("4. ğŸ“„ å¸®åŠ©ä¿¡æ¯")
    print("5. ğŸ”§ é«˜çº§å‚æ•°æ¨¡å¼")
    
    print("\nğŸ’¡ å¿«é€Ÿä½¿ç”¨:")
    print("  python final_crawler.py        # æ˜¾ç¤ºæ­¤èœå•")
    print("  python final_crawler.py --test # ç›´æ¥æµ‹è¯•æ¨¡å¼")
    print("  python final_crawler.py --help # æŸ¥çœ‹æ‰€æœ‰å‚æ•°")

def show_help_info():
    """æ˜¾ç¤ºè¯¦ç»†å¸®åŠ©ä¿¡æ¯"""
    print("\nğŸ“„ è¯¦ç»†ä½¿ç”¨è¯´æ˜:")
    print("=" * 50)
    print("ğŸ”¥ è‡ªåŠ¨æ–‡ä»¶é€‰æ‹©:")
    print("  ç¨‹åºä¼šè‡ªåŠ¨æ‰«æ data/input/ ç›®å½•")
    print("  é€‰æ‹©è¡Œæ•°æœ€å¤šçš„æœ‰æ•ˆCSVæ–‡ä»¶")
    print("  ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºæ–‡ä»¶")
    print()
    print("ğŸ“‹ å‘½ä»¤è¡Œå‚æ•°:")
    print("  --test          æµ‹è¯•æ¨¡å¼(å‰5ä¸ªåœ°å€)")
    print("  --no-headless   æ˜¾ç¤ºChromeçª—å£")
    print("  --workers N     è®¾ç½®å¹¶å‘çº¿ç¨‹æ•°")
    print("  --input FILE    æŒ‡å®šè¾“å…¥æ–‡ä»¶")
    print("  --output FILE   æŒ‡å®šè¾“å‡ºæ–‡ä»¶")
    print()
    print("ğŸ“‚ æ–‡ä»¶è¦æ±‚:")
    print("  - CSVæ ¼å¼")
    print("  - å¿…é¡»åŒ…å« 'Address' åˆ—")
    print("  - æ”¾åœ¨ data/input/ ç›®å½•ä¸‹")
    print()
    print("ğŸ¯ ç¤ºä¾‹å‘½ä»¤:")
    print("  python final_crawler.py                    # æ˜¾ç¤ºèœå•")
    print("  python final_crawler.py --test             # æµ‹è¯•å‰5ä¸ªåœ°å€")
    print("  python final_crawler.py --workers 2        # ä½¿ç”¨2ä¸ªçº¿ç¨‹")
    print("  python final_crawler.py --no-headless      # æ˜¾ç¤ºChrome")

def main():
    """ä¸»å‡½æ•° - ç»Ÿä¸€å¯åŠ¨å…¥å£"""
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°æ”¯æŒ
    import argparse
    parser = argparse.ArgumentParser(description='POIçˆ¬è™«å·¥å…·', add_help=False)
    parser.add_argument('mode', nargs='?', help='å¯åŠ¨æ¨¡å¼ (1-5)')
    parser.add_argument('--input', '-i', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', '-o', help='è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„') 
    parser.add_argument('--workers', '-w', type=int, default=4, help='å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 4)')
    parser.add_argument('--headless', action='store_true', help='æ— å¤´æ¨¡å¼è¿è¡Œ')
    parser.add_argument('--no-headless', action='store_true', help='æ˜¾ç¤ºChromeçª—å£')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ (å¤„ç†å‰5ä¸ªåœ°å€)')
    parser.add_argument('--help', '-h', action='store_true', help='æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯')
    args = parser.parse_args()
    
    # å¤„ç†å¸®åŠ©ä¿¡æ¯
    if args.help:
        parser.print_help()
        show_help_info()
        return
    
    # å¤„ç†ç›´æ¥å‘½ä»¤è¡Œå‚æ•°
    if args.test or args.input or args.no_headless:
        run_crawler_direct(args)
        return
    
    # å¤„ç†èœå•æ¨¡å¼
    if args.mode:
        run_menu_mode(args.mode)
        return
    
    # æ˜¾ç¤ºå¯åŠ¨èœå•
    show_startup_menu()
    try:
        choice = input("\nè¯·é€‰æ‹©æ¨¡å¼ (1-5): ").strip()
        if choice:
            run_menu_mode(choice)
    except (KeyboardInterrupt, EOFError):
        print("\nâŒ å·²å–æ¶ˆ")

def run_menu_mode(mode):
    """æ ¹æ®èœå•é€‰æ‹©è¿è¡Œ"""
    if mode == '1':
        # è‡ªåŠ¨çˆ¬å–æ¨¡å¼
        print("\nğŸš€ å¯åŠ¨è‡ªåŠ¨çˆ¬å–æ¨¡å¼...")
        run_crawler_with_config(test_mode=False, headless=True)
        
    elif mode == '2':
        # æµ‹è¯•æ¨¡å¼
        print("\nğŸ§ª å¯åŠ¨æµ‹è¯•æ¨¡å¼...")
        run_crawler_with_config(test_mode=True, headless=True)
        
    elif mode == '3':
        # æ˜¾ç¤ºChromeçª—å£æ¨¡å¼
        print("\nğŸ–¥ï¸ å¯åŠ¨æ˜¾ç¤ºChromeçª—å£æ¨¡å¼...")
        run_crawler_with_config(test_mode=False, headless=False)
        
    elif mode == '4':
        # å¸®åŠ©ä¿¡æ¯
        show_help_info()
        
    elif mode == '5':
        # é«˜çº§å‚æ•°æ¨¡å¼
        print("\nğŸ”§ é«˜çº§å‚æ•°æ¨¡å¼:")
        print("ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¡Œå‚æ•°:")
        print("  python final_crawler.py --input your_file.csv --workers 2")
        print("  python final_crawler.py --test --no-headless")
        
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·ä½¿ç”¨ 1-5")

def run_crawler_direct(args):
    """ç›´æ¥è¿è¡Œçˆ¬è™«ï¼ˆå‘½ä»¤è¡Œå‚æ•°æ¨¡å¼ï¼‰"""
    print("ğŸ¯ å¢å¼ºç‰ˆPOIçˆ¬è™«")
    print("=" * 60)
    
    # ç®€å•æ–‡ä»¶é€‰æ‹©é€»è¾‘
    if args.input and args.output:
        input_file = args.input
        output_file = args.output
        print(f"ğŸ“„ ä½¿ç”¨æŒ‡å®šæ–‡ä»¶:")
        print(f"  ğŸ“¥ è¾“å…¥: {input_file}")
        print(f"  ğŸ“¤ è¾“å‡º: {output_file}")
    else:
        print("ğŸ“‚ è‡ªåŠ¨é€‰æ‹©æ–‡ä»¶...")
        file_config = get_simple_file_config(suffix="test" if args.test else "poi_enhanced")
        
        if not file_config['has_input']:
            print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶")
            print("ğŸ’¡ è¯·å°†CSVæ–‡ä»¶æ”¾å…¥ data/input/ ç›®å½•")
            return
        
        input_file = file_config['input_file']
        output_file = file_config['output_file']
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
    headless = True  # é»˜è®¤æ— å¤´æ¨¡å¼
    if args.no_headless:
        headless = False
    elif args.headless:
        headless = True
    
    # è¿è¡Œçˆ¬è™«
    run_crawler_core(input_file, output_file, args.workers, headless, args.test)

def run_crawler_with_config(test_mode=False, headless=True, workers=4):
    """ä½¿ç”¨é¢„è®¾é…ç½®è¿è¡Œçˆ¬è™«"""
    print("ğŸ“‚ è‡ªåŠ¨é€‰æ‹©æ–‡ä»¶...")
    file_config = get_simple_file_config(suffix="test" if test_mode else "poi_enhanced")
    
    if not file_config['has_input']:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¾“å…¥æ–‡ä»¶")
        print("ğŸ’¡ è¯·å°†CSVæ–‡ä»¶æ”¾å…¥ data/input/ ç›®å½•")
        return
    
    # æµ‹è¯•æ¨¡å¼æ¸…ç†æ£€æŸ¥ç‚¹
    if test_mode:
        checkpoint_file = Path('checkpoint.json')
        if checkpoint_file.exists():
            checkpoint_file.unlink()
            print("ğŸ§¹ å·²æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶")
    
    run_crawler_core(
        file_config['input_file'], 
        file_config['output_file'], 
        workers, 
        headless, 
        test_mode
    )

def run_crawler_core(input_file, output_file, workers, headless, test_mode):
    """çˆ¬è™«æ ¸å¿ƒè¿è¡Œé€»è¾‘"""
    config = {
        'max_workers': workers,
        'driver_pool_size': workers,
        'batch_size': 15,
        'timeout': 12,
        'retry_times': 2,
        'headless': headless,
        'checkpoint_interval': 30,
        'input_file': input_file,
        'output_file': output_file
    }
    
    print(f"\nâš™ï¸ è¿è¡Œé…ç½®:")
    print(f"  ğŸ“¥ è¾“å…¥æ–‡ä»¶: {config['input_file']}")
    print(f"  ğŸ“¤ è¾“å‡ºæ–‡ä»¶: {config['output_file']}")
    print(f"  ğŸ”§ å¹¶å‘çº¿ç¨‹: {config['max_workers']}")
    print(f"  {'ğŸ”¥' if config['headless'] else 'ğŸ–¥ï¸'} è¿è¡Œæ¨¡å¼: {'æ— å¤´æ¨¡å¼ (åå°)' if config['headless'] else 'æ˜¾ç¤ºChromeçª—å£'}")
    
    try:
        # éªŒè¯è¾“å…¥æ–‡ä»¶
        df_input = pd.read_csv(config['input_file'])
        print(f"\nğŸ“Š è¾“å…¥æ–‡ä»¶ç»Ÿè®¡:")
        print(f"  æ•°æ®è¡Œæ•°: {len(df_input):,}")
        print(f"  åˆ—å: {list(df_input.columns)}")
        
        # è·å–åœ°å€åˆ—è¡¨
        if 'Address' in df_input.columns:
            addresses = df_input['Address'].dropna().tolist()
        else:
            print("âš ï¸ æœªæ‰¾åˆ°'Address'åˆ—ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€åˆ—")
            addresses = df_input.iloc[:, 0].dropna().tolist()
        
        # æµ‹è¯•æ¨¡å¼å¤„ç†
        if test_mode:
            test_count = min(5, len(addresses))
            addresses = addresses[:test_count]
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: åªå¤„ç†å‰ {test_count} ä¸ªåœ°å€")
        
        print(f"  å¤„ç†åœ°å€æ•°: {len(addresses):,} ä¸ª")
        
        if len(addresses) == 0:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆåœ°å€æ•°æ®")
            return
        
        # é¢„ä¼°æ—¶é—´
        estimated_time = len(addresses) * 2.5 / config['max_workers'] / 60
        print(f"  â±ï¸ é¢„è®¡è€—æ—¶: {estimated_time:.1f} åˆ†é’Ÿ")
        
        # ç¡®è®¤æ‰§è¡Œ (æµ‹è¯•æ¨¡å¼è·³è¿‡ç¡®è®¤)
        if len(addresses) > 100 and not test_mode:
            print(f"\nâš ï¸ å°†è¦å¤„ç† {len(addresses):,} ä¸ªåœ°å€ï¼Œè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
            try:
                confirm = input("ç¡®è®¤ç»§ç»­ï¼Ÿ(y/n): ").lower()
                if confirm != 'y':
                    print("âŒ å·²å–æ¶ˆæ‰§è¡Œ")
                    return
            except EOFError:
                print("ğŸ¤– éäº¤äº’ç¯å¢ƒï¼Œè‡ªåŠ¨ç»§ç»­æ‰§è¡Œ")
        
        print(f"\nğŸš€ å¼€å§‹çˆ¬å–...")
        start_time = time.time()
        
        # åˆ›å»ºçˆ¬è™«å¹¶è¿è¡Œ
        crawler = FinalPOICrawler(config)
        try:
            crawler.process_addresses(addresses)
            
            elapsed_time = time.time() - start_time
            print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
            print(f"â±ï¸ æ€»è€—æ—¶: {elapsed_time/60:.1f} åˆ†é’Ÿ")
            print(f"ğŸ“ˆ å¹³å‡é€Ÿåº¦: {elapsed_time/len(addresses):.1f} ç§’/åœ°å€")
            print(f"ğŸ“ ç»“æœæ–‡ä»¶: {config['output_file']}")
            
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        except Exception as e:
            print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            crawler.close()
            
    except FileNotFoundError:
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {config['input_file']}")
    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")

if __name__ == "__main__":
    main()