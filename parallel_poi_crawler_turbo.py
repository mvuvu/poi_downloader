#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import asyncio
import logging
import multiprocessing as mp
import os
import time
import pandas as pd
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import concurrent.futures
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import WebDriverException, TimeoutException
import sys
import json
import argparse
import threading
from queue import Queue, Empty
import signal
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import random
from threading import Lock, RLock
from collections import deque
import gc
import psutil  # ç”¨äºå†…å­˜ç›‘æ§
from multiprocessing import Process, Queue as MPQueue, Event as MPEvent
import queue

from info_tool import get_building_type, get_building_name, get_all_poi_info, get_coords, wait_for_coords_url
from driver_action import click_on_more_button, scroll_poi_section

# çº¿ç¨‹æœ¬åœ°ç»Ÿè®¡ç®¡ç†
thread_local_stats = threading.local()

class ChromeDriverPool:
    """Chromeé©±åŠ¨æ± ç®¡ç†å™¨ - çº¿ç¨‹å®‰å…¨ï¼Œé¢„åˆ†é…æœºåˆ¶"""
    def __init__(self, max_drivers=30):
        self.max_drivers = max_drivers
        self.pool = deque()
        self.lock = RLock()
        self.total_created = 0
        self.reserved_drivers = {}  # worker_id -> driver é¢„åˆ†é…æ˜ å°„
        self.driver_usage_count = {}  # driver -> usage_count ä½¿ç”¨è®¡æ•°
        self.driver_max_tasks = 100  # æ¯ä¸ªdriveræœ€å¤šå¤„ç†100ä¸ªä»»åŠ¡åé‡å¯
        
    def create_optimized_driver(self):
        """åˆ›å»ºé«˜æ€§èƒ½ç¨³å®šçš„Chromeå®ä¾‹"""
        # è®¾ç½®ç¯å¢ƒå˜é‡ç¦ç”¨GPU
        import os
        os.environ['LIBGL_ALWAYS_SOFTWARE'] = '1'  # å¼ºåˆ¶ä½¿ç”¨è½¯ä»¶æ¸²æŸ“
        
        options = webdriver.ChromeOptions()
        
        # é«˜æ€§èƒ½ç¨³å®šé…ç½®
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        
        # å¼ºåŠ›ç¦ç”¨GPUç›¸å…³åŠŸèƒ½
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-gpu-sandbox')
        options.add_argument('--disable-gpu-rasterization')
        options.add_argument('--disable-accelerated-2d-canvas')
        options.add_argument('--disable-accelerated-video-decode')
        options.add_argument('--disable-accelerated-video-encode')
        options.add_argument('--disable-gpu-memory-buffer-video-frames')
        options.add_argument('--disable-gpu-compositing')
        options.add_argument('--use-gl=swiftshader')  # ä½¿ç”¨SwiftShaderè½¯ä»¶æ¸²æŸ“
        options.add_argument('--enable-software-rasterizer')  # å¼ºåˆ¶ä½¿ç”¨è½¯ä»¶æ¸²æŸ“
        
        # ç½‘ç»œå’Œæ€§èƒ½ä¼˜åŒ–
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-default-apps')
        options.add_argument('--disable-sync')
        options.add_argument('--disable-translate')
        options.add_argument('--no-first-run')
        options.add_argument('--disable-popup-blocking')
        options.add_argument('--disable-background-networking')
        options.add_argument('--disable-background-timer-throttling')
        options.add_argument('--disable-renderer-backgrounding')
        options.add_argument('--disable-backgrounding-occluded-windows')
        options.add_argument('--disable-client-side-phishing-detection')
        
        # å†…å­˜ä¼˜åŒ– - ä¸¥æ ¼æ§åˆ¶æ¯ä¸ªChromeå®ä¾‹å†…å­˜ä½¿ç”¨
        options.add_argument('--memory-pressure-off')
        options.add_argument('--max-memory-in-mb=350')  # ä¿å®ˆçš„350MBé™åˆ¶ï¼Œ30ä¸ªå®ä¾‹çº¦10.5GB
        options.add_argument('--aggressive-cache-discard')
        options.add_argument('--disable-background-timer-throttling')
        options.add_argument('--disable-ipc-flooding-protection')
        options.add_argument('--max-old-space-size=256')  # V8å¼•æ“å†…å­˜é™åˆ¶
        # VizDisplayCompositorå·²è¢«æ–°çš„GPUç¦ç”¨å‚æ•°è¦†ç›–ï¼Œç§»é™¤é¿å…å†²çª
        options.add_argument('--disable-canvas-aa')  # å‡å°‘ç”»å¸ƒå†…å­˜
        options.add_argument('--disable-2d-canvas-clip-aa')  # å‡å°‘2Dç”»å¸ƒå†…å­˜
        
        # ç¦ç”¨èµ„æºåŠ è½½æé«˜é€Ÿåº¦
        prefs = {
            'profile.default_content_setting_values': {
                'images': 2,
                'plugins': 2,
                'popups': 2,
                'geolocation': 2,
                'notifications': 2,
                'media_stream': 2,
            }
        }
        options.add_experimental_option('prefs', prefs)
        options.page_load_strategy = 'eager'
        
        # é™é»˜æœåŠ¡
        service = Service(
            ChromeDriverManager().install(),
            log_path='NUL' if os.name == 'nt' else '/dev/null',
            service_args=['--silent']
        )

        try:
            driver = webdriver.Chrome(service=service, options=options)
            driver.set_page_load_timeout(8)
            driver.implicitly_wait(1.5)
            
            # æµ‹è¯•Chromeå®ä¾‹æ˜¯å¦å¯ç”¨
            driver.get("data:,")  # åŠ è½½ç©ºé¡µé¢æµ‹è¯•
            
            return driver
        except Exception as e:
            pass  # é™é»˜å¤„ç†é©±åŠ¨åˆ›å»ºå¤±è´¥
            raise e
    
    def get_driver(self):
        """è·å–ä¸€ä¸ªå¯ç”¨çš„é©±åŠ¨å®ä¾‹"""
        with self.lock:
            # å°è¯•ä»æ± ä¸­è·å–å¯ç”¨çš„é©±åŠ¨
            while self.pool:
                driver = self.pool.popleft()
                try:
                    # ç®€å•çš„å¥åº·æ£€æŸ¥
                    driver.window_handles
                    pass  # é™é»˜è·å–é©±åŠ¨
                    return driver
                except:
                    # é©±åŠ¨å·²æŸåï¼Œå°è¯•å…³é—­
                    pass  # é™é»˜å¤„ç†æŸåé©±åŠ¨
                    try:
                        driver.quit()
                    except:
                        pass
            
            # æ± ä¸­æ²¡æœ‰å¯ç”¨é©±åŠ¨ï¼Œåˆ›å»ºæ–°çš„ï¼ˆå¦‚æœæœªè¾¾åˆ°ä¸Šé™ï¼‰
            if self.total_created < self.max_drivers:
                driver = self.create_optimized_driver()
                self.total_created += 1
                print(f"åˆ›å»ºæ–°Chromeå®ä¾‹ #{self.total_created}")
                return driver
            
            # è¾¾åˆ°ä¸Šé™ï¼Œç­‰å¾…å…¶ä»–çº¿ç¨‹é‡Šæ”¾é©±åŠ¨
            pass  # é™é»˜å¤„ç†é©±åŠ¨æ± å·²æ»¡
            return None
    
    def increment_driver_usage(self, driver):
        """å¢åŠ driverä½¿ç”¨è®¡æ•°ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦é‡å¯"""
        with self.lock:
            if driver in self.driver_usage_count:
                self.driver_usage_count[driver] += 1
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ç”Ÿå‘½å‘¨æœŸ
                if self.driver_usage_count[driver] >= self.driver_max_tasks:
                    print(f"Driverè¾¾åˆ°ç”Ÿå‘½å‘¨æœŸé™åˆ¶({self.driver_max_tasks}ä¸ªä»»åŠ¡)ï¼Œå‡†å¤‡é‡å¯")
                    return True  # éœ€è¦é‡å¯
            else:
                self.driver_usage_count[driver] = 1
            return False  # ä¸éœ€è¦é‡å¯
    
    def restart_driver(self, old_driver, worker_id=None):
        """é‡å¯driverï¼ˆç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼‰"""
        with self.lock:
            # å¼ºåˆ¶é‡Šæ”¾æ—§driver
            self._force_quit_driver_internal(old_driver)
            
            # ä»è®¡æ•°ä¸­ç§»é™¤
            if old_driver in self.driver_usage_count:
                del self.driver_usage_count[old_driver]
            
            # ä»é¢„åˆ†é…æ˜ å°„ä¸­ç§»é™¤
            if worker_id and worker_id in self.reserved_drivers:
                del self.reserved_drivers[worker_id]
            
            # åˆ›å»ºæ–°çš„driver
            if self.total_created < self.max_drivers:
                new_driver = self.create_optimized_driver()
                if worker_id:
                    self.reserved_drivers[worker_id] = new_driver
                    self.driver_usage_count[new_driver] = 0
                print(f"Driveré‡å¯å®Œæˆ (Worker: {worker_id})")
                return new_driver
            return None
    
    def return_driver(self, driver):
        """å½’è¿˜é©±åŠ¨åˆ°æ± ä¸­"""
        if driver is None:
            return
            
        with self.lock:
            try:
                # ç®€åŒ–æ¸…ç†æ“ä½œï¼Œå¿«é€Ÿå½’è¿˜
                driver.get("data:,")  # å¯¼èˆªåˆ°ç©ºé¡µé¢æ¸…ç†çŠ¶æ€
                self.pool.append(driver)
                
                # å®šæœŸæ£€æŸ¥Chromeè¿›ç¨‹æ•°é‡
                if len(self.pool) % 10 == 0:  # æ¯å½’è¿˜10ä¸ªé©±åŠ¨æ£€æŸ¥ä¸€æ¬¡
                    self._check_chrome_process_count()
                    
            except Exception as e:
                # æ¸…ç†å¤±è´¥ï¼Œä¸å½’è¿˜åˆ°æ± ä¸­
                self._force_quit_driver(driver)
    
    def release_driver(self, driver):
        """å¼ºåˆ¶é‡Šæ”¾å¹¶é”€æ¯driver - ç¡®ä¿å½»åº•æ¸…ç†"""
        if driver is None:
            return
            
        with self.lock:
            # ä»æ± ä¸­ç§»é™¤æ­¤driverï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            try:
                while driver in self.pool:
                    self.pool.remove(driver)
            except ValueError:
                pass  # driverä¸åœ¨æ± ä¸­
            
            # å¼ºåˆ¶é”€æ¯driver
            try:
                driver.quit()
                print(f"âœ… å¼ºåˆ¶é”€æ¯driveræˆåŠŸ")
            except Exception as e:
                print(f"âŒ æ— æ³•å…³é—­driver: {e}")
                # å³ä½¿quitå¤±è´¥ï¼Œä¹Ÿå°è¯•å¼ºåˆ¶åœæ­¢service
                try:
                    if hasattr(driver, 'service') and driver.service:
                        driver.service.stop()
                        print(f"ğŸ”§ å¼ºåˆ¶åœæ­¢driver service")
                except Exception as e2:
                    print(f"âŒ æ— æ³•åœæ­¢driver service: {e2}")
            
            # æ›´æ–°ç»Ÿè®¡
            self.total_created = max(0, self.total_created - 1)
    
    def cleanup_all(self):
        """æ¸…ç†æ‰€æœ‰é©±åŠ¨"""
        with self.lock:
            # æ¸…ç†æ± ä¸­çš„é©±åŠ¨
            while self.pool:
                driver = self.pool.popleft()
                try:
                    driver.quit()
                except:
                    pass
            
            print(f"Chromeé©±åŠ¨æ± å·²æ¸…ç†ï¼Œå…±åˆ›å»ºäº† {self.total_created} ä¸ªå®ä¾‹")
            
            # å¼ºåˆ¶æ¸…ç†æ®‹ç•™çš„Chromeè¿›ç¨‹
            self._force_cleanup_chrome_processes()
    
    def _force_quit_driver(self, driver):
        """å¼ºåˆ¶å…³é—­Chromeé©±åŠ¨"""
        try:
            # é¦–å…ˆå°è¯•æ­£å¸¸å…³é—­
            driver.quit()
        except:
            pass
        
        try:
            # å¦‚æœæœ‰serviceï¼Œå¼ºåˆ¶åœæ­¢
            if hasattr(driver, 'service') and driver.service:
                driver.service.stop()
        except:
            pass
    
    def _force_quit_driver_internal(self, driver):
        """å†…éƒ¨ä½¿ç”¨çš„å¼ºåˆ¶å…³é—­driverï¼ˆæ— é”ç‰ˆæœ¬ï¼‰"""
        try:
            driver.quit()
            print(f"âœ… å¼ºåˆ¶å…³é—­driver")
        except Exception as e:
            print(f"âŒ å…³é—­driverå¤±è´¥: {e}")
            # å³ä½¿quitå¤±è´¥ï¼Œä¹Ÿå°è¯•å¼ºåˆ¶åœæ­¢service
            try:
                if hasattr(driver, 'service') and driver.service:
                    driver.service.stop()
                    print(f"ğŸ”§ å¼ºåˆ¶åœæ­¢driver service")
            except Exception as e2:
                print(f"âŒ åœæ­¢driver serviceå¤±è´¥: {e2}")
    
    def _check_chrome_process_count(self):
        """æ£€æŸ¥Chromeè¿›ç¨‹æ•°é‡ï¼Œè¶…è¿‡é˜ˆå€¼æ—¶æ¸…ç†"""
        try:
            import psutil
            chrome_count = 0
            for proc in psutil.process_iter(['name', 'cmdline']):
                try:
                    proc_name = proc.info['name'].lower()
                    if 'chrome' in proc_name or 'chromium' in proc_name:
                        cmdline = ' '.join(proc.info.get('cmdline', []))
                        if '--headless' in cmdline or '--no-sandbox' in cmdline:
                            chrome_count += 1
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            
            if chrome_count > 50:  # è¶…è¿‡50ä¸ªChromeè¿›ç¨‹æ—¶è­¦å‘Šå¹¶æ¸…ç†
                print(f"âš ï¸  Chromeè¿›ç¨‹è¿‡å¤š: {chrome_count}ä¸ªï¼Œå¼€å§‹è‡ªåŠ¨æ¸…ç†...")
                self._force_cleanup_chrome_processes()
                
        except Exception:
            pass  # é™é»˜å¤„ç†æ£€æŸ¥é”™è¯¯
    
    def _force_cleanup_chrome_processes(self):
        """å¼ºåˆ¶æ¸…ç†æ®‹ç•™çš„Chromeè¿›ç¨‹"""
        try:
            import psutil
            chrome_procs = []
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    proc_name = proc.info['name'].lower()
                    if 'chrome' in proc_name or 'chromium' in proc_name:
                        # åªæ¸…ç†çˆ¬è™«ç›¸å…³çš„Chromeè¿›ç¨‹ï¼Œé¿å…è¯¯æ€ç”¨æˆ·æµè§ˆå™¨
                        cmdline = ' '.join(proc.info.get('cmdline', []))
                        if '--headless' in cmdline or '--no-sandbox' in cmdline:
                            chrome_procs.append(proc)
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            
            if len(chrome_procs) > 30:  # è¶…è¿‡30ä¸ªheadless Chromeè¿›ç¨‹æ—¶æ¸…ç†
                print(f"ğŸ§¹ å‘ç° {len(chrome_procs)} ä¸ªheadless Chromeè¿›ç¨‹ï¼Œå¼€å§‹æ¸…ç†...")
                killed_count = 0
                for proc in chrome_procs:
                    try:
                        proc.terminate()
                        killed_count += 1
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        continue
                
                # ç­‰å¾…è¿›ç¨‹æ­£å¸¸é€€å‡º
                time.sleep(2)
                
                # å¼ºåˆ¶æ€æ­»ä»ç„¶å­˜åœ¨çš„è¿›ç¨‹
                for proc in chrome_procs:
                    try:
                        if proc.is_running():
                            proc.kill()
                    except (psutil.NoSuchProcess, psutil.AccessDenied):
                        continue
                
                print(f"âœ… æ¸…ç†äº† {killed_count} ä¸ªChromeè¿›ç¨‹")
                
        except Exception as e:
            print(f"Chromeè¿›ç¨‹æ¸…ç†å¤±è´¥: {e}")


class TurboTaskScheduler:
    """Turboä»»åŠ¡è°ƒåº¦å™¨ - é«˜å¹¶å‘ä»»åŠ¡åˆ†å‘"""
    def __init__(self, max_threads=36):
        self.max_threads = max_threads
        # é«˜æ•ˆé˜Ÿåˆ—é…ç½® - æ ¹æ®æœ€ä½³å®è·µè°ƒæ•´é˜Ÿåˆ—å¤§å°
        queue_size = min(300, max_threads * 8)  # é™åˆ¶é˜Ÿåˆ—æœ€å¤§300ä¸ªä»»åŠ¡
        self.task_queue = Queue(maxsize=queue_size)  # FIFOé˜Ÿåˆ—ç¡®ä¿å…¬å¹³å¤„ç†
        self.result_queue = Queue()
        self.pending_tasks = deque()
        self.active_threads = 0
        self.completed_count = 0
        self.lock = Lock()
        self.stop_event = threading.Event()
        
    def add_tasks(self, tasks):
        """æ‰¹é‡æ·»åŠ ä»»åŠ¡"""
        self.pending_tasks.extend(tasks)
        
    def feed_tasks(self):
        """æŒç»­å‘é˜Ÿåˆ—ä¸­æ·»åŠ ä»»åŠ¡"""
        while self.pending_tasks and not self.stop_event.is_set():
            try:
                # å°è¯•å‘é˜Ÿåˆ—æ·»åŠ ä»»åŠ¡ï¼ˆéé˜»å¡ï¼‰
                if not self.task_queue.full():
                    for _ in range(min(10, len(self.pending_tasks))):
                        if self.pending_tasks:
                            task = self.pending_tasks.popleft()
                            self.task_queue.put(task, timeout=0.1)
                        else:
                            break
                time.sleep(0.01)  # çŸ­æš‚ä¼‘æ¯
            except:
                time.sleep(0.1)
    
    def get_task(self):
        """è·å–ä¸€ä¸ªä»»åŠ¡"""
        try:
            return self.task_queue.get(timeout=0.5)
        except Empty:
            return None
    
    def put_result(self, result):
        """æäº¤ç»“æœ"""
        self.result_queue.put(result)
        with self.lock:
            self.completed_count += 1
    
    def get_result(self):
        """è·å–ç»“æœ"""
        try:
            return self.result_queue.get(timeout=0.1)
        except Empty:
            return None
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨"""
        self.stop_event.set()


class TurboWorker(threading.Thread):
    """Turboå·¥ä½œçº¿ç¨‹"""
    def __init__(self, worker_id, driver_pool, task_scheduler, crawler):
        super().__init__(daemon=True)
        self.worker_id = worker_id
        self.driver_pool = driver_pool
        self.task_scheduler = task_scheduler
        self.crawler = crawler
        self.processed_count = 0
        self.current_driver = None
        self.stats = {'success': 0, 'errors': 0}  # çº¿ç¨‹æœ¬åœ°ç»Ÿè®¡
        
    def run(self):
        """çº¿ç¨‹ä¸»å¾ªç¯"""
        print(f"Worker {self.worker_id} å¯åŠ¨")
        
        while not self.task_scheduler.stop_event.is_set():
            # è·å–ä»»åŠ¡
            task = self.task_scheduler.get_task()
            if task is None:
                # æ²¡æœ‰ä»»åŠ¡ï¼ŒçŸ­æš‚ä¼‘æ¯åç»§ç»­
                time.sleep(0.01)
                continue
            
            # è·å–Chromeé©±åŠ¨ - å¸¦é‡è¯•æœºåˆ¶
            if self.current_driver is None:
                # é‡è¯•æœºåˆ¶ï¼šæœ€å¤šç­‰å¾…3æ¬¡ï¼Œæ¯æ¬¡0.5ç§’
                for retry_attempt in range(3):
                    self.current_driver = self.driver_pool.get_driver()
                    if self.current_driver is not None:
                        pass  # é™é»˜å¤„ç†é‡è¯•æˆåŠŸ
                        break
                    if retry_attempt < 2:  # ä¸æ˜¯æœ€åä¸€æ¬¡é‡è¯•
                        pass  # é™é»˜å¤„ç†é©±åŠ¨æ± é‡è¯•
                        time.sleep(0.5)  # ç­‰å¾…é©±åŠ¨é‡Šæ”¾
                
                if self.current_driver is None:
                    # é‡è¯•åä»æ— æ³•è·å–é©±åŠ¨ï¼Œè®°å½•å¤±è´¥
                    pass  # é™é»˜å¤„ç†é©±åŠ¨è·å–å¤±è´¥
                    failed_result = {
                        'success': False,
                        'error': f'Chromeé©±åŠ¨æ± å·²æ»¡ï¼Œé‡è¯•3æ¬¡åä»æ— æ³•è·å–é©±åŠ¨',
                        'address': str(task.get('address', 'unknown')),
                        'worker_id': self.worker_id,
                        'index': task.get('index', -1),
                        'retry_info': 'driver_pool_exhausted_after_retry'
                    }
                    self.task_scheduler.put_result(failed_result)
                    self.stats['errors'] += 1
                    time.sleep(1.0)  # è¾ƒé•¿ç­‰å¾…é¿å…CPUç©ºè½¬ï¼Œç­‰å¾…é©±åŠ¨é‡Šæ”¾
                    continue
            
            # å¤„ç†ä»»åŠ¡
            try:
                result = self.process_task(task)
                self.task_scheduler.put_result(result)
                self.processed_count += 1
                
                # æ›´æ–°çº¿ç¨‹æœ¬åœ°ç»Ÿè®¡
                if result['success']:
                    self.stats['success'] += 1
                else:
                    self.stats['errors'] += 1
                
                # æ£€æŸ¥Driverç”Ÿå‘½å‘¨æœŸï¼ˆæ¯æ¬¡ä»»åŠ¡å®Œæˆåï¼‰
                if self.current_driver and result['success']:
                    needs_restart = self.driver_pool.increment_driver_usage(self.current_driver)
                    if needs_restart:
                        # Driverè¾¾åˆ°ç”Ÿå‘½å‘¨æœŸï¼Œé‡å¯
                        old_driver = self.current_driver
                        self.current_driver = self.driver_pool.restart_driver(old_driver, self.worker_id)
                        if self.current_driver is None:
                            print(f"Worker {self.worker_id}: Driveré‡å¯å¤±è´¥ï¼Œå°†åœ¨ä¸‹æ¬¡ä»»åŠ¡æ—¶é‡æ–°è·å–")
                
                # å®šæœŸæŠ¥å‘Šè¿›åº¦å’Œå†…å­˜æ¸…ç†
                if self.processed_count % 20 == 0:
                    print(f"Worker {self.worker_id}: å·²å¤„ç† {self.processed_count} ä¸ªä»»åŠ¡")
                    # å®šæœŸæ¸…ç†Chromeç¼“å­˜
                    if self.current_driver and self.processed_count % 50 == 0:
                        try:
                            self.current_driver.execute_script("window.gc();")  # å¼ºåˆ¶åƒåœ¾å›æ”¶
                            self.current_driver.delete_all_cookies()  # æ¸…ç†cookies
                        except:
                            pass
                
            except Exception as e:
                # å¤„ç†å¤±è´¥ï¼Œç”Ÿæˆé”™è¯¯ç»“æœ
                error_result = {
                    'success': False,
                    'error': str(e),
                    'address': str(task.get('address', 'unknown')),
                    'worker_id': self.worker_id,
                    'index': task.get('index', -1)
                }
                self.task_scheduler.put_result(error_result)
                
                self.stats['errors'] += 1
                
                # Chromeå¯èƒ½æœ‰é—®é¢˜ï¼Œå¼ºåˆ¶é‡Šæ”¾å¹¶é‡æ–°è·å–
                if self.current_driver:
                    self.driver_pool.release_driver(self.current_driver)
                    self.current_driver = None
        
        # å¼ºåˆ¶é‡Šæ”¾é©±åŠ¨ - ç¡®ä¿å½»åº•æ¸…ç†
        if self.current_driver:
            self.driver_pool.release_driver(self.current_driver)
            self.current_driver = None
        
        print(f"Worker {self.worker_id} å®Œæˆï¼Œå…±å¤„ç† {self.processed_count} ä¸ªä»»åŠ¡")
    
    def process_task(self, task):
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        address_obj = task['address']
        idx = task['index']
        
        start_time = time.time()
        
        # è·å–åœ°å€
        current_address = address_obj.get('primary') if isinstance(address_obj, dict) else address_obj
        used_address = current_address
        retry_info = []
        
        # å¤„ç†åœ°å€
        result = self.crawler._crawl_poi_info_turbo(current_address, self.current_driver)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯•
        should_retry = (isinstance(result, dict) and 
                       not result.get('is_building', False) and
                       result.get('poi_count', 0) == 0 and
                       result.get('status') != 'hotel_category_page_skipped')
        
        if should_retry and isinstance(address_obj, dict) and address_obj.get('secondary'):
            retry_result = self.crawler._crawl_poi_info_turbo(address_obj['secondary'], self.current_driver, check_building_type=False)
            retry_info.append(f"é‡è¯•: {address_obj['secondary']}")
            
            if (isinstance(retry_result, dict) and 
                retry_result.get('status') == 'success'):
                result = retry_result
                used_address = address_obj['secondary']
        
        processing_time = time.time() - start_time
        
        # æ„å»ºç»“æœ - åŒºåˆ†æŠ€æœ¯æˆåŠŸå’Œæ•°æ®ç»“æœ
        if (isinstance(result, dict) and 
            result.get('status') == 'success'):  # åªè¦æŠ€æœ¯æˆåŠŸå°±ç®—æˆåŠŸ
            return {
                'success': True,
                'data': result.get('data'),  # dataå¯èƒ½ä¸ºNoneï¼Œè¿™æ˜¯æ­£å¸¸çš„
                'poi_count': result.get('poi_count', 0),
                'result_type': result.get('result_type', 'data_found' if result.get('data') is not None else 'no_data'),
                'address': used_address,
                'worker_id': self.worker_id,
                'index': idx,
                'retry_info': retry_info,
                'processing_time': processing_time
            }
        else:
            # çœŸæ­£çš„å¤±è´¥ï¼ˆæŠ€æœ¯å¤±è´¥ï¼‰
            error_msg = result.get('error_message', 'å¤„ç†å¼‚å¸¸') if isinstance(result, dict) else 'å¤„ç†å¼‚å¸¸'
            return {
                'success': False,
                'error': error_msg,
                'address': used_address,
                'worker_id': self.worker_id,
                'index': idx,
                'retry_info': retry_info,
                'processing_time': processing_time
            }


logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')

def monitor_system_resources():
    """å…¨é¢ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
    try:
        process = psutil.Process()
        memory_info = process.memory_info()
        process_memory_mb = memory_info.rss / 1024 / 1024
        
        # ç³»ç»Ÿå†…å­˜ä½¿ç”¨
        system_memory = psutil.virtual_memory()
        memory_used_gb = system_memory.used / 1024 / 1024 / 1024
        memory_percent = system_memory.percent
        
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=0.1)
        
        # ç³»ç»Ÿè´Ÿè½½
        try:
            load_avg = psutil.getloadavg()[0]  # 1åˆ†é’Ÿå¹³å‡è´Ÿè½½
        except AttributeError:
            # Windowsç³»ç»Ÿæ²¡æœ‰getloadavg
            load_avg = cpu_percent / 100.0
        
        # Chromeè¿›ç¨‹æ•°é‡
        chrome_processes = 0
        try:
            for proc in psutil.process_iter(['name']):
                if 'chrome' in proc.info['name'].lower():
                    chrome_processes += 1
        except:
            chrome_processes = 0
        
        return {
            'process_memory_mb': process_memory_mb,
            'system_memory_gb': memory_used_gb,
            'system_memory_percent': memory_percent,
            'cpu_percent': cpu_percent,
            'load_average': load_avg,
            'chrome_processes': chrome_processes,
            'timestamp': time.time()
        }
    except Exception as e:
        print(f"èµ„æºç›‘æ§å¤±è´¥: {e}")
        return None

def monitor_memory_usage():
    """å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒå‘åå…¼å®¹"""
    resources = monitor_system_resources()
    if resources:
        return {
            'process_memory_mb': resources['process_memory_mb'],
            'system_memory_gb': resources['system_memory_gb'],
            'system_memory_percent': resources['system_memory_percent']
        }
    return None


class DataSaveWorker:
    """ä¸“ç”¨çš„æ•°æ®ä¿å­˜è¿›ç¨‹å·¥ä½œå™¨ - éš”ç¦»CPUå¯†é›†å‹ä»»åŠ¡"""
    
    @staticmethod
    def save_worker_process(save_queue, stop_event, output_file_path, worker_id=0):
        """æ•°æ®ä¿å­˜è¿›ç¨‹çš„ä¸»å‡½æ•°"""
        import pandas as pd
        import time
        from pathlib import Path
        
        print(f"ğŸ”§ æ•°æ®ä¿å­˜è¿›ç¨‹ {worker_id} å¯åŠ¨")
        batch_count = 0
        total_saved = 0
        
        try:
            while not stop_event.is_set():
                try:
                    # ä»é˜Ÿåˆ—è·å–æ•°æ®ï¼Œè®¾ç½®è¶…æ—¶é¿å…æ— é™ç­‰å¾…
                    data_batch = save_queue.get(timeout=1.0)
                    
                    if data_batch == "STOP":
                        print(f"ğŸ“¤ æ•°æ®ä¿å­˜è¿›ç¨‹ {worker_id} æ”¶åˆ°åœæ­¢ä¿¡å·")
                        break
                    
                    # å¤„ç†æ•°æ®æ‰¹æ¬¡
                    if data_batch and len(data_batch) > 0:
                        batch_count += 1
                        start_time = time.time()
                        
                        # CPUå¯†é›†å‹æ“ä½œï¼šDataFrameåˆå¹¶
                        if isinstance(data_batch[0], pd.DataFrame):
                            combined_df = pd.concat(data_batch, ignore_index=True)
                        else:
                            # å¦‚æœæ˜¯dictåˆ—è¡¨ï¼Œè½¬æ¢ä¸ºDataFrame
                            combined_df = pd.DataFrame(data_batch)
                        
                        # CPUå¯†é›†å‹æ“ä½œï¼šå»é‡
                        if not combined_df.empty:
                            # ç¡®ä¿æœ‰å¿…è¦çš„åˆ—è¿›è¡Œå»é‡
                            dedup_columns = ['name', 'lat', 'lng']
                            existing_columns = [col for col in dedup_columns if col in combined_df.columns]
                            
                            if existing_columns:
                                before_count = len(combined_df)
                                combined_df = combined_df.drop_duplicates(subset=existing_columns, keep='first')
                                after_count = len(combined_df)
                                removed_count = before_count - after_count
                                
                                if removed_count > 0:
                                    print(f"ğŸ”„ æ‰¹æ¬¡ {batch_count}: å»é‡ç§»é™¤ {removed_count} æ¡é‡å¤æ•°æ®")
                        
                        # I/Oå¯†é›†å‹æ“ä½œï¼šä¿å­˜åˆ°æ–‡ä»¶
                        if not combined_df.empty:
                            output_path = Path(output_file_path)
                            file_exists = output_path.exists()
                            
                            combined_df.to_csv(
                                output_path,
                                mode='a',
                                header=not file_exists,
                                index=False,
                                encoding='utf-8-sig'
                            )
                            
                            total_saved += len(combined_df)
                            processing_time = time.time() - start_time
                            
                            print(f"ğŸ’¾ æ‰¹æ¬¡ {batch_count}: ä¿å­˜ {len(combined_df)} æ¡æ•°æ® "
                                  f"(è€—æ—¶: {processing_time:.2f}s, ç´¯è®¡: {total_saved})")
                        
                except queue.Empty:
                    # é˜Ÿåˆ—ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
                    continue
                except Exception as e:
                    print(f"âŒ æ•°æ®ä¿å­˜è¿›ç¨‹ {worker_id} å¤„ç†é”™è¯¯: {e}")
                    continue
                    
        except KeyboardInterrupt:
            print(f"âš ï¸  æ•°æ®ä¿å­˜è¿›ç¨‹ {worker_id} è¢«ä¸­æ–­")
        except Exception as e:
            print(f"âŒ æ•°æ®ä¿å­˜è¿›ç¨‹ {worker_id} å¼‚å¸¸é€€å‡º: {e}")
        finally:
            print(f"ğŸ æ•°æ®ä¿å­˜è¿›ç¨‹ {worker_id} ç»“æŸï¼Œå…±å¤„ç† {batch_count} æ‰¹æ¬¡ï¼Œä¿å­˜ {total_saved} æ¡æ•°æ®")


class AsyncDataSaveManager:
    """å¼‚æ­¥æ•°æ®ä¿å­˜ç®¡ç†å™¨"""
    
    def __init__(self, output_file_path, max_queue_size=100):
        self.output_file_path = output_file_path
        self.save_queue = MPQueue(maxsize=max_queue_size)
        self.stop_event = MPEvent()
        self.save_process = None
        self.is_running = False
        
    def start(self):
        """å¯åŠ¨æ•°æ®ä¿å­˜è¿›ç¨‹"""
        if self.is_running:
            return
            
        self.stop_event.clear()
        self.save_process = Process(
            target=DataSaveWorker.save_worker_process,
            args=(self.save_queue, self.stop_event, self.output_file_path, 1),
            daemon=False  # ç¡®ä¿è¿›ç¨‹èƒ½æ­£å¸¸ç»“æŸ
        )
        self.save_process.start()
        self.is_running = True
        print(f"ğŸš€ å¼‚æ­¥æ•°æ®ä¿å­˜è¿›ç¨‹å¯åŠ¨ (PID: {self.save_process.pid})")
    
    def save_batch_async(self, data_batch, timeout=5.0):
        """å¼‚æ­¥æäº¤æ•°æ®æ‰¹æ¬¡åˆ°ä¿å­˜é˜Ÿåˆ—"""
        if not self.is_running or not data_batch:
            return False
            
        try:
            # éé˜»å¡æäº¤åˆ°é˜Ÿåˆ—
            self.save_queue.put(data_batch, timeout=timeout)
            return True
        except queue.Full:
            print(f"âš ï¸  æ•°æ®ä¿å­˜é˜Ÿåˆ—å·²æ»¡ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
            return False
        except Exception as e:
            print(f"âŒ æäº¤æ•°æ®æ‰¹æ¬¡å¤±è´¥: {e}")
            return False
    
    def stop(self, timeout=10.0):
        """åœæ­¢æ•°æ®ä¿å­˜è¿›ç¨‹"""
        if not self.is_running:
            return
            
        print("ğŸ›‘ æ­£åœ¨åœæ­¢æ•°æ®ä¿å­˜è¿›ç¨‹...")
        
        try:
            # å‘é€åœæ­¢ä¿¡å·
            self.save_queue.put("STOP", timeout=2.0)
            self.stop_event.set()
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            if self.save_process and self.save_process.is_alive():
                self.save_process.join(timeout=timeout)
                
                if self.save_process.is_alive():
                    print("âš ï¸  å¼ºåˆ¶ç»ˆæ­¢æ•°æ®ä¿å­˜è¿›ç¨‹")
                    self.save_process.terminate()
                    self.save_process.join(timeout=2.0)
                    
        except Exception as e:
            print(f"âŒ åœæ­¢æ•°æ®ä¿å­˜è¿›ç¨‹æ—¶å‡ºé”™: {e}")
        finally:
            self.is_running = False
            print("âœ… æ•°æ®ä¿å­˜è¿›ç¨‹å·²åœæ­¢")
    
    def get_queue_size(self):
        """è·å–å½“å‰é˜Ÿåˆ—å¤§å°"""
        try:
            return self.save_queue.qsize()
        except:
            return 0


class DynamicResourceScheduler:
    """åŠ¨æ€èµ„æºæ„ŸçŸ¥è°ƒåº¦å™¨ - æ ¹æ®ç³»ç»Ÿè´Ÿè½½æ™ºèƒ½è°ƒèŠ‚"""
    
    def __init__(self, check_interval=10):
        self.check_interval = check_interval  # èµ„æºæ£€æŸ¥é—´éš”(ç§’)
        self.feeding_event = threading.Event()
        self.feeding_event.set()  # åˆå§‹çŠ¶æ€ï¼šå…è®¸åˆ†å‘
        
        # è°ƒåº¦çŠ¶æ€
        self.is_paused = False
        self.last_check_time = 0
        self.pause_count = 0
        self.resume_count = 0
        
        # é˜ˆå€¼é…ç½®
        self.memory_high_threshold = 85   # å†…å­˜é«˜é˜ˆå€¼
        self.memory_low_threshold = 60    # å†…å­˜ä½é˜ˆå€¼
        self.cpu_high_threshold = 90      # CPUé«˜é˜ˆå€¼
        self.cpu_low_threshold = 70       # CPUä½é˜ˆå€¼
        self.load_high_threshold = 8.0    # è´Ÿè½½é«˜é˜ˆå€¼
        
        # å†å²çŠ¶æ€è·Ÿè¸ª
        self.resource_history = []
        self.max_history = 10
        
    def check_and_adjust_schedule(self):
        """æ£€æŸ¥èµ„æºçŠ¶æ€å¹¶è°ƒæ•´è°ƒåº¦ç­–ç•¥"""
        current_time = time.time()
        
        # é¿å…é¢‘ç¹æ£€æŸ¥
        if current_time - self.last_check_time < self.check_interval:
            return self.is_paused
            
        self.last_check_time = current_time
        
        # è·å–ç³»ç»Ÿèµ„æºçŠ¶æ€
        resources = monitor_system_resources()
        if not resources:
            return self.is_paused
            
        # è®°å½•å†å²çŠ¶æ€
        self.resource_history.append(resources)
        if len(self.resource_history) > self.max_history:
            self.resource_history.pop(0)
        
        # å†³ç­–é€»è¾‘
        should_pause = self._should_pause_scheduling(resources)
        should_resume = self._should_resume_scheduling(resources)
        
        if should_pause and not self.is_paused:
            self._pause_scheduling(resources)
        elif should_resume and self.is_paused:
            self._resume_scheduling(resources)
            
        return self.is_paused
    
    def _should_pause_scheduling(self, resources):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æš‚åœè°ƒåº¦"""
        memory_overload = resources['system_memory_percent'] > self.memory_high_threshold
        cpu_overload = resources['cpu_percent'] > self.cpu_high_threshold
        load_overload = resources['load_average'] > self.load_high_threshold
        
        # å¤šé‡æ¡ä»¶åˆ¤æ–­
        critical_conditions = sum([memory_overload, cpu_overload, load_overload])
        
        # å¦‚æœæœ‰2ä¸ªæˆ–ä»¥ä¸Šå…³é”®æŒ‡æ ‡è¶…æ ‡ï¼Œæˆ–å†…å­˜ä¸¥é‡è¶…æ ‡
        return critical_conditions >= 2 or resources['system_memory_percent'] > 90
    
    def _should_resume_scheduling(self, resources):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ¢å¤è°ƒåº¦"""
        if not self.is_paused:
            return False
            
        memory_ok = resources['system_memory_percent'] < self.memory_low_threshold
        cpu_ok = resources['cpu_percent'] < self.cpu_low_threshold
        load_ok = resources['load_average'] < 6.0
        
        # æ‰€æœ‰å…³é”®æŒ‡æ ‡éƒ½æ­£å¸¸æ‰æ¢å¤
        return memory_ok and cpu_ok and load_ok
    
    def _pause_scheduling(self, resources):
        """æš‚åœä»»åŠ¡è°ƒåº¦"""
        self.is_paused = True
        self.pause_count += 1
        self.feeding_event.clear()  # é˜»æ­¢ä»»åŠ¡åˆ†å‘
        
        print(f"ğŸ›‘ æš‚åœä»»åŠ¡è°ƒåº¦ (ç¬¬{self.pause_count}æ¬¡)")
        print(f"   å†…å­˜: {resources['system_memory_percent']:.1f}% "
              f"CPU: {resources['cpu_percent']:.1f}% "
              f"è´Ÿè½½: {resources['load_average']:.2f}")
        
    def _resume_scheduling(self, resources):
        """æ¢å¤ä»»åŠ¡è°ƒåº¦"""
        self.is_paused = False
        self.resume_count += 1
        self.feeding_event.set()  # å…è®¸ä»»åŠ¡åˆ†å‘
        
        print(f"ğŸŸ¢ æ¢å¤ä»»åŠ¡è°ƒåº¦ (ç¬¬{self.resume_count}æ¬¡)")
        print(f"   å†…å­˜: {resources['system_memory_percent']:.1f}% "
              f"CPU: {resources['cpu_percent']:.1f}% "
              f"è´Ÿè½½: {resources['load_average']:.2f}")
    
    def wait_for_scheduling(self, timeout=None):
        """ç­‰å¾…è°ƒåº¦è®¸å¯ï¼ˆè¢«æš‚åœæ—¶ä¼šé˜»å¡ï¼‰"""
        return self.feeding_event.wait(timeout=timeout)
    
    def force_pause(self):
        """å¼ºåˆ¶æš‚åœè°ƒåº¦"""
        self.is_paused = True
        self.feeding_event.clear()
        print("ğŸ›‘ æ‰‹åŠ¨æš‚åœä»»åŠ¡è°ƒåº¦")
    
    def force_resume(self):
        """å¼ºåˆ¶æ¢å¤è°ƒåº¦"""
        self.is_paused = False
        self.feeding_event.set()
        print("ğŸŸ¢ æ‰‹åŠ¨æ¢å¤ä»»åŠ¡è°ƒåº¦")
        
    def get_status(self):
        """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
        return {
            'is_paused': self.is_paused,
            'pause_count': self.pause_count,
            'resume_count': self.resume_count,
            'resource_history_length': len(self.resource_history)
        }


class AsyncCrawlWrapper:
    """å¼‚æ­¥çˆ¬è™«åŒ…è£…å™¨ - åç¨‹+çº¿ç¨‹æ± æ··åˆæ¶æ„"""
    def __init__(self, crawler_instance, driver_pool, max_workers=24):
        self.crawler = crawler_instance
        self.driver_pool = driver_pool
        self.executor = ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="ChromeWorker")
        # é™åˆ¶åŒæ—¶å¹¶å‘çˆ¬è™«ä»»åŠ¡ä¸º20-25ä¸ªï¼Œé¿å…driverç«äº‰
        self.semaphore = asyncio.Semaphore(min(25, max_workers))  
        self.resource_scheduler = DynamicResourceScheduler(check_interval=8)  # èµ„æºæ„ŸçŸ¥è°ƒåº¦å™¨
        
    async def crawl_single_address_async(self, address_data):
        """å¼‚æ­¥å•åœ°å€çˆ¬å– - æ”¯æŒåŠ¨æ€èµ„æºè°ƒåº¦"""
        async with self.semaphore:
            # åŠ¨æ€èµ„æºè°ƒåº¦æ£€æŸ¥
            is_paused = self.resource_scheduler.check_and_adjust_schedule()
            if is_paused:
                # å¦‚æœè¢«æš‚åœï¼Œç­‰å¾…æ¢å¤ï¼ˆå¼‚æ­¥ç­‰å¾…ï¼‰
                await asyncio.get_event_loop().run_in_executor(
                    None, 
                    self.resource_scheduler.wait_for_scheduling,
                    30.0  # æœ€é•¿ç­‰å¾…30ç§’
                )
            
            loop = asyncio.get_event_loop()
            try:
                # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡ŒChromeæ“ä½œ
                result = await loop.run_in_executor(
                    self.executor,
                    self._crawl_with_driver_sync,
                    address_data
                )
                return result
            except Exception as e:
                return {
                    'success': False,
                    'error': f'å¼‚æ­¥è°ƒç”¨å¤±è´¥: {str(e)}',
                    'address': str(address_data.get('address', 'unknown')),
                    'index': address_data.get('index', -1)
                }
    
    def _crawl_with_driver_sync(self, address_data):
        """åŒæ­¥Chromeçˆ¬å–æ“ä½œï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        driver = None
        try:
            # è·å–Driver - å¸¦é‡è¯•æœºåˆ¶
            for retry_attempt in range(3):
                driver = self.driver_pool.get_driver()
                if driver is not None:
                    pass  # é™é»˜å¤„ç†å¼‚æ­¥é‡è¯•æˆåŠŸ
                    break
                if retry_attempt < 2:  # ä¸æ˜¯æœ€åä¸€æ¬¡é‡è¯•
                    pass  # é™é»˜å¤„ç†å¼‚æ­¥é‡è¯•
                    time.sleep(0.5)  # ç­‰å¾…é©±åŠ¨é‡Šæ”¾
            
            if driver is None:
                return {
                    'success': False,
                    'error': 'Chromeé©±åŠ¨æ± å·²æ»¡ï¼Œé‡è¯•3æ¬¡åä»æ— æ³•è·å–é©±åŠ¨',
                    'address': str(address_data.get('address', 'unknown')),
                    'index': address_data.get('index', -1)
                }
            
            # æ‰§è¡Œçˆ¬å– - æ”¯æŒå¤šåœ°å€é‡è¯•
            primary_address = address_data.get('address') or address_data.get('primary')
            secondary_address = address_data.get('secondary')
            fallback_address = address_data.get('fallback')
            idx = address_data['index']
            
            used_address = primary_address
            retry_info = []
            
            # é¦–å…ˆä½¿ç”¨primaryåœ°å€
            result = self.crawler._crawl_poi_info_turbo(primary_address, driver)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è¯• - ä¸Workeré€»è¾‘ä¿æŒä¸€è‡´
            should_retry = (isinstance(result, dict) and 
                           not result.get('is_building', False) and
                           result.get('poi_count', 0) == 0 and
                           result.get('status') != 'hotel_category_page_skipped')
            
            # é‡è¯•secondaryåœ°å€
            if should_retry and secondary_address:
                retry_result = self.crawler._crawl_poi_info_turbo(secondary_address, driver, check_building_type=False)
                retry_info.append(f"é‡è¯•secondary: {secondary_address}")
                
                if (isinstance(retry_result, dict) and 
                    retry_result.get('status') == 'success'):
                    result = retry_result
                    used_address = secondary_address
                    should_retry = False
            
            # é‡è¯•fallbackåœ°å€
            if should_retry and fallback_address:
                retry_result = self.crawler._crawl_poi_info_turbo(fallback_address, driver, check_building_type=False)
                retry_info.append(f"é‡è¯•fallback: {fallback_address}")
                
                if (isinstance(retry_result, dict) and 
                    retry_result.get('status') == 'success'):
                    result = retry_result
                    used_address = fallback_address
            
            # æ£€æŸ¥ç”Ÿå‘½å‘¨æœŸ
            needs_restart = self.driver_pool.increment_driver_usage(driver)
            if needs_restart:
                # é‡å¯driver
                new_driver = self.driver_pool.restart_driver(driver)
                if new_driver:
                    driver = new_driver
                else:
                    driver = None
            
            # æ„å»ºç»“æœ - ä½¿ç”¨ä¸Workerç›¸åŒçš„é€»è¾‘
            if (isinstance(result, dict) and 
                result.get('status') == 'success'):
                return {
                    'success': True,
                    'data': result.get('data'),
                    'poi_count': result.get('poi_count', 0),
                    'result_type': result.get('result_type', 'data_found' if result.get('data') is not None else 'no_data'),
                    'address': str(used_address),
                    'index': idx,
                    'retry_info': retry_info
                }
            else:
                # çœŸæ­£çš„å¤±è´¥
                error_msg = result.get('error_message', 'å¤„ç†å¼‚å¸¸') if isinstance(result, dict) else 'å¤„ç†å¼‚å¸¸'
                return {
                    'success': False,
                    'error': error_msg,
                    'address': str(used_address),
                    'index': idx,
                    'retry_info': retry_info
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'address': str(address_data.get('address', 'unknown')),
                'index': address_data.get('index', -1)
            }
        finally:
            # å¼ºåˆ¶é‡Šæ”¾Driver - ç¡®ä¿å½»åº•æ¸…ç†
            if driver:
                self.driver_pool.release_driver(driver)
    
    async def crawl_batch_async(self, address_list):
        """å¼‚æ­¥æ‰¹é‡çˆ¬å–"""
        tasks = []
        for address_data in address_list:
            task = self.crawl_single_address_async(address_data)
            tasks.append(task)
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        return results
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.executor.shutdown(wait=True)


class ParallelPOICrawler:
    def __init__(self, max_workers=None, output_dir="data/output", batch_size=50, enable_resume=True):
        # ç§‘å­¦çš„é«˜æ€§èƒ½é…ç½®
        cpu_count = mp.cpu_count()
        if max_workers is None:
            if cpu_count >= 12:
                # 12æ ¸å¿ƒä»¥ä¸Šï¼šIOå¯†é›†å‹ä¼˜åŒ–é…ç½®
                self.max_workers = min(36, int(cpu_count * 3.0))  # æ ¸å¿ƒæ•° Ã— 2.5~3
            elif cpu_count >= 8:
                # 8-11æ ¸å¿ƒï¼šä½¿ç”¨CPUæ•°é‡*3
                self.max_workers = min(32, cpu_count * 3)
            else:
                # 8æ ¸å¿ƒä»¥ä¸‹ï¼šä½¿ç”¨CPUæ•°é‡*2
                self.max_workers = min(24, cpu_count * 2)
        else:
            self.max_workers = max_workers
            
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.batch_size = batch_size
        self.output_file = None
        self.enable_resume = enable_resume
        self.progress_dir = Path("data/progress")
        self.progress_dir.mkdir(parents=True, exist_ok=True)
        self.progress_file = None
        
        # è­¦å‘Šä¿å­˜ç³»ç»Ÿ
        self.warnings_dir = Path("data/warnings")
        self.warnings_dir.mkdir(parents=True, exist_ok=True)
        self.warnings_file = None
        self.warning_batch = []
        self.warning_lock = Lock()
        
        # no_poi_warnings ç³»ç»Ÿ
        self.no_poi_warnings_dir = Path("no_poi_warnings")
        self.no_poi_warnings_dir.mkdir(exist_ok=True)
        self.no_poi_batch_tracker = {}  # è·Ÿè¸ªæ¯ä¸ªæ‰¹æ¬¡çš„ç»“æœ
        self.no_poi_tracker_lock = Lock()
        
        # é«˜æ•ˆChromeé©±åŠ¨æ± é…ç½® - driveræ•°é‡æ§åˆ¶åœ¨çº¿ç¨‹æ•°ä»¥ä¸‹ï¼Œé¿å…å†…å­˜è¿‡è½½
        chrome_pool_size = min(30, max(24, int(self.max_workers * 0.75)))  # driveræ•° < çº¿ç¨‹æ•°
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.driver_pool = ChromeDriverPool(max_drivers=chrome_pool_size)
        self.task_scheduler = TurboTaskScheduler(max_threads=self.max_workers)
        
        # å¼‚æ­¥é”ç”¨äºçº¿ç¨‹å®‰å…¨çš„è¿›åº¦ä¿å­˜
        self.progress_lock = None  # å°†åœ¨å¼‚æ­¥ç¯å¢ƒä¸­åˆå§‹åŒ–ä¸º asyncio.Lock()
        
    def _crawl_poi_info_turbo(self, address, driver, check_building_type=True):
        """Turboæ¨¡å¼çš„POIä¿¡æ¯çˆ¬å–"""
        url = f'https://www.google.com/maps/place/{address}'
        
        try:
            driver.get(url)
            
            # å¿«é€Ÿæ£€æŸ¥é…’åº—ç±»åˆ«é¡µé¢
            hotel_check_result = self._has_category_header(driver)
            if hotel_check_result:
                print(f"ğŸ¨ é…’åº—é¡µé¢: {address[:50]}...")
                return {
                    'data': None,
                    'is_building': False,
                    'poi_count': 0,
                    'status': 'success',  # æŠ€æœ¯æˆåŠŸï¼Œä½†æ˜¯é…’åº—ç±»åˆ«é¡µé¢
                    'result_type': 'hotel_category_page',
                    'hotel_type': hotel_check_result.get('type', 'unknown'),
                    'address': address
                }
            
            # æ¢å¤å……è¶³ç­‰å¾…æ—¶é—´ç¡®ä¿é¡µé¢å®Œå…¨åŠ è½½
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            poi_count = 0
            place_type = 'unknown'
            is_building = False
            
            # è·å–åœ°ç‚¹åç§°
            try:
                place_name = get_building_name(driver)
            except Exception:
                place_name = self._get_fallback_location_name(driver, address) or 'Unknown Location'
            
            # å°è¯•å±•å¼€POIåˆ—è¡¨
            try:
                more_button = driver.find_elements('class name', 'M77dve')
                if more_button:
                    click_on_more_button(driver)
                    scroll_poi_section(driver)
            except:
                pass
            
            # è·å–POIä¿¡æ¯
            df = get_all_poi_info(driver)
            
            if df is not None and not df.empty:
                poi_count = len(df)
                
                # å¿«é€Ÿè·å–åæ ‡
                try:
                    final_url = wait_for_coords_url(driver, timeout=2)
                    if final_url:
                        lat, lng = get_coords(final_url)
                    else:
                        lat, lng = None, None
                except:
                    lat, lng = None, None
                
                # æ·»åŠ åˆ—ä¿¡æ¯
                try:
                    df['blt_name'] = place_name
                    df['lat'] = lat
                    df['lng'] = lng
                    
                    # éªŒè¯åˆ—å®Œæ•´æ€§
                    required_cols = ['name', 'rating', 'class', 'add', 'comment_count', 'blt_name', 'lat', 'lng']
                    if not all(col in df.columns for col in required_cols):
                        error_msg = f"DataFrameåˆ—ä¸å®Œæ•´ï¼Œç¼ºå°‘: {[col for col in required_cols if col not in df.columns]}"
                        print(f"è­¦å‘Š: {error_msg}ï¼Œåœ°å€: {address[:50]}...")
                        # æ³¨æ„ï¼šè¿™é‡Œæ— æ³•ç›´æ¥è°ƒç”¨self._save_warningï¼Œå› ä¸ºè¿™æ˜¯åœ¨workerä¸­
                        return {
                            'data': None,
                            'is_building': True,
                            'poi_count': 0,
                            'status': 'success',  # æŠ€æœ¯æˆåŠŸï¼Œä½†æ˜¯æ•°æ®åˆ—ä¸å®Œæ•´
                            'result_type': 'column_error',
                            'warning_info': {'type': 'column_incomplete', 'message': error_msg}
                        }
                    
                    print(f"{address}  | POI: {poi_count} | çŠ¶æ€: å·²ä¿å­˜")
                    
                    return {
                        'data': df,
                        'is_building': True,
                        'poi_count': poi_count,
                        'status': 'success'
                    }
                except Exception as e:
                    error_msg = f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}"
                    print(f"è­¦å‘Š: {error_msg}ï¼Œåœ°å€: {address[:50]}...")
                    return {
                        'data': None,
                        'is_building': True,
                        'poi_count': 0,
                        'status': 'error',  # çœŸæ­£çš„æ•°æ®å¤„ç†å¤±è´¥
                        'error_message': error_msg,
                        'warning_info': {'type': 'data_processing_error', 'message': error_msg}
                    }
            else:
                if check_building_type:
                    try:
                        place_type = get_building_type(driver)
                        is_building = place_type == 'å»ºç­‘ç‰©'
                        print(f"{address}  | ç±»å‹: {place_type} | POI: {poi_count}")
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯chÅmeæ ¼å¼åœ°å€çš„éå»ºç­‘ç‰©æƒ…å†µ
                        import re
                        chome_pattern = r'\d+-ch[Åo]me-\d+-\d+\+\w+'
                        if not is_building and re.search(chome_pattern, address, re.IGNORECASE):
                            warning_info = {
                                'type': 'chome_format_non_building', 
                                'message': f"chÅmeæ ¼å¼åœ°å€éå»ºç­‘ç‰©: {place_type}, åœ°å€æ ¼å¼: {address}"
                            }
                            return {
                                'data': None,
                                'is_building': is_building,
                                'poi_count': 0,
                                'status': 'success',  # æŠ€æœ¯æˆåŠŸï¼Œä½†æ˜¯æ— POIæ•°æ®
                                'result_type': 'no_poi_data',
                                'warning_info': warning_info
                            }
                    except:
                        print(f"{address}  | POI: {poi_count}")
                else:
                    print(f"{address}  | POI: {poi_count}")
                
                return {
                    'data': None,
                    'is_building': is_building,
                    'poi_count': 0,
                    'status': 'success',  # æŠ€æœ¯æˆåŠŸï¼Œä½†æ˜¯æ— POIæ•°æ®
                    'result_type': 'no_poi_data'
                }
                
        except Exception as e:
            import traceback
            error_detail = f"{type(e).__name__}: {str(e)}"
            pass  # é™é»˜å¤„ç†Chromeä»»åŠ¡å¼‚å¸¸
            return {
                'data': None,
                'is_building': False,
                'poi_count': 0,
                'status': 'error',  # çœŸæ­£çš„å¤±è´¥
                'error_message': error_detail,
                'error_type': type(e).__name__
            }
    
    def _has_category_header(self, driver):
        """æ£€æŸ¥æ˜¯å¦æ˜¯é…’åº—ç±»åˆ«é¡µé¢ï¼Œè¿”å›æ£€æµ‹ç»“æœè¯¦æƒ…"""
        try:
            # æ£€æŸ¥é…’åº—ç±»åˆ«æ ‡é¢˜
            category_headers = driver.find_elements("css selector", "h2.kPvgOb.fontHeadlineSmall")
            for header in category_headers:
                text = header.text.strip()
                # æ”¯æŒä¸­æ–‡ã€æ—¥æ–‡å’Œè‹±æ–‡é…’åº—æ£€æµ‹
                if text == "é…’åº—":
                    return {"type": "ä¸­æ–‡é…’åº—ç±»åˆ«é¡µé¢", "keyword": text}
                elif text == "ãƒ›ãƒ†ãƒ«":
                    return {"type": "æ—¥æ–‡é…’åº—ç±»åˆ«é¡µé¢", "keyword": text}
                elif "hotel" in text.lower():
                    return {"type": "è‹±æ–‡é…’åº—ç±»åˆ«é¡µé¢", "keyword": text}
            
            # æ£€æŸ¥å…¶ä»–ç±»åˆ«é¡µé¢æŒ‡ç¤ºå™¨
            category_indicators = [
                ("div.aIiAFe h1", "ä¸»æ ‡é¢˜"),
                ("h1.jRccSf", "ç±»åˆ«æ ‡é¢˜"),
                ("h1.ZoUhNb", "å•†å®¶ç±»åˆ«æ ‡é¢˜")
            ]
            
            for selector, desc in category_indicators:
                try:
                    elements = driver.find_elements("css selector", selector)
                    for element in elements:
                        text = element.text.strip()
                        text_lower = text.lower()
                        
                        if "é…’åº—" in text:
                            return {"type": f"ä¸­æ–‡é…’åº—é¡µé¢({desc})", "keyword": text}
                        elif "ãƒ›ãƒ†ãƒ«" in text:
                            return {"type": f"æ—¥æ–‡é…’åº—é¡µé¢({desc})", "keyword": text}
                        elif any(keyword in text_lower for keyword in ["hotel", "lodging", "accommodation"]):
                            return {"type": f"è‹±æ–‡é…’åº—é¡µé¢({desc})", "keyword": text}
                except:
                    continue
                    
            return None
        except Exception as e:
            return None
    
    def _get_fallback_location_name(self, driver, address):
        """è·å–å¤‡ç”¨åœ°ç‚¹åç§°"""
        try:
            title = driver.title
            if title and title != "Google Maps" and "Google" not in title:
                clean_title = title.replace(" - Google Maps", "").replace(" - Google åœ°å›¾", "").strip()
                if clean_title:
                    return clean_title
            return None
        except:
            return None
    
    def _save_progress(self, district_name, completed_count, total_count, total_success, total_errors):
        """ä¿å­˜è¿›åº¦ - åŒæ­¥ç‰ˆæœ¬"""
        if not self.enable_resume:
            return
            
        progress_data = {
            'district_name': district_name,
            'completed_count': completed_count,
            'total_count': total_count,
            'total_success': total_success,
            'total_errors': total_errors,
            'output_file': str(self.output_file),
            'timestamp': time.time()
        }
        
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
    
    async def _save_progress_async(self, district_name, completed_count, total_count, total_success, total_errors):
        """å¼‚æ­¥ä¿å­˜è¿›åº¦ - çº¿ç¨‹å®‰å…¨ç‰ˆæœ¬"""
        if not self.enable_resume:
            return
            
        # åˆå§‹åŒ–å¼‚æ­¥é”ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
        if self.progress_lock is None:
            self.progress_lock = asyncio.Lock()
            
        progress_data = {
            'district_name': district_name,
            'completed_count': completed_count,
            'total_count': total_count,
            'total_success': total_success,
            'total_errors': total_errors,
            'output_file': str(self.output_file),
            'timestamp': time.time()
        }
        
        # ä½¿ç”¨å¼‚æ­¥é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
        async with self.progress_lock:
            # åŸå­æ€§å†™å…¥ï¼šå…ˆå†™ä¸´æ—¶æ–‡ä»¶ï¼Œå†é‡å‘½å
            temp_file = f"{self.progress_file}.tmp"
            try:
                with open(temp_file, 'w', encoding='utf-8') as f:
                    json.dump(progress_data, f, ensure_ascii=False, indent=2)
                
                # åŸå­æ€§é‡å‘½å
                import os
                os.rename(temp_file, self.progress_file)
            except Exception as e:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)
                except:
                    pass
                raise e
    
    def _load_progress(self, district_name):
        """åŠ è½½è¿›åº¦"""
        if not self.enable_resume or not self.progress_file.exists():
            return None
            
        try:
            with open(self.progress_file, 'r', encoding='utf-8') as f:
                progress_data = json.load(f)
            if progress_data.get('district_name') == district_name:
                return progress_data
        except:
            pass
        return None
    
    def _cleanup_progress(self):
        """æ¸…ç†è¿›åº¦æ–‡ä»¶"""
        if self.progress_file and self.progress_file.exists():
            try:
                self.progress_file.unlink()
            except:
                pass
    
    def _extract_district_name(self, input_file):
        """æå–åŒºå"""
        filename = Path(input_file).stem
        if 'åŒº' in filename:
            return filename.split('åŒº')[0] + 'åŒº'
        return 'unknown_district'
    
    def _batch_append_to_output_file(self, data_list):
        """æ‰¹é‡å†™å…¥æ–‡ä»¶"""
        if self.output_file is None or not data_list:
            return
        
        # è¿‡æ»¤å’ŒéªŒè¯æ•°æ®
        valid_data = []
        for data in data_list:
            if data is not None and isinstance(data, pd.DataFrame) and not data.empty:
                # ç¡®ä¿DataFrameæœ‰å¿…è¦çš„åˆ—
                required_cols = ['name', 'rating', 'class', 'add', 'comment_count', 'blt_name', 'lat', 'lng']
                if all(col in data.columns for col in required_cols):
                    valid_data.append(data)
                else:
                    missing_cols = [col for col in required_cols if col not in data.columns]
                    error_msg = f"æ•°æ®ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}"
                    print(f"è­¦å‘Š: {error_msg}ï¼Œè·³è¿‡")
                    self._save_warning('batch_write_column_missing', 'batch_data', error_msg)
        
        if not valid_data:
            return
            
        try:
            combined_df = pd.concat(valid_data, ignore_index=True)
            combined_df = combined_df.drop_duplicates(subset=['name', 'lat', 'lng'], keep='first')
            
            if not combined_df.empty:
                combined_df.to_csv(self.output_file, mode='a', header=False, index=False, encoding='utf-8-sig')
        except Exception as e:
            error_msg = f"æ‰¹é‡å†™å…¥å¤±è´¥: {str(e)}, æœ‰æ•ˆæ•°æ®é‡: {len(valid_data)}, åŸå§‹æ•°æ®é‡: {len(data_list)}"
            print(error_msg)
            self._save_warning('batch_write_failed', 'batch_operation', error_msg)
    
    def _save_warning(self, warning_type, address, error_msg, worker_id=None, extra_info=None):
        """ä¿å­˜è­¦å‘Šä¿¡æ¯åˆ°æ–‡ä»¶"""
        with self.warning_lock:
            warning_record = {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'warning_type': warning_type,
                'address': address[:100] if address else 'unknown',  # é™åˆ¶åœ°å€é•¿åº¦
                'error_message': str(error_msg)[:200],  # é™åˆ¶é”™è¯¯ä¿¡æ¯é•¿åº¦
                'worker_id': worker_id,
                'extra_info': extra_info
            }
            self.warning_batch.append(warning_record)
            
            # æ‰¹é‡å†™å…¥è­¦å‘Šï¼ˆæ¯50æ¡æˆ–ç¨‹åºç»“æŸæ—¶ï¼‰
            if len(self.warning_batch) >= 50:
                self._flush_warnings()
    
    def _flush_warnings(self):
        """æ‰¹é‡å†™å…¥è­¦å‘Šåˆ°CSVæ–‡ä»¶"""
        if not self.warning_batch or self.warnings_file is None:
            return
            
        try:
            warning_df = pd.DataFrame(self.warning_batch)
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä»¥å†³å®šæ˜¯å¦å†™å…¥å¤´éƒ¨
            file_exists = self.warnings_file.exists()
            warning_df.to_csv(
                self.warnings_file, 
                mode='a', 
                header=not file_exists, 
                index=False, 
                encoding='utf-8-sig'
            )
            print(f"å·²ä¿å­˜ {len(self.warning_batch)} æ¡è­¦å‘Šè®°å½•åˆ°: {self.warnings_file}")
            self.warning_batch.clear()
        except Exception as e:
            print(f"è­¦å‘Šæ–‡ä»¶å†™å…¥å¤±è´¥: {e}")
    
    def _track_no_poi_result(self, result, batch_size=50):
        """è·Ÿè¸ªno_poiç»“æœï¼Œæ£€æµ‹å¤§èŒƒå›´éå»ºç­‘ç‰©æƒ…å†µ"""
        with self.no_poi_tracker_lock:
            # è®¡ç®—è™šæ‹Ÿæ‰¹æ¬¡IDï¼ˆåŸºäºå¤„ç†é¡ºåºï¼‰
            total_processed = sum(len(batch_results) for batch_results in self.no_poi_batch_tracker.values())
            batch_id = total_processed // batch_size
            
            if batch_id not in self.no_poi_batch_tracker:
                self.no_poi_batch_tracker[batch_id] = []
            
            self.no_poi_batch_tracker[batch_id].append(result)
            
            # å½“æ‰¹æ¬¡æ»¡äº†ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆè­¦å‘Š
            if len(self.no_poi_batch_tracker[batch_id]) >= batch_size:
                self._check_no_poi_batch_warning(batch_id)
    
    def _check_no_poi_batch_warning(self, batch_id):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆno_poiæ‰¹æ¬¡è­¦å‘Š"""
        batch_results = self.no_poi_batch_tracker.get(batch_id, [])
        if not batch_results:
            return
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in batch_results if r.get('success', False))
        total_count = len(batch_results)
        
        # æ£€æŸ¥ä¸åŒç±»å‹çš„ç»“æœ
        # æŠ€æœ¯æˆåŠŸä½†æ— POIæ•°æ®
        no_poi_count = sum(1 for r in batch_results 
                          if r.get('success', False) and 
                             r.get('result_type') == 'no_poi_data')
        
        # æŠ€æœ¯æˆåŠŸä¸”æœ‰POIæ•°æ®
        has_poi_count = sum(1 for r in batch_results 
                           if r.get('success', False) and 
                              r.get('data') is not None)
        
        # é…’åº—ç±»åˆ«é¡µé¢
        hotel_category_count = sum(1 for r in batch_results 
                                  if r.get('success', False) and 
                                     r.get('result_type') == 'hotel_category_page_skipped')
        
        # æ£€æŸ¥ç‰¹å®šæ ¼å¼åœ°å€çš„éå»ºç­‘ç‰©æƒ…å†µ (å¦‚: 3-chÅme-5-23+åœ°å)
        import re
        chome_format_pattern = r'\d+-ch[Åo]me-\d+-\d+\+\w+'  # åŒ¹é… "æ•°å­—-chÅme-æ•°å­—-æ•°å­—+åœ°å" æ ¼å¼
        
        chome_format_no_poi = sum(1 for r in batch_results 
                                if r.get('success', False) and
                                r.get('result_type') == 'no_poi_data' and
                                re.search(chome_format_pattern, str(r.get('address', '')), re.IGNORECASE))
        
        # è§¦å‘è­¦å‘Šçš„æ¡ä»¶ï¼šæ•´ä¸ªæ‰¹æ¬¡æŠ€æœ¯å¤±è´¥ç‡100% æˆ– chÅmeæ ¼å¼åœ°å€80%ä»¥ä¸Šæ— POI
        fail_count = total_count - success_count
        should_warn = (fail_count == total_count) or \
                     (chome_format_no_poi >= total_count * 0.8)
        
        if should_warn:
            self._generate_no_poi_warning(batch_id, batch_results, chome_format_no_poi > 0)
    
    def _generate_no_poi_warning(self, batch_id, batch_results, is_chome_format=False):
        """ç”Ÿæˆno_poiæ‰¹æ¬¡è­¦å‘Š"""
        district_name = getattr(self, 'current_district_name', 'å½“å‰åŒºåŸŸ')
        
        # è¾“å‡ºé†’ç›®çš„è­¦å‘Šä¿¡æ¯
        warning_type = "chÅmeæ ¼å¼åœ°å€å¤§èŒƒå›´éå»ºç­‘ç‰©" if is_chome_format else "æ‰¹æ¬¡å…¨éƒ¨å¤±è´¥"
        print(f"\n{'='*70}")
        print(f"âš ï¸  è­¦å‘Š: {warning_type}æ£€æµ‹ï¼")
        print(f"{'='*70}")
        print(f"åŒºåŸŸ: {district_name}")
        print(f"è™šæ‹Ÿæ‰¹æ¬¡: {batch_id + 1}")
        print(f"åœ°å€æ•°é‡: {len(batch_results)}")
        
        if is_chome_format:
            print(f"çŠ¶æ€: chÅmeæ ¼å¼åœ°å€å¤§èŒƒå›´éå»ºç­‘ç‰©æƒ…å†µ")
            print(f"\nç‰¹å¾:")
            print(f"  - åŒ¹é…æ ¼å¼: æ•°å­—-chÅme-æ•°å­—-æ•°å­—+åœ°å")
            print(f"  - å…¸å‹ç¤ºä¾‹: 3-chÅme-5-23+Ikejiri")
            print(f"  - å¤§éƒ¨åˆ†åœ°å€æ— POIæ•°æ®")
            print(f"  - é€šå¸¸æ˜¯ä½å®…åŒºæˆ–éå•†ä¸šåŒºåŸŸ")
        else:
            print(f"çŠ¶æ€: æ‰€æœ‰åœ°å€éƒ½æœªæ‰¾åˆ°POIæ•°æ®ï¼ˆ100%å¤±è´¥ï¼‰")
        
        print(f"\nå¯èƒ½çš„åŸå› :")
        print(f"  1. chÅmeæ ¼å¼åœ°å€åœ¨Google Mapsä¸­å®šä½å›°éš¾")
        print(f"  2. è¿™ç±»åœ°å€é€šå¸¸æŒ‡å‘ä½å®…åŒºï¼Œå•†ä¸šPOIç¨€å°‘")
        print(f"  3. åœ°å€æ•°æ®å¯èƒ½éœ€è¦æ ¼å¼è½¬æ¢")
        print(f"  4. Google Maps APIå¯¹æ­¤ç±»æ ¼å¼æ”¯æŒæœ‰é™")
        print(f"\nå»ºè®®æ“ä½œ:")
        print(f"  1. æ£€æŸ¥CSVæ–‡ä»¶ä¸­chÅmeæ ¼å¼åœ°å€çš„å æ¯”")
        print(f"  2. å°è¯•å°†åœ°å€è½¬æ¢ä¸ºæ ‡å‡†æ—¥å¼åœ°å€æ ¼å¼")
        print(f"  3. è€ƒè™‘è¿‡æ»¤æˆ–é¢„å¤„ç†è¿™ç±»æ ¼å¼çš„åœ°å€")
        print(f"  4. éªŒè¯æ ·æœ¬åœ°å€åœ¨Google Mapsä¸Šçš„å¯è®¿é—®æ€§")
        print(f"{'='*70}\n")
        
        # è®°å½•è¯¦ç»†ä¿¡æ¯åˆ°æ—¥å¿—æ–‡ä»¶
        area_suffix = "_chome_format" if is_chome_format else ""
        log_file = self.no_poi_warnings_dir / f"{district_name}_batch_{batch_id + 1}{area_suffix}_warning.log"
        
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write(f"è­¦å‘Šæ—¥å¿— - {district_name} æ‰¹æ¬¡ {batch_id + 1}\n")
                f.write(f"æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ‰¹æ¬¡å¤§å°: {len(batch_results)}\n")
                f.write(f"è­¦å‘Šç±»å‹: {warning_type}\n")
                
                # åˆ†æchÅmeæ ¼å¼åœ°å€çš„ç»Ÿè®¡
                if is_chome_format:
                    import re
                    chome_pattern = r'\d+-ch[Åo]me-\d+-\d+\+\w+'
                    chome_addresses = [r for r in batch_results 
                                     if re.search(chome_pattern, str(r.get('address', '')), re.IGNORECASE)]
                    f.write(f"chÅmeæ ¼å¼åœ°å€æ•°é‡: {len(chome_addresses)}\n")
                    f.write(f"chÅmeæ ¼å¼å æ¯”: {(len(chome_addresses)/len(batch_results)*100):.1f}%\n")
                
                success_count = sum(1 for r in batch_results if r.get('success', False))
                f.write(f"æˆåŠŸç‡: {(success_count/len(batch_results)*100):.1f}%\n")
                f.write(f"å¤±è´¥ç‡: {((len(batch_results)-success_count)/len(batch_results)*100):.1f}%\n")
                
                f.write(f"\nåœ°å€åˆ—è¡¨åŠç»“æœ:\n")
                f.write("-" * 80 + "\n")
                for i, r in enumerate(batch_results, 1):
                    status = "æˆåŠŸ" if r.get('success', False) else "å¤±è´¥"
                    address = str(r.get('address', 'unknown'))[:60]
                    error = str(r.get('error', ''))[:40] if not r.get('success', False) else ""
                    f.write(f"{i:3d}. [{status}] {address}\n")
                    if error:
                        f.write(f"     é”™è¯¯: {error}\n")
            
            print(f"è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜åˆ°: {log_file}\n")
        except Exception as e:
            print(f"æ— æ³•ä¿å­˜è­¦å‘Šæ—¥å¿—: {e}")
    
    def _final_deduplication(self):
        """æœ€ç»ˆå»é‡"""
        if self.output_file is None or not self.output_file.exists():
            return
            
        try:
            print("æ­£åœ¨è¿›è¡Œæœ€ç»ˆå»é‡å¤„ç†...")
            df = pd.read_csv(self.output_file, encoding='utf-8-sig')
            original_count = len(df)
            
            df_deduped = df.drop_duplicates(subset=['name', 'lat', 'lng'], keep='first')
            final_count = len(df_deduped)
            
            if original_count > final_count:
                df_deduped.to_csv(self.output_file, index=False, encoding='utf-8-sig')
                print(f"å»é‡å®Œæˆ: {original_count} â†’ {final_count} (åˆ é™¤äº† {original_count - final_count} ä¸ªé‡å¤é¡¹)")
            else:
                print("æœªå‘ç°é‡å¤æ•°æ®")
        except Exception as e:
            print(f"å»é‡å¤„ç†å¤±è´¥: {e}")
    
    async def crawl_from_csv_async(self, input_file):
        """å¼‚æ­¥åç¨‹æ¨¡å¼çˆ¬å–CSVæ–‡ä»¶"""
        try:
            df = pd.read_csv(input_file)
        except Exception as e:
            print(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}")
            return 0, 1
        
        # å‡†å¤‡åœ°å€æ•°æ® - ä¼˜å…ˆä½¿ç”¨FormattedAddress
        addresses = []
        for index, row in df.iterrows():
            address_obj = {
                'primary': None,
                'secondary': None,
                'fallback': None,
                'index': index
            }
            
            # ä¼˜å…ˆä½¿ç”¨FormattedAddress
            if 'FormattedAddress' in df.columns and pd.notna(row['FormattedAddress']) and row['FormattedAddress'].strip():
                address_obj['primary'] = row['FormattedAddress'].strip()
            
            # å…¶æ¬¡ä½¿ç”¨Address
            if 'Address' in df.columns and pd.notna(row['Address']):
                address_obj['secondary'] = row['Address']
            
            # æœ€åä½¿ç”¨ConvertedAddress
            if 'ConvertedAddress' in df.columns and pd.notna(row['ConvertedAddress']) and row['ConvertedAddress'].strip():
                address_obj['fallback'] = row['ConvertedAddress'].strip()
            
            # åœ°å€é¢„å¤„ç†ï¼šå¦‚æœprimaryä¸ºç©ºï¼Œä¾æ¬¡ä½¿ç”¨secondaryå’Œfallback
            if not address_obj['primary'] and address_obj['secondary']:
                address_obj['primary'] = address_obj['secondary']
                address_obj['secondary'] = address_obj['fallback']
                address_obj['fallback'] = None
            elif not address_obj['primary'] and address_obj['fallback']:
                address_obj['primary'] = address_obj['fallback']
                address_obj['fallback'] = None
            
            # è®¾ç½®ä¸»è¦addressé”®ä¾›å¼‚æ­¥ä½¿ç”¨
            address_obj['address'] = address_obj['primary']
            
            addresses.append(address_obj)
        
        district_name = self._extract_district_name(input_file)
        total_addresses = len(addresses)
        
        # è®¾ç½®è¿›åº¦æ–‡ä»¶
        self.progress_file = self.progress_dir / f"{district_name}_progress.json"
        
        # æ–­ç‚¹ç»­ä¼ é€»è¾‘
        start_idx = 0
        total_success = 0
        total_errors = 0
        
        progress_data = self._load_progress(district_name)
        if progress_data:
            print(f"å‘ç°æ–­ç‚¹ç»­ä¼ æ–‡ä»¶: {self.progress_file}")
            start_idx = progress_data.get('completed_count', 0)
            total_success = progress_data.get('total_success', 0)
            total_errors = progress_data.get('total_errors', 0)
            
            # æ¢å¤è¾“å‡ºæ–‡ä»¶
            if progress_data.get('output_file'):
                self.output_file = Path(progress_data['output_file'])
                print(f"æ¢å¤è¾“å‡ºæ–‡ä»¶: {self.output_file}")
            else:
                self.output_file = self.output_dir / f"{district_name}_poi_data_{int(time.time())}.csv"
                
            print(f"ä»ç¬¬ {start_idx} ä¸ªåœ°å€ç»§ç»­å¤„ç† (å·²å®Œæˆ: {start_idx}, æˆåŠŸ: {total_success}, å¤±è´¥: {total_errors})")
        else:
            print(f"å¼€å§‹æ–°çš„{district_name}å¼‚æ­¥çˆ¬å–ä»»åŠ¡")
            self.output_file = self.output_dir / f"{district_name}_poi_data_{int(time.time())}.csv"
            
        # è¿‡æ»¤æ‰å·²å¤„ç†çš„åœ°å€
        addresses = addresses[start_idx:]
        
        # å¼‚æ­¥æ¨¡å¼ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹æ¬¡å¤§å°
        async_batch_size = 125  # æ¯æ‰¹å¤„ç†125ä¸ªåœ°å€ï¼Œå‡å°‘ç£ç›˜IOé¢‘ç‡
        
        print(f"ğŸš€ å¼‚æ­¥åç¨‹+å¤šè¿›ç¨‹æ¨¡å¼å¯åŠ¨")
        print(f"æ•°æ®é›†: {district_name} (æ€»è®¡: {total_addresses} ä¸ªåœ°å€, å¾…å¤„ç†: {len(addresses)})")
        print(f"ğŸ“Š é…ç½®å‚æ•°:")
        print(f"   çº¿ç¨‹æ± : {self.max_workers} ä¸ªçº¿ç¨‹ (æ¨è: {mp.cpu_count()}æ ¸ Ã— 3)")
        print(f"   Chromeæ± : {self.driver_pool.max_drivers} ä¸ªå®ä¾‹ (å†…å­˜é¢„ä¼°: {self.driver_pool.max_drivers * 350}MB)")
        print(f"   å¹¶å‘é™åˆ¶: {min(25, self.max_workers)} ä¸ªåŒæ—¶çˆ¬è™«ä»»åŠ¡")
        print(f"   æ‰¹æ¬¡å¤§å°: {async_batch_size} ä¸ªåœ°å€/æ‰¹æ¬¡ (å¼‚æ­¥ä¼˜åŒ–)")
        print(f"   è¾“å‡ºæ–‡ä»¶: {self.output_file}")
        if start_idx > 0:
            print(f"   æ–­ç‚¹ç»­ä¼ : ä»ç¬¬ {start_idx} ä¸ªåœ°å€ç»§ç»­")
        
        # é…ç½®åˆç†æ€§æ£€æŸ¥
        if self.max_workers > self.driver_pool.max_drivers:
            print(f"âš ï¸  è­¦å‘Š: çº¿ç¨‹æ•°({self.max_workers}) > Driveræ•°({self.driver_pool.max_drivers})ï¼Œå¯èƒ½å¯¼è‡´ç«äº‰ç­‰å¾…")
        
        memory_estimate = self.driver_pool.max_drivers * 350 / 1024  # GB
        if memory_estimate > 16:
            print(f"âš ï¸  è­¦å‘Š: Chromeå†…å­˜é¢„ä¼° {memory_estimate:.1f}GBï¼Œå¯èƒ½è¶…å‡ºç³»ç»Ÿå®¹é‡")
        
        # æ˜¾ç¤ºåˆå§‹å†…å­˜çŠ¶æ€
        initial_memory = monitor_memory_usage()
        if initial_memory:
            print(f"åˆå§‹å†…å­˜: {initial_memory['system_memory_percent']:.1f}%")
        
        # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤´éƒ¨ï¼ˆåªæœ‰åœ¨æ–°ä»»åŠ¡æ—¶ï¼‰
        if start_idx == 0:
            header_df = pd.DataFrame(columns=['name', 'rating', 'class', 'add', 'comment_count', 'blt_name', 'lat', 'lng'])
            header_df.to_csv(self.output_file, index=False, encoding='utf-8-sig')
        
        # å¯åŠ¨å¼‚æ­¥æ•°æ®ä¿å­˜ç®¡ç†å™¨
        save_manager = AsyncDataSaveManager(str(self.output_file), max_queue_size=50)
        save_manager.start()
        
        # åˆ›å»ºå¼‚æ­¥åŒ…è£…å™¨
        async_wrapper = AsyncCrawlWrapper(
            crawler_instance=self,
            driver_pool=self.driver_pool,
            max_workers=self.max_workers
        )
        
        try:
            # æ‰¹é‡å¼‚æ­¥å¤„ç†
            success_count = 0
            error_count = 0
            batch_data = []
            processed_count = 0
            last_progress_save = time.time()  # è¿›åº¦ä¿å­˜æ—¶é—´æˆ³
            
            # ç´¯ç§¯å†å²ç»Ÿè®¡
            accumulated_success = total_success
            accumulated_errors = total_errors
            
            for i in range(0, len(addresses), async_batch_size):
                batch_addresses = addresses[i:i + async_batch_size]
                print(f"\nå¤„ç†æ‰¹æ¬¡ {i//async_batch_size + 1}/{(len(addresses)-1)//async_batch_size + 1}")
                
                # å¼‚æ­¥æ‰¹é‡å¤„ç†
                batch_results = await async_wrapper.crawl_batch_async(batch_addresses)
                
                # å¤„ç†ç»“æœ
                for result in batch_results:
                    if isinstance(result, Exception):
                        error_count += 1
                        print(f"ä»»åŠ¡å¼‚å¸¸: {result}")
                        continue
                    
                    if result.get('success'):
                        success_count += 1
                        # åªæœ‰çœŸæ­£æœ‰æ•°æ®çš„æ‰æ·»åŠ åˆ°batch_data
                        if result.get('data') is not None:
                            batch_data.append(result['data'])
                    else:
                        error_count += 1
                
                # æ›´æ–°å·²å¤„ç†æ•°é‡
                processed_count += len(batch_addresses)
                
                # å®šæœŸä¿å­˜è¿›åº¦ - æ¯2åˆ†é’Ÿæˆ–æ¯500ä¸ªåœ°å€
                current_time = time.time()
                if (current_time - last_progress_save > 120) or (processed_count % 500 == 0):
                    await self._save_progress_async(district_name, start_idx + processed_count, total_addresses, 
                                                   accumulated_success + success_count, accumulated_errors + error_count)
                    last_progress_save = current_time
                
                # å¤šè¿›ç¨‹å¼‚æ­¥ä¿å­˜æ•°æ®
                if batch_data:
                    # æäº¤åˆ°å¤šè¿›ç¨‹ä¿å­˜é˜Ÿåˆ—ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
                    save_success = save_manager.save_batch_async(batch_data.copy())
                    if save_success:
                        print(f"ğŸ“¤ æäº¤ {len(batch_data)} æ¡æ•°æ®åˆ°ä¿å­˜é˜Ÿåˆ—")
                        batch_data = []  # æˆåŠŸæäº¤åæ¸…ç©ºç¼“å­˜
                    else:
                        print(f"âš ï¸  æ•°æ®ä¿å­˜é˜Ÿåˆ—ç¹å¿™ï¼Œä¿æŒæ•°æ®åœ¨å†…å­˜ä¸­")
                        # å¦‚æœé˜Ÿåˆ—æ»¡äº†ï¼Œæš‚æ—¶ä¿æŒæ•°æ®ä¸æ¸…ç©º
                        if save_manager.get_queue_size() < 10:  # é˜Ÿåˆ—ä¸å¤ªæ»¡æ—¶å†æ¸…ç©º
                            batch_data = []
                
                # æ˜¾ç¤ºè¿›åº¦å’Œèµ„æºçŠ¶æ€
                progress = (i + len(batch_addresses)) / len(addresses) * 100
                resources = monitor_system_resources()
                queue_size = save_manager.get_queue_size()
                
                # è·å–è°ƒåº¦å™¨çŠ¶æ€
                scheduler_status = async_wrapper.resource_scheduler.get_status()
                
                if resources:
                    resource_str = (f" - å†…å­˜: {resources['system_memory_percent']:.1f}% "
                                  f"CPU: {resources['cpu_percent']:.1f}% "
                                  f"è´Ÿè½½: {resources['load_average']:.1f}")
                    queue_str = f" - ä¿å­˜é˜Ÿåˆ—: {queue_size}"
                    scheduler_str = ""
                    
                    if scheduler_status['is_paused']:
                        scheduler_str = f" - ğŸ›‘è°ƒåº¦å·²æš‚åœ"
                    elif scheduler_status['pause_count'] > 0:
                        scheduler_str = f" - æš‚åœ/æ¢å¤: {scheduler_status['pause_count']}/{scheduler_status['resume_count']}"
                        
                    # æ·»åŠ æˆåŠŸç±»å‹ç»Ÿè®¡
                    technical_success = success_count + error_count  # æ€»çš„å·²å¤„ç†æ•°
                    if technical_success > 0:
                        success_rate = (success_count / technical_success) * 100
                        print(f"è¿›åº¦: {progress:.1f}% - æˆåŠŸ: {success_count}, å¤±è´¥: {error_count} (æˆåŠŸç‡: {success_rate:.1f}%){resource_str}{queue_str}{scheduler_str}")
                    else:
                        print(f"è¿›åº¦: {progress:.1f}% - æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}{resource_str}{queue_str}{scheduler_str}")
                else:
                    print(f"è¿›åº¦: {progress:.1f}% - æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")
                
                # èµ„æºè¿‡è½½æ£€æŸ¥å’Œåé¦ˆ
                if resources:
                    if resources['system_memory_percent'] > 85:
                        print(f"âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {resources['system_memory_percent']:.1f}%")
                        gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    
                    if resources['cpu_percent'] > 85:
                        print(f"âš ï¸  CPUä½¿ç”¨ç‡è¿‡é«˜: {resources['cpu_percent']:.1f}%")
                    
                    if resources['load_average'] > 8.0:
                        print(f"âš ï¸  ç³»ç»Ÿè´Ÿè½½è¿‡é«˜: {resources['load_average']:.2f}")
                    
                    # æ£€æŸ¥Chromeè¿›ç¨‹æ•°é‡
                    if resources['chrome_processes'] > 50:
                        print(f"âš ï¸  Chromeè¿›ç¨‹è¿‡å¤š: {resources['chrome_processes']}")
                        
                    # æç«¯æƒ…å†µï¼šå¼ºåˆ¶æš‚åœ
                    if resources['system_memory_percent'] > 95:
                        print(f"ğŸš¨ å†…å­˜å±é™©ï¼å¼ºåˆ¶æš‚åœ5ç§’")
                        async_wrapper.resource_scheduler.force_pause()
                        await asyncio.sleep(5)
                        async_wrapper.resource_scheduler.force_resume()
            
            # æäº¤å‰©ä½™æ•°æ®
            if batch_data:
                print(f"ğŸ“¤ æäº¤æœ€å {len(batch_data)} æ¡æ•°æ®åˆ°ä¿å­˜é˜Ÿåˆ—")
                save_manager.save_batch_async(batch_data)
            
            print(f"\nâœ… å¼‚æ­¥å¤„ç†å®Œæˆ")
            print(f"æˆåŠŸ: {success_count}, å¤±è´¥: {error_count}")
            print(f"ç­‰å¾…æ•°æ®ä¿å­˜è¿›ç¨‹å®Œæˆ...")
            
            # æœ€ç»ˆä¿å­˜è¿›åº¦
            await self._save_progress_async(district_name, start_idx + processed_count, total_addresses, 
                                           accumulated_success + success_count, accumulated_errors + error_count)
            
            # ç­‰å¾…ä¿å­˜é˜Ÿåˆ—æ¸…ç©º
            timeout_count = 0
            while save_manager.get_queue_size() > 0 and timeout_count < 30:
                print(f"â³ ç­‰å¾…ä¿å­˜é˜Ÿåˆ—æ¸…ç©ºï¼Œå‰©ä½™: {save_manager.get_queue_size()}")
                time.sleep(2)
                timeout_count += 1
            
            return success_count, error_count
            
        finally:
            # æ¸…ç†èµ„æº
            print("ğŸ§¹ æ¸…ç†èµ„æºä¸­...")
            
            # è¾“å‡ºè°ƒåº¦å™¨ç»Ÿè®¡ä¿¡æ¯
            scheduler_status = async_wrapper.resource_scheduler.get_status()
            if scheduler_status['pause_count'] > 0 or scheduler_status['resume_count'] > 0:
                print(f"ğŸ“Š èµ„æºè°ƒåº¦ç»Ÿè®¡: æš‚åœ {scheduler_status['pause_count']} æ¬¡, "
                      f"æ¢å¤ {scheduler_status['resume_count']} æ¬¡")
            else:
                print("ğŸ“Š èµ„æºè°ƒåº¦: è¿è¡ŒæœŸé—´æœªè§¦å‘æš‚åœ/æ¢å¤")
            
            async_wrapper.cleanup()
            save_manager.stop()  # åœæ­¢æ•°æ®ä¿å­˜è¿›ç¨‹
    
    def crawl_from_csv_turbo(self, input_file):
        """[å·²å¼ƒç”¨] æ—§çš„åŒæ­¥æ¨¡å¼ï¼Œè‡ªåŠ¨é‡å®šå‘åˆ°å¼‚æ­¥æ¨¡å¼"""
        print("âš ï¸  crawl_from_csv_turboå·²å¼ƒç”¨ï¼Œè‡ªåŠ¨ä½¿ç”¨å¼‚æ­¥æ¨¡å¼...")
        import asyncio
        return asyncio.run(self.crawl_from_csv_async(input_file))
    def list_pending_tasks(self):
        """åˆ—å‡ºæœªå®Œæˆä»»åŠ¡"""
        progress_files = list(self.progress_dir.glob("*_progress.json"))
        if not progress_files:
            print("æ²¡æœ‰å‘ç°æœªå®Œæˆçš„ä»»åŠ¡")
            return
        
        print("æœªå®Œæˆçš„ä»»åŠ¡:")
        print("=" * 60)
        for progress_file in progress_files:
            try:
                with open(progress_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                district = data['district_name']
                completed = data['completed_count']
                total = data['total_count']
                success = data['total_success']
                errors = data['total_errors']
                timestamp = data['timestamp']
                
                progress_percent = (completed / total) * 100 if total > 0 else 0
                time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(timestamp))
                
                print(f"åŒºåŸŸ: {district}")
                print(f"è¿›åº¦: {completed}/{total} ({progress_percent:.1f}%)")
                print(f"æˆåŠŸ: {success}, å¤±è´¥: {errors}")
                print(f"æœ€åæ›´æ–°: {time_str}")
                print(f"è¾“å‡ºæ–‡ä»¶: {data['output_file']}")
                print("-" * 60)
                
            except Exception as e:
                print(f"è¯»å–è¿›åº¦æ–‡ä»¶ {progress_file} å¤±è´¥: {e}")
    
    def clean_all_progress(self):
        """æ¸…ç†æ‰€æœ‰è¿›åº¦æ–‡ä»¶"""
        progress_files = list(self.progress_dir.glob("*_progress.json"))
        if not progress_files:
            print("æ²¡æœ‰è¿›åº¦æ–‡ä»¶éœ€è¦æ¸…ç†")
            return
        
        for progress_file in progress_files:
            try:
                progress_file.unlink()
                print(f"å·²æ¸…ç†: {progress_file.name}")
            except Exception as e:
                print(f"æ¸…ç†å¤±è´¥ {progress_file.name}: {e}")
        
        print(f"å…±æ¸…ç†äº† {len(progress_files)} ä¸ªè¿›åº¦æ–‡ä»¶")

    # å·²ç§»é™¤æ—§çš„åŒæ­¥æ–¹æ³• crawl_all_districtsï¼Œè¯·ä½¿ç”¨ crawl_all_districts_async

    async def crawl_all_districts_async(self, input_dir="data/input"):
        """å¼‚æ­¥æ‰¹é‡å¤„ç†æ‰€æœ‰åŒºæ–‡ä»¶"""
        input_path = Path(input_dir)
        csv_files = list(input_path.glob("*.csv"))
        
        if not csv_files:
            print(f"åœ¨ {input_dir} ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶")
            return
        
        print(f"å‘ç° {len(csv_files)} ä¸ªåŒºæ–‡ä»¶ï¼Œå¼€å§‹å¼‚æ­¥æ‰¹é‡å¤„ç†...\n")
        
        all_success = 0
        all_errors = 0
        processed_districts = []
        
        start_time = time.time()
        
        for i, csv_file in enumerate(csv_files):
            district_name = self._extract_district_name(csv_file)
            print(f"{'='*60}")
            print(f"å¤„ç†ç¬¬ {i+1}/{len(csv_files)} ä¸ªåŒº: {district_name}")
            print(f"{'='*60}")
            
            try:
                success, errors = await self.crawl_from_csv_async(csv_file)
                all_success += success
                all_errors += errors
                processed_districts.append(f"{district_name}: æˆåŠŸ{success}, å¤±è´¥{errors}")
                
            except Exception as e:
                print(f"å¤„ç† {district_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                all_errors += 1
                processed_districts.append(f"{district_name}: å¤„ç†å¤±è´¥")
                continue
            
            print(f"\n{district_name} å¤„ç†å®Œæˆ\n")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"{'='*60}")
        print(f"å…¨éƒ¨åŒºåŸŸå¤„ç†å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
        print(f"æ€»æˆåŠŸ: {all_success}")
        print(f"æ€»å¤±è´¥: {all_errors}")
        print(f"å¤„ç†äº† {len(processed_districts)} ä¸ªåŒº:")
        
        for district_summary in processed_districts:
            print(f"  {district_summary}")

    # å·²ç§»é™¤æ—§çš„åŒæ­¥æ–¹æ³• crawl_multiple_filesï¼Œè¯·ä½¿ç”¨ crawl_multiple_files_async

    async def crawl_multiple_files_async(self, file_paths):
        """å¼‚æ­¥å¤„ç†å¤šä¸ªæŒ‡å®šæ–‡ä»¶"""
        print(f"å‡†å¤‡å¼‚æ­¥å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...\n")
        
        all_success = 0
        all_errors = 0
        processed_files = []
        
        start_time = time.time()
        
        for i, file_path in enumerate(file_paths):
            file_name = os.path.basename(file_path)
            print(f"{'='*60}")
            print(f"å¤„ç†ç¬¬ {i+1}/{len(file_paths)} ä¸ªæ–‡ä»¶: {file_name}")
            print(f"{'='*60}")
            
            try:
                success, errors = await self.crawl_from_csv_async(file_path)
                all_success += success
                all_errors += errors
                processed_files.append(f"{file_name}: æˆåŠŸ{success}, å¤±è´¥{errors}")
                print(f"\n{file_name} å®Œæˆ - æˆåŠŸ: {success}, å¤±è´¥: {errors}\n")
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {e}")
                processed_files.append(f"{file_name}: å¤„ç†å¤±è´¥")
                continue
        
        total_time = time.time() - start_time
        
        print(f"{'='*60}")
        print(f"æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")
        print(f"{'='*60}")
        print(f"æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
        print(f"æ€»æˆåŠŸ: {all_success}")
        print(f"æ€»å¤±è´¥: {all_errors}")
        print(f"å¤„ç†äº† {len(processed_files)} ä¸ªæ–‡ä»¶:")
        
        for file_summary in processed_files:
            print(f"  {file_summary}")


def main():
    parser = argparse.ArgumentParser(description='POIçˆ¬è™« - ä¼˜åŒ–ç‰ˆæœ¬')
    parser.add_argument('input_files', nargs='*', help='è¾“å…¥CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥æŒ‡å®šå¤šä¸ªï¼‰')
    parser.add_argument('--all', action='store_true', help='æ‰¹é‡å¤„ç†æ‰€æœ‰åŒºæ–‡ä»¶')
    parser.add_argument('--pattern', type=str, help='ä½¿ç”¨é€šé…ç¬¦æ¨¡å¼é€‰æ‹©æ–‡ä»¶ï¼Œå¦‚ "*åŒº_complete*.csv"')
    parser.add_argument('--file-list', type=str, help='ä»æ–‡ä»¶ä¸­è¯»å–è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªæ–‡ä»¶è·¯å¾„ï¼‰')
    parser.add_argument('--no-resume', action='store_true', help='ç¦ç”¨æ–­ç‚¹ç»­ä¼ åŠŸèƒ½')
    parser.add_argument('--workers', type=int, default=None, help='å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼ˆé»˜è®¤ï¼šCPUæ ¸å¿ƒæ•°Ã—2ï¼‰')
    parser.add_argument('--batch-size', type=int, default=150, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--status', action='store_true', help='æŸ¥çœ‹æœªå®Œæˆä»»åŠ¡çŠ¶æ€')
    parser.add_argument('--clean-progress', action='store_true', help='æ¸…ç†æ‰€æœ‰è¿›åº¦æ–‡ä»¶')
    # ç§»é™¤ --async å‚æ•°ï¼Œå¼‚æ­¥æ¨¡å¼ç°åœ¨æ˜¯é»˜è®¤ä¸”å”¯ä¸€çš„æ¨¡å¼
    
    args = parser.parse_args()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ç”¨äºç®¡ç†åŠŸèƒ½
    crawler = ParallelPOICrawler(enable_resume=True)
    
    # å¤„ç†ç®¡ç†å‘½ä»¤
    if args.status:
        crawler.list_pending_tasks()
        return
    
    if args.clean_progress:
        crawler.clean_all_progress()
        return
    
    if not args.input_files and not args.all and not args.pattern and not args.file_list:
        print("ç”¨æ³•:")
        print("  å•ä¸ªæ–‡ä»¶: python parallel_poi_crawler_turbo.py <è¾“å…¥CSVæ–‡ä»¶> [é€‰é¡¹]")
        print("  å¤šä¸ªæ–‡ä»¶: python parallel_poi_crawler_turbo.py <æ–‡ä»¶1> <æ–‡ä»¶2> ... [é€‰é¡¹]")
        print('  é€šé…ç¬¦:   python parallel_poi_crawler_turbo.py --pattern "*åŒº_complete*.csv" [é€‰é¡¹]')
        print("  æ–‡ä»¶åˆ—è¡¨: python parallel_poi_crawler_turbo.py --file-list files.txt [é€‰é¡¹]")
        print("  æ‰¹é‡å¤„ç†: python parallel_poi_crawler_turbo.py --all [é€‰é¡¹]")
        print("  è¿›åº¦ç®¡ç†: python parallel_poi_crawler_turbo.py --status | --clean-progress")
        print("")
        print("é€‰é¡¹:")
        print("  --pattern PATTERN  ä½¿ç”¨é€šé…ç¬¦æ¨¡å¼é€‰æ‹©æ–‡ä»¶")
        print("  --file-list FILE   ä»æ–‡ä»¶ä¸­è¯»å–æ–‡ä»¶åˆ—è¡¨")
        print("  --no-resume        ç¦ç”¨æ–­ç‚¹ç»­ä¼ åŠŸèƒ½")
        print("  --workers N        è®¾ç½®å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°ï¼ˆæ¨èï¼šCPUæ ¸å¿ƒæ•°*4-8ï¼‰")
        print("  --batch-size N     è®¾ç½®æ‰¹æ¬¡å¤§å°")
        print("  --status          æŸ¥çœ‹æœªå®Œæˆä»»åŠ¡çŠ¶æ€")
        print("  --clean-progress  æ¸…ç†æ‰€æœ‰è¿›åº¦æ–‡ä»¶")
        # ç§»é™¤ --async é€‰é¡¹è¯´æ˜ï¼Œå¼‚æ­¥æ¨¡å¼ç°åœ¨æ˜¯é»˜è®¤çš„
        print("")
        print("ğŸ“Š æœ€ä½³å®è·µé…ç½®ï¼ˆ12æ ¸32GBç³»ç»Ÿï¼‰ï¼š")
        print("  - çº¿ç¨‹æ•°: 36ä¸ª (æ ¸å¿ƒæ•° Ã— 3ï¼ŒIOå¯†é›†å‹ä¼˜åŒ–)")
        print("  - Chromeå®ä¾‹: 27ä¸ª (çº¿ç¨‹æ•° Ã— 0.75ï¼Œé¿å…å†…å­˜è¿‡è½½)")  
        print("  - åŒæ—¶å¹¶å‘: 25ä¸ªçˆ¬è™«ä»»åŠ¡ (é¿å…driverç«äº‰)")
        print("  - å†…å­˜é™åˆ¶: æ¯ä¸ªChrome 350MB (æ€»è®¡çº¦10.5GB)")
        print("  - æ‰¹æ¬¡å¤§å°: 125ä¸ªåœ°å€ (å‡å°‘ç£ç›˜IOé¢‘ç‡)")
        print("")
        print("ğŸ›ï¸ æ™ºèƒ½ç‰¹æ€§ï¼š")
        print("  - åŠ¨æ€èµ„æºæ„ŸçŸ¥è°ƒåº¦ (å†…å­˜/CPUè¿‡è½½è‡ªåŠ¨æš‚åœ)")
        print("  - å¤šè¿›ç¨‹æ•°æ®ä¿å­˜ (CPUå¯†é›†å‹ä»»åŠ¡éš”ç¦»)")
        print("  - Chromeç”Ÿå‘½å‘¨æœŸç®¡ç† (100ä»»åŠ¡åè‡ªåŠ¨é‡å¯)")
        print("  - å¼‚æ­¥åç¨‹+çº¿ç¨‹æ±  (é»˜è®¤é«˜æ•ˆIOå¤„ç†)")
        print("  - Driveré¢„åˆ†é…æœºåˆ¶ (é¿å…ç«äº‰ç­‰å¾…)")
        return
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    enable_resume = not args.no_resume
    crawler = ParallelPOICrawler(
        max_workers=args.workers, 
        batch_size=args.batch_size,
        enable_resume=enable_resume
    )
    
    print(f"å¯åŠ¨ä¼˜åŒ–æ¨¡å¼ï¼Œä½¿ç”¨ {crawler.max_workers} ä¸ªå¹¶å‘çº¿ç¨‹")
    print(f"Chromeé©±åŠ¨æ± : {crawler.driver_pool.max_drivers} ä¸ªå®ä¾‹")
    
    # æ”¶é›†è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    files_to_process = []
    
    if args.all:
        print("ğŸš€ ä½¿ç”¨å¼‚æ­¥åç¨‹æ¨¡å¼")
        asyncio.run(crawler.crawl_all_districts_async())
        return
    
    if args.input_files:
        files_to_process.extend(args.input_files)
    
    if args.pattern:
        import glob
        pattern_files = glob.glob(args.pattern)
        if pattern_files:
            files_to_process.extend(pattern_files)
            print(f"é€šé…ç¬¦ '{args.pattern}' åŒ¹é…åˆ° {len(pattern_files)} ä¸ªæ–‡ä»¶")
        else:
            print(f"è­¦å‘Š: é€šé…ç¬¦ '{args.pattern}' æ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•æ–‡ä»¶")
    
    if args.file_list:
        if os.path.exists(args.file_list):
            with open(args.file_list, 'r', encoding='utf-8') as f:
                list_files = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                files_to_process.extend(list_files)
                print(f"ä» '{args.file_list}' è¯»å–äº† {len(list_files)} ä¸ªæ–‡ä»¶")
        else:
            print(f"è­¦å‘Š: æ–‡ä»¶åˆ—è¡¨ '{args.file_list}' ä¸å­˜åœ¨")
    
    files_to_process = list(dict.fromkeys(files_to_process))
    valid_files = []
    
    for file_path in files_to_process:
        if os.path.exists(file_path):
            valid_files.append(file_path)
        else:
            print(f"è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ - {file_path}")
    
    if not valid_files:
        print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡ä»¶è¿›è¡Œå¤„ç†")
        return
    
    # å¼‚æ­¥æ¨¡å¼æ˜¯é»˜è®¤ä¸”å”¯ä¸€çš„æ¨¡å¼
    print("ğŸš€ ä½¿ç”¨å¼‚æ­¥åç¨‹æ¨¡å¼")
    
    if len(valid_files) == 1:
        # å•æ–‡ä»¶å¼‚æ­¥å¤„ç†
        asyncio.run(crawler.crawl_from_csv_async(valid_files[0]))
    else:
        # å¤šæ–‡ä»¶å¼‚æ­¥æ‰¹é‡å¤„ç†
        asyncio.run(crawler.crawl_multiple_files_async(valid_files))


if __name__ == "__main__":
    main()