#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
POIçˆ¬è™«å¿«é€Ÿå¯åŠ¨è„šæœ¬
æ”¯æŒäº¤äº’å¼æ–‡ä»¶é€‰æ‹©å’Œå¤šç§è¿è¡Œæ¨¡å¼
"""

import sys
import os
from pathlib import Path

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ POIçˆ¬è™«å¿«é€Ÿå¯åŠ¨å™¨")
    print("=" * 50)
    
    print("ğŸ“‹ é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. ğŸ–±ï¸  äº¤äº’å¼æ–‡ä»¶é€‰æ‹©æ¨¡å¼")
    print("2. âŒ¨ï¸  å‘½ä»¤è¡Œå‚æ•°æ¨¡å¼") 
    print("3. ğŸ““ Jupyter Notebookæ¨¡å¼")
    print("4. ğŸ”§ æµ‹è¯•æ¨¡å¼ (å‰5ä¸ªåœ°å€)")
    
    try:
        mode = input("\nè¯·é€‰æ‹©æ¨¡å¼ (1-4, é»˜è®¤1): ").strip()
        
        if not mode:
            mode = '1'
        
        if mode == '1':
            # äº¤äº’å¼æ–‡ä»¶é€‰æ‹©æ¨¡å¼
            print("\nğŸš€ å¯åŠ¨äº¤äº’å¼æ–‡ä»¶é€‰æ‹©...")
            from final_crawler import main
            main()
            
        elif mode == '2':
            # å‘½ä»¤è¡Œå‚æ•°æ¨¡å¼
            print("\nğŸ“‹ å‘½ä»¤è¡Œå‚æ•°æ¨¡å¼:")
            print("å¯ç”¨å‚æ•°:")
            print("  --input, -i      è¾“å…¥CSVæ–‡ä»¶è·¯å¾„")
            print("  --output, -o     è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„")
            print("  --workers, -w    å¹¶å‘çº¿ç¨‹æ•° (é»˜è®¤: 4)")
            print("  --headless       æ— å¤´æ¨¡å¼è¿è¡Œ")
            print("  --no-headless    æ˜¾ç¤ºChromeçª—å£")
            print("  --interactive    äº¤äº’å¼æ–‡ä»¶é€‰æ‹©")
            
            print(f"\nç¤ºä¾‹å‘½ä»¤:")
            print(f"python final_crawler.py --input data/input/your_file.csv --workers 2")
            print(f"python final_crawler.py --interactive")
            
            choice = input("\nç›´æ¥è¿è¡Œäº¤äº’æ¨¡å¼ï¼Ÿ(y/n): ").lower()
            if choice == 'y':
                os.system("python final_crawler.py --interactive")
            else:
                print("è¯·ä½¿ç”¨ä¸Šè¿°å‘½ä»¤è¡Œå‚æ•°è¿è¡Œ")
                
        elif mode == '3':
            # Jupyter Notebookæ¨¡å¼
            print("\nğŸ““ å¯åŠ¨Jupyter Notebook...")
            
            try:
                import subprocess
                subprocess.run(["jupyter", "notebook", "enhanced_poi_crawler.ipynb"])
            except FileNotFoundError:
                print("âŒ Jupyteræœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…: pip install jupyter")
                print("æˆ–æ‰‹åŠ¨æ‰“å¼€æ–‡ä»¶: enhanced_poi_crawler.ipynb")
            except Exception as e:
                print(f"âŒ å¯åŠ¨Jupyterå¤±è´¥: {e}")
                print("è¯·æ‰‹åŠ¨æ‰“å¼€æ–‡ä»¶: enhanced_poi_crawler.ipynb")
                
        elif mode == '4':
            # æµ‹è¯•æ¨¡å¼
            print("\nğŸ§ª å¯åŠ¨æµ‹è¯•æ¨¡å¼...")
            run_test_mode()
            
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©")
            
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {e}")

def run_test_mode():
    """æµ‹è¯•æ¨¡å¼ - å¤„ç†å‰5ä¸ªåœ°å€"""
    from file_selector import FileSelector
    from final_crawler import FinalPOICrawler
    import pandas as pd
    import time
    
    print("ğŸ§ª æµ‹è¯•æ¨¡å¼ - å¤„ç†å‰5ä¸ªåœ°å€")
    
    # æ–‡ä»¶é€‰æ‹©
    selector = FileSelector()
    csv_files = selector.scan_csv_files()
    
    if not csv_files:
        print("âŒ æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        return
    
    print(f"ğŸ“‹ ä½¿ç”¨æ–‡ä»¶: {csv_files[0][0]}")
    input_file = f"data/input/{csv_files[0][0]}"
    output_file = "test_poi_results.csv"
    
    # è¯»å–æ•°æ®
    try:
        df = pd.read_csv(input_file)
        addresses = df['Address'].dropna().tolist()[:5]
        
        print(f"ğŸ“Š æµ‹è¯•åœ°å€:")
        for i, addr in enumerate(addresses, 1):
            print(f"  {i}. {addr}")
        
        # æµ‹è¯•é…ç½®
        config = {
            'max_workers': 2,
            'driver_pool_size': 2,
            'batch_size': 5,
            'timeout': 15,
            'retry_times': 2,
            'headless': True,
            'checkpoint_interval': 5,
            'input_file': input_file,
            'output_file': output_file
        }
        
        print(f"\nğŸš€ å¼€å§‹æµ‹è¯•çˆ¬å–...")
        start_time = time.time()
        
        crawler = FinalPOICrawler(config)
        try:
            crawler.process_addresses(addresses)
            
            elapsed = time.time() - start_time
            print(f"\nâœ… æµ‹è¯•å®Œæˆï¼")
            print(f"â±ï¸ è€—æ—¶: {elapsed:.1f} ç§’")
            print(f"ğŸ“ˆ å¹³å‡: {elapsed/len(addresses):.1f} ç§’/åœ°å€")
            
            # æŸ¥çœ‹ç»“æœ
            if Path(output_file).exists():
                results = pd.read_csv(output_file)
                print(f"ğŸ“Š è·å¾— {len(results)} ä¸ªPOI")
                print(f"ğŸ“ ç»“æœæ–‡ä»¶: {output_file}")
            
        finally:
            crawler.close()
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    main()