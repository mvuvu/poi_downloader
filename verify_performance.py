#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éªŒè¯POIçˆ¬è™«æ€§èƒ½ä¸‹é™åŸå› çš„è‡ªåŠ¨åŒ–è„šæœ¬
é€šè¿‡åˆ†æç°æœ‰æ•°æ®å’Œæ—¥å¿—æ¥è¯†åˆ«é—®é¢˜
"""

import json
import os
import re
import glob
from pathlib import Path
from datetime import datetime
import pandas as pd
import time

class PerformanceVerifier:
    """æ€§èƒ½é—®é¢˜éªŒè¯å™¨"""
    
    def __init__(self):
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'findings': [],
            'recommendations': []
        }
        
    def verify_progress_files(self):
        """éªŒè¯è¿›åº¦æ–‡ä»¶ï¼Œåˆ†æä»»åŠ¡å¤„ç†é€Ÿåº¦"""
        print("\nğŸ“Š åˆ†æè¿›åº¦æ–‡ä»¶...")
        
        progress_dir = Path("data/progress")
        if not progress_dir.exists():
            print("âŒ è¿›åº¦ç›®å½•ä¸å­˜åœ¨")
            return
            
        progress_files = list(progress_dir.glob("*_simple_progress.json"))
        
        if not progress_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è¿›åº¦æ–‡ä»¶")
            return
            
        analysis = []
        
        for pf in progress_files:
            try:
                with open(pf, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # è®¡ç®—å¤„ç†é€Ÿåº¦
                if 'timestamp' in data and 'last_updated' in data:
                    duration = data['last_updated'] - data['timestamp']
                    if duration > 0:
                        speed = data['processed_tasks'] / duration
                        
                        analysis.append({
                            'file': data['file_name'],
                            'total_tasks': data['total_tasks'],
                            'processed': data['processed_tasks'],
                            'success_rate': (data['success_count'] / data['processed_tasks'] * 100) if data['processed_tasks'] > 0 else 0,
                            'speed_per_second': speed,
                            'duration_minutes': duration / 60
                        })
                        
            except Exception as e:
                print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {pf}: {e}")
                
        if analysis:
            # åˆ†æç»“æœ
            df = pd.DataFrame(analysis)
            
            print(f"\nâœ… åˆ†æäº† {len(analysis)} ä¸ªæ–‡ä»¶çš„è¿›åº¦")
            print(f"ğŸ“ˆ å¹³å‡å¤„ç†é€Ÿåº¦: {df['speed_per_second'].mean():.3f} ä»»åŠ¡/ç§’")
            print(f"ğŸ“Š é€Ÿåº¦èŒƒå›´: {df['speed_per_second'].min():.3f} - {df['speed_per_second'].max():.3f} ä»»åŠ¡/ç§’")
            print(f"âœ… å¹³å‡æˆåŠŸç‡: {df['success_rate'].mean():.1f}%")
            
            # æ£€æµ‹æ€§èƒ½é—®é¢˜
            slow_files = df[df['speed_per_second'] < 0.1]
            if len(slow_files) > 0:
                print(f"\nâš ï¸  å‘ç° {len(slow_files)} ä¸ªå¤„ç†ç¼“æ…¢çš„æ–‡ä»¶ (< 0.1 ä»»åŠ¡/ç§’):")
                for _, row in slow_files.iterrows():
                    print(f"   - {row['file']}: {row['speed_per_second']:.3f} ä»»åŠ¡/ç§’")
                    
                self.results['findings'].append({
                    'issue': 'ä»»åŠ¡å¤„ç†é€Ÿåº¦è¿‡æ…¢',
                    'detail': f'{len(slow_files)} ä¸ªæ–‡ä»¶å¤„ç†é€Ÿåº¦ä½äº 0.1 ä»»åŠ¡/ç§’',
                    'severity': 'high'
                })
                
            # é•¿æ—¶é—´è¿è¡Œçš„æ–‡ä»¶
            long_running = df[df['duration_minutes'] > 120]  # è¶…è¿‡2å°æ—¶
            if len(long_running) > 0:
                print(f"\nâ±ï¸  å‘ç° {len(long_running)} ä¸ªé•¿æ—¶é—´è¿è¡Œçš„æ–‡ä»¶ (> 2å°æ—¶):")
                for _, row in long_running.iterrows():
                    print(f"   - {row['file']}: {row['duration_minutes']:.1f} åˆ†é’Ÿ")
                    
            return df
        
    def analyze_output_files(self):
        """åˆ†æè¾“å‡ºæ–‡ä»¶ï¼ŒæŸ¥çœ‹POIæ•°é‡åˆ†å¸ƒ"""
        print("\nğŸ“ åˆ†æè¾“å‡ºæ–‡ä»¶...")
        
        output_dir = Path("data/output")
        if not output_dir.exists():
            print("âŒ è¾“å‡ºç›®å½•ä¸å­˜åœ¨")
            return
            
        csv_files = list(output_dir.glob("*.csv"))
        
        if not csv_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è¾“å‡ºæ–‡ä»¶")
            return
            
        poi_stats = []
        
        for cf in csv_files[:10]:  # åªåˆ†æå‰10ä¸ªæ–‡ä»¶
            try:
                df = pd.read_csv(cf, encoding='utf-8-sig')
                if not df.empty:
                    # ç»Ÿè®¡POIåˆ†å¸ƒ
                    poi_counts = df.groupby('blt_name').size()
                    
                    poi_stats.append({
                        'file': cf.name,
                        'total_pois': len(df),
                        'unique_buildings': len(poi_counts),
                        'avg_pois_per_building': poi_counts.mean(),
                        'max_pois': poi_counts.max()
                    })
                    
            except Exception as e:
                continue
                
        if poi_stats:
            df_stats = pd.DataFrame(poi_stats)
            
            print(f"\nâœ… åˆ†æäº† {len(poi_stats)} ä¸ªè¾“å‡ºæ–‡ä»¶")
            print(f"ğŸ“Š å¹³å‡POIæ•°/æ–‡ä»¶: {df_stats['total_pois'].mean():.1f}")
            print(f"ğŸ¢ å¹³å‡å»ºç­‘ç‰©æ•°/æ–‡ä»¶: {df_stats['unique_buildings'].mean():.1f}")
            print(f"ğŸ“ å¹³å‡POIæ•°/å»ºç­‘ç‰©: {df_stats['avg_pois_per_building'].mean():.1f}")
            
            # æ£€æµ‹é«˜POIå»ºç­‘ç‰©
            high_poi_files = df_stats[df_stats['max_pois'] > 100]
            if len(high_poi_files) > 0:
                print(f"\nâš ï¸  å‘ç°å«æœ‰å¤§é‡POIçš„å»ºç­‘ç‰© (> 100 POIs):")
                for _, row in high_poi_files.iterrows():
                    print(f"   - {row['file']}: æœ€å¤§ {row['max_pois']} POIs")
                    
                self.results['findings'].append({
                    'issue': 'å­˜åœ¨é«˜POIå¯†åº¦å»ºç­‘ç‰©',
                    'detail': f'éƒ¨åˆ†å»ºç­‘ç‰©åŒ…å«è¶…è¿‡100ä¸ªPOIï¼Œä¼šå¯¼è‡´æ»šåŠ¨æ—¶é—´è¿‡é•¿',
                    'severity': 'medium'
                })
                
    def check_system_resources(self):
        """æ£€æŸ¥ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        print("\nğŸ’» æ£€æŸ¥ç³»ç»Ÿèµ„æº...")
        
        try:
            import psutil
            
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=2)
            print(f"CPUä½¿ç”¨ç‡: {cpu_percent}%")
            
            # å†…å­˜ä½¿ç”¨
            memory = psutil.virtual_memory()
            print(f"å†…å­˜ä½¿ç”¨: {memory.percent}% ({memory.used / (1024**3):.1f}GB / {memory.total / (1024**3):.1f}GB)")
            
            # Chromeè¿›ç¨‹
            chrome_count = 0
            chrome_memory = 0
            
            for proc in psutil.process_iter(['name', 'memory_info']):
                try:
                    if 'chrome' in proc.info['name'].lower():
                        chrome_count += 1
                        chrome_memory += proc.memory_info().rss / (1024**2)
                except:
                    pass
                    
            print(f"Chromeè¿›ç¨‹: {chrome_count} ä¸ªï¼Œæ€»å†…å­˜: {chrome_memory:.0f}MB")
            
            if chrome_count > 20:
                self.results['findings'].append({
                    'issue': 'Chromeè¿›ç¨‹è¿‡å¤š',
                    'detail': f'å‘ç° {chrome_count} ä¸ªChromeè¿›ç¨‹ï¼Œå¯èƒ½å­˜åœ¨è¿›ç¨‹æ³„æ¼',
                    'severity': 'high'
                })
                
            if chrome_memory > 4000:  # 4GB
                self.results['findings'].append({
                    'issue': 'Chromeå†…å­˜å ç”¨è¿‡é«˜',
                    'detail': f'Chromeæ€»å†…å­˜å ç”¨ {chrome_memory:.0f}MB',
                    'severity': 'high'
                })
                
        except ImportError:
            print("âš ï¸  psutilæœªå®‰è£…ï¼Œè·³è¿‡ç³»ç»Ÿèµ„æºæ£€æŸ¥")
            
    def analyze_warnings(self):
        """åˆ†æè­¦å‘Šæ–‡ä»¶"""
        print("\nâš ï¸  åˆ†æè­¦å‘Šæ–‡ä»¶...")
        
        warning_dirs = [
            "data/warnings",
            "data/no_poi_warnings",
            "data/non_building_warnings"
        ]
        
        warning_stats = {}
        
        for wd in warning_dirs:
            wd_path = Path(wd)
            if wd_path.exists():
                files = list(wd_path.glob("*.json"))
                if files:
                    total_warnings = 0
                    for wf in files:
                        try:
                            with open(wf, 'r', encoding='utf-8') as f:
                                warnings = json.load(f)
                                total_warnings += len(warnings)
                        except:
                            pass
                            
                    warning_stats[wd] = total_warnings
                    print(f"   {wd}: {total_warnings} æ¡è­¦å‘Š")
                    
        if sum(warning_stats.values()) > 1000:
            self.results['findings'].append({
                'issue': 'å¤§é‡è­¦å‘Šäº§ç”Ÿ',
                'detail': f'æ€»è­¦å‘Šæ•°: {sum(warning_stats.values())}',
                'severity': 'medium'
            })
            
    def check_retry_patterns(self):
        """æ£€æŸ¥é‡è¯•æ¨¡å¼ï¼ˆé€šè¿‡æ¨¡æ‹Ÿï¼‰"""
        print("\nğŸ”„ åˆ†æé‡è¯•æ¨¡å¼...")
        
        # ç”±äºæ— æ³•ç›´æ¥è®¿é—®è¿è¡Œæ—¶æ•°æ®ï¼Œæˆ‘ä»¬é€šè¿‡è¿›åº¦æ–‡ä»¶æ¨æ–­
        progress_dir = Path("data/progress")
        if progress_dir.exists():
            progress_files = list(progress_dir.glob("*_simple_progress.json"))
            
            for pf in progress_files:
                try:
                    with open(pf, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        
                    # æ£€æŸ¥é”™è¯¯ç‡
                    if data.get('processed_tasks', 0) > 0:
                        error_rate = data.get('error_count', 0) / data['processed_tasks'] * 100
                        
                        if error_rate > 20:
                            print(f"   âš ï¸  {data['file_name']}: é”™è¯¯ç‡ {error_rate:.1f}%")
                            
                            self.results['findings'].append({
                                'issue': 'é«˜é”™è¯¯ç‡',
                                'detail': f'{data["file_name"]} é”™è¯¯ç‡è¾¾åˆ° {error_rate:.1f}%',
                                'severity': 'high'
                            })
                            
                except:
                    pass
                    
    def generate_report(self):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“‹ æ€§èƒ½é—®é¢˜éªŒè¯æŠ¥å‘Š")
        print("="*60)
        
        if not self.results['findings']:
            print("\nâœ… æœªå‘ç°æ˜æ˜¾çš„æ€§èƒ½é—®é¢˜")
        else:
            print(f"\nğŸ” å‘ç° {len(self.results['findings'])} ä¸ªé—®é¢˜:\n")
            
            # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
            high_severity = [f for f in self.results['findings'] if f['severity'] == 'high']
            medium_severity = [f for f in self.results['findings'] if f['severity'] == 'medium']
            
            if high_severity:
                print("ğŸ”´ é«˜ä¸¥é‡åº¦é—®é¢˜:")
                for f in high_severity:
                    print(f"   - {f['issue']}: {f['detail']}")
                    
            if medium_severity:
                print("\nğŸŸ¡ ä¸­ç­‰ä¸¥é‡åº¦é—®é¢˜:")
                for f in medium_severity:
                    print(f"   - {f['issue']}: {f['detail']}")
                    
        # æœ€å¯èƒ½çš„åŸå› 
        print("\nğŸ¯ æœ€å¯èƒ½çš„æ€§èƒ½ä¸‹é™åŸå› :")
        
        findings_text = str(self.results['findings'])
        
        if 'ä»»åŠ¡å¤„ç†é€Ÿåº¦è¿‡æ…¢' in findings_text:
            print("1. â±ï¸  ä»»åŠ¡å¤„ç†æ—¶é—´å¢åŠ ")
            print("   - åŸå› : é‡è¯•ä»»åŠ¡ç´¯ç§¯ï¼Œæ¯ä¸ªé‡è¯•éœ€è¦å®Œæ•´çš„æ»šåŠ¨æµç¨‹")
            print("   - å½±å“: CPUç©ºé—²ç­‰å¾…ç½‘é¡µåŠ è½½")
            
        if 'Chromeè¿›ç¨‹è¿‡å¤š' in findings_text or 'Chromeå†…å­˜å ç”¨è¿‡é«˜' in findings_text:
            print("2. ğŸŒ Chromeèµ„æºé—®é¢˜")
            print("   - åŸå› : é•¿æ—¶é—´è¿è¡Œå¯¼è‡´å†…å­˜æ³„æ¼")
            print("   - å½±å“: ç³»ç»Ÿèµ„æºè€—å°½ï¼Œå¤„ç†å˜æ…¢")
            
        if 'é«˜é”™è¯¯ç‡' in findings_text:
            print("3. ğŸ”„ é‡è¯•ä»»åŠ¡å †ç§¯")
            print("   - åŸå› : é”™è¯¯å’Œéå»ºç­‘ç‰©åœ°å€è§¦å‘å¤§é‡é‡è¯•")
            print("   - å½±å“: é‡è¯•é˜Ÿåˆ—å ç”¨Workerï¼Œæ­£å¸¸ä»»åŠ¡ç­‰å¾…")
            
        if 'é«˜POIå¯†åº¦å»ºç­‘ç‰©' in findings_text:
            print("4. ğŸ“ å¤æ‚ä»»åŠ¡å¢å¤š")
            print("   - åŸå› : åæœŸé‡åˆ°POIå¯†é›†çš„å¤§å‹å»ºç­‘")
            print("   - å½±å“: å•ä»»åŠ¡æ»šåŠ¨æ—¶é—´å¯è¾¾3-4åˆ†é’Ÿ")
            
        # é€šç”¨åŸå› 
        print("\n5. ğŸ”§ ç³»ç»Ÿæ€§ç“¶é¢ˆ")
        print("   - åŒæ­¥ç­‰å¾…ç´¯ç§¯: WebDriverWait(10ç§’) + æ»šåŠ¨ç­‰å¾…(2ç§’Ã—Næ¬¡)")
        print("   - ä»»åŠ¡åˆ†å¸ƒä¸å‡: ç®€å•ä»»åŠ¡å…ˆå®Œæˆï¼Œå‰©ä½™éƒ½æ˜¯å›°éš¾ä»»åŠ¡")
        print("   - ç½‘ç»œé™æµ: Google Mapså¯èƒ½å¯¹é¢‘ç¹è¯·æ±‚é™é€Ÿ")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path("performance_analysis") / f"verification_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
            
        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
    def run_verification(self):
        """è¿è¡Œå®Œæ•´éªŒè¯"""
        print("ğŸš€ å¼€å§‹éªŒè¯POIçˆ¬è™«æ€§èƒ½é—®é¢˜...")
        print(f"â° æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ‰§è¡Œå„é¡¹æ£€æŸ¥
        self.verify_progress_files()
        self.analyze_output_files()
        self.check_system_resources()
        self.analyze_warnings()
        self.check_retry_patterns()
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()


if __name__ == "__main__":
    verifier = PerformanceVerifier()
    verifier.run_verification()